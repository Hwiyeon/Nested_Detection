{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current version [1.3.1]\n",
      "Packages Loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)  ## just for ignore DeprcationWarning message\n",
    "print(\"Current version [%s]\" %(tf.__version__))\n",
    "print(\"Packages Loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLAGS READY\n"
     ]
    }
   ],
   "source": [
    "# Dataset Configurations\n",
    "tf.app.flags.DEFINE_integer('img_size', 28, \"\"\"Image size of MNIST dataset\"\"\")\n",
    "\n",
    "# Network Configurations\n",
    "tf.app.flags.DEFINE_integer('batch_size', 100, \"\"\"Number of images to process in a batch\"\"\")\n",
    "tf.app.flags.DEFINE_float('l1_ratio', 0.5, \"\"\"Ratio of level1\"\"\")\n",
    "tf.app.flags.DEFINE_float('l2_ratio', 0.5, \"\"\"Ratio of level2\"\"\")\n",
    "\n",
    "# Optimization Configurations\n",
    "tf.app.flags.DEFINE_float('lr', 0.001, \"\"\"Learning rate\"\"\")\n",
    "\n",
    "# Training Configurations\n",
    "tf.app.flags.DEFINE_integer('training_epochs', 500, \"\"\"Number of epochs to run\"\"\")\n",
    "tf.app.flags.DEFINE_integer('display_step', 5, \"\"\"Number of iterations to display training output\"\"\")\n",
    "tf.app.flags.DEFINE_integer('save_step', 5, \"\"\"Number of interations to save checkpoint\"\"\")\n",
    "tf.app.flags.DEFINE_integer('save_max', 5, \"\"\"Number of checkpoints to remain\"\"\")\n",
    "\n",
    "\n",
    "# Save Configurations\n",
    "tf.app.flags.DEFINE_string('nets', './nets', \"\"\"Directory where to write the checkpoints\"\"\")\n",
    "tf.app.flags.DEFINE_string('outputs', './outputs', \"\"\"Directory where to save the output images\"\"\")\n",
    "tf.app.flags.DEFINE_string('tboard', './tensorboard', \"\"\"Directory where to save the tensorboard logs\"\"\")\n",
    "\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "print(\"FLAGS READY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.allow_soft_placement = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../data/train-images-idx3-ubyte.gz\n",
      "Extracting ../../data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../../data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../data/t10k-labels-idx1-ubyte.gz\n",
      "MNIST ready\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('../../data/', one_hot=True)\n",
    "train_img = mnist.train.images\n",
    "train_label = mnist.train.labels\n",
    "test_img = mnist.test.images\n",
    "test_label = mnist.test.labels\n",
    "print(\"MNIST ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating random noise mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise_mask(prob=0.5):\n",
    "    mask = np.zeros([FLAGS.img_size, FLAGS.img_size])\n",
    "    rd = np.random.random()\n",
    "    if rd > prob:\n",
    "        # threshold of the size of masks\n",
    "        uthd = FLAGS.img_size    \n",
    "        lthd = 0     \n",
    "        # mask size should be beween 14x14, 5x5\n",
    "        while(uthd>14 or lthd<5):\n",
    "            ver1 = np.random.random_integers(0, FLAGS.img_size-1, size= 2)   # vertex1\n",
    "            ver2 = np.random.random_integers(0, FLAGS.img_size-1, size= 2)    # vertex2\n",
    "            uthd = np.maximum(np.abs(ver1[0]-ver2[0]), np.abs(ver1[1]-ver2[1]))    # upperbound\n",
    "            lthd = np.minimum(np.abs(ver1[0]-ver2[0]), np.abs(ver1[1]-ver2[1]))    # lowerbound\n",
    "        xmin = np.minimum(ver1[0], ver2[0])    # left x value\n",
    "        xmax = np.maximum(ver1[0], ver2[0])    # right x value\n",
    "        ymin = np.minimum(ver1[1], ver2[1])    # top y value\n",
    "        ymax = np.maximum(ver1[1], ver2[1])    # bottom y value\n",
    "        noise = np.random.random((xmax-xmin+1, ymax-ymin+1))    # random sample in [0,1]\n",
    "        mask[xmin:xmax+1, ymin:ymax+1] = noise    # noise mask with location\n",
    "        mask_meta = [xmin, xmax, ymin, ymax, noise, mask]\n",
    "    mask = np.reshape(mask, [-1])\n",
    "    return mask\n",
    "\n",
    "def noise_batch(batch_num):\n",
    "    # make random noise batch\n",
    "    mask_batch = np.zeros([batch_num, FLAGS.img_size*FLAGS.img_size])\n",
    "    for i in range(batch_num):\n",
    "        mask_batch[i,:] = noise_mask()\n",
    "    return mask_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Occlusion generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def occl(target, disturb):\n",
    "    mask = (disturb==0).astype(float)\n",
    "    masked_target = np.multiply(target, mask)\n",
    "    crpt = np.add(masked_target, disturb)\n",
    "    return crpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nested MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _nested_mlp(l1, l2_s, l2, out_channel, name, stddev=0.1, is_init=False, is_last=False):\n",
    "    l1_shape = l1.get_shape()[1]\n",
    "    l2_shape = l2.get_shape()[1]\n",
    "    l2_s_shape = l2_s.get_shape()[1]\n",
    "    \n",
    "    if is_init:\n",
    "        # input is the input image\n",
    "        with tf.device('/CPU:0'):\n",
    "            with tf.variable_scope('level1'):\n",
    "                with tf.variable_scope(name):\n",
    "                    l1_weights = tf.get_variable('weights', \n",
    "                                                 [l1_shape, out_channel*FLAGS.l1_ratio], \n",
    "                                                 tf.float32, \n",
    "                                                 initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "                    l1_biases = tf.get_variable('biases', \n",
    "                                                [out_channel*FLAGS.l1_ratio],\n",
    "                                                tf.float32, \n",
    "                                                initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "            with tf.variable_scope('level2'):\n",
    "                with tf.variable_scope(name):\n",
    "                    l2_s_weights = tf.get_variable('weights_shell', \n",
    "                                                 [l2_s_shape, out_channel*FLAGS.l2_ratio], \n",
    "                                                 tf.float32, \n",
    "                                                 initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "                    l2_s_biases = tf.get_variable('biases_shell', \n",
    "                                                [out_channel*FLAGS.l2_ratio],\n",
    "                                                tf.float32, \n",
    "                                                initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "                    \n",
    "\n",
    "        l1_mlp = tf.nn.sigmoid(tf.add(tf.matmul(l1, l1_weights), l1_biases))\n",
    "        l2_s_mlp = tf.nn.sigmoid(tf.add(tf.matmul(l2_s, l2_s_weights), l2_s_biases))\n",
    "        l2_mlp = tf.concat((l1_mlp, l2_s_mlp), 1)\n",
    "    \n",
    "    elif is_last:\n",
    "        # output is the generated image\n",
    "        with tf.device('/CPU:0'):\n",
    "            with tf.variable_scope('level1'):\n",
    "                with tf.variable_scope(name):\n",
    "                    l1_weights = tf.get_variable('weights', \n",
    "                                                 [l1_shape, out_channel], \n",
    "                                                 tf.float32, \n",
    "                                                 initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "                    l1_biases = tf.get_variable('biases', \n",
    "                                                [out_channel],\n",
    "                                                tf.float32, \n",
    "                                                initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "            with tf.variable_scope('level2'):\n",
    "                with tf.variable_scope(name):\n",
    "                    l2_s_weights = tf.get_variable('weights_shell', \n",
    "                                                 [l2_s_shape, out_channel], \n",
    "                                                 tf.float32, \n",
    "                                                 initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "                    l2_s_biases = tf.get_variable('biases_shell', \n",
    "                                                [out_channel],\n",
    "                                                tf.float32, \n",
    "                                                initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "                    l2_weights = tf.get_variable('weights', \n",
    "                                                 [l2_shape, out_channel], \n",
    "                                                 tf.float32, \n",
    "                                                 initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "                    l2_biases = tf.get_variable('biases', \n",
    "                                                [out_channel],\n",
    "                                                tf.float32, \n",
    "                                                initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "                    \n",
    "\n",
    "        l1_mlp = tf.nn.sigmoid(tf.add(tf.matmul(l1, l1_weights), l1_biases))\n",
    "        l2_s_mlp = tf.nn.sigmoid(tf.add(tf.matmul(l2_s, l2_s_weights), l2_s_biases))\n",
    "        l2_mlp = tf.nn.sigmoid(tf.add(tf.matmul(l2, l2_weights), l2_biases))\n",
    "                                 \n",
    "    else:\n",
    "        with tf.device('/CPU:0'):\n",
    "            with tf.variable_scope('level1'):\n",
    "                with tf.variable_scope(name):\n",
    "                    l1_weights = tf.get_variable('weights', \n",
    "                                                 [l1_shape, out_channel*FLAGS.l1_ratio], \n",
    "                                                 tf.float32, \n",
    "                                                 initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "                    l1_biases = tf.get_variable('biases', \n",
    "                                                [out_channel*FLAGS.l1_ratio],\n",
    "                                                tf.float32, \n",
    "                                                initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "            with tf.variable_scope('level2'):\n",
    "                with tf.variable_scope(name):\n",
    "                    l2_s_weights = tf.get_variable('weights_shell', \n",
    "                                                   [l2_s_shape, out_channel*FLAGS.l2_ratio], \n",
    "                                                   tf.float32, \n",
    "                                                   initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "                    l2_s_biases = tf.get_variable('biases_shell', \n",
    "                                                  [out_channel*FLAGS.l2_ratio],\n",
    "                                                  tf.float32, \n",
    "                                                  initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "                    l2_weights_1 = tf.get_variable('weights_1', \n",
    "                                                   [l2_s_shape, out_channel*FLAGS.l1_ratio], \n",
    "                                                   tf.float32, \n",
    "                                                   initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "                    l2_biases_1 = tf.get_variable('biases_1', \n",
    "                                                  [out_channel*FLAGS.l1_ratio],\n",
    "                                                  tf.float32, \n",
    "                                                  initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "                    l2_weights_2 = tf.get_variable('weights_2', \n",
    "                                                   [l1_shape, out_channel*FLAGS.l2_ratio], \n",
    "                                                   tf.float32, \n",
    "                                                   initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "                    l2_biases_2 = tf.get_variable('biases_2', \n",
    "                                                  [out_channel*FLAGS.l2_ratio],\n",
    "                                                  tf.float32, \n",
    "                                                  initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "\n",
    "        l1_mlp_r = tf.add(tf.matmul(l1, l1_weights), l1_biases)\n",
    "        l1_mlp = tf.nn.sigmoid(l1_mlp_r)\n",
    "        \n",
    "        l2_s_mlp = tf.nn.sigmoid(tf.add(tf.matmul(l2_s, l2_s_weights), l2_s_biases))\n",
    "        \n",
    "        l2_mlp_1_r = tf.add(tf.matmul(l2[:,l1_shape:l2_shape], l2_weights_1), l2_biases_1)\n",
    "        l2_mlp_1 = tf.nn.sigmoid(tf.add(l1_mlp_r, l2_mlp_1_r))\n",
    "        l2_mlp_2_r = tf.add(tf.matmul(l2[:,:l1_shape], l2_weights_2), l2_biases_2)\n",
    "        l2_mlp_3_r = tf.add(tf.matmul(l2[:,l1_shape:l2_shape], l2_s_weights), l2_s_biases)\n",
    "        l2_mlp_2 = tf.nn.sigmoid(tf.add(l2_mlp_2_r, l2_mlp_3_r))\n",
    "        l2_mlp = tf.concat((l2_mlp_1, l2_mlp_2), 1)\n",
    "        \n",
    "        \n",
    "    return l1_mlp, l2_s_mlp, l2_mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graphs Ready\n"
     ]
    }
   ],
   "source": [
    "# Network Topology\n",
    "n_input = FLAGS.img_size*FLAGS.img_size\n",
    "n_enc1 = 1024\n",
    "n_enc2 = 512\n",
    "n_enc3 = 256\n",
    "n_dec1 = 512\n",
    "n_dec2 = 1024\n",
    "n_out = 784\n",
    "\n",
    "# Inputs and Outputs\n",
    "ph_pure = tf.placeholder(\"float\", [None, n_input])    # pure image --- core\n",
    "ph_noise= tf.placeholder(\"float\", [None, n_input])    # noise --- shell1\n",
    "ph_crpt = tf.placeholder(\"float\", [None, n_input])    # corrupted image   --- level2\n",
    "\n",
    "\n",
    "# Model\n",
    "def nested_ae_mlp(_X):\n",
    "    l1_enc1, l2_s_enc1, l2_enc1 = _nested_mlp(_X, _X, _X, n_enc1, name='enc1', is_init=True)\n",
    "    l1_enc2, l2_s_enc2, l2_enc2 = _nested_mlp(l1_enc1, l2_s_enc1, l2_enc1, n_enc2, name='enc2')\n",
    "    l1_enc3, l2_s_enc3, l2_enc3 = _nested_mlp(l1_enc2, l2_s_enc2, l2_enc2, n_enc3, name='enc3')\n",
    "    l1_dec1, l2_s_dec1, l2_dec1 = _nested_mlp(l1_enc3, l2_s_enc3, l2_enc3, n_dec1, name='dec1')\n",
    "    l1_dec2, l2_s_dec2, l2_dec2 = _nested_mlp(l1_dec1, l2_s_dec1, l2_dec1, n_dec2, name='dec2')\n",
    "    l1_out, l2_s_out, l2_out = _nested_mlp(l1_dec2, l2_s_dec2, l2_dec2, n_out, name='out',is_last=True)\n",
    "    return l1_out, l2_s_out, l2_out\n",
    "\n",
    "# Generation\n",
    "core_gen, shell2_gen, full_gen = nested_ae_mlp(ph_crpt)   # [None, n_input]\n",
    "\n",
    "# Loss & Optimizer\n",
    "with tf.name_scope(\"loss\") as scope:\n",
    "    loss = tf.reduce_mean(tf.nn.l2_loss(full_gen-ph_crpt)) + 2*tf.reduce_mean(tf.nn.l2_loss(core_gen-ph_pure))\\\n",
    "            + tf.reduce_mean(tf.nn.l2_loss(shell2_gen-ph_noise))\n",
    "    _train_loss = tf.summary.scalar(\"train_loss\", loss)\n",
    "    _test_loss = tf.summary.scalar(\"test_loss\", loss)\n",
    "\n",
    "optm = tf.train.AdamOptimizer(learning_rate=FLAGS.lr).minimize(loss)\n",
    "\n",
    "\n",
    "print(\"Graphs Ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize Ready\n"
     ]
    }
   ],
   "source": [
    "merged = tf.summary.merge_all()\n",
    "tensorboard_path = FLAGS.tboard\n",
    "if not os.path.exists(tensorboard_path):\n",
    "    os.makedirs(tensorboard_path)\n",
    "writer = tf.summary.FileWriter(tensorboard_path)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "print(\"Initialize Ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saver ready\n"
     ]
    }
   ],
   "source": [
    "outputdir = FLAGS.outputs\n",
    "if not os.path.exists(outputdir+'/train'):\n",
    "    os.makedirs(outputdir+'/train')\n",
    "\n",
    "if not os.path.exists(outputdir+'/test'):\n",
    "    os.makedirs(outputdir+'/test')\n",
    "    \n",
    "savedir = FLAGS.nets\n",
    "if not os.path.exists(savedir):\n",
    "    os.makedirs(savedir)\n",
    "    \n",
    "saver = tf.train.Saver(max_to_keep=FLAGS.save_max)\n",
    "print(\"Saver ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 001/500  Train_loss : 5454.3647461  Test_loss : 5012.3193359\n",
      "Epoch : 002/500  Train_loss : 4255.8417969  Test_loss : 3928.7670898\n",
      "Epoch : 003/500  Train_loss : 3372.7214355  Test_loss : 3534.4455566\n",
      "Epoch : 004/500  Train_loss : 3281.3063965  Test_loss : 3327.9497070\n",
      "Epoch : 005/500  Train_loss : 2920.5751953  Test_loss : 2762.7153320\n",
      "[./nets/net-5.ckpt] SAVED\n",
      "Epoch : 006/500  Train_loss : 2775.1901855  Test_loss : 2521.0156250\n",
      "Epoch : 007/500  Train_loss : 2335.6823730  Test_loss : 2475.6821289\n",
      "Epoch : 008/500  Train_loss : 2889.8679199  Test_loss : 2621.0732422\n",
      "Epoch : 009/500  Train_loss : 2148.3845215  Test_loss : 2323.3823242\n",
      "Epoch : 010/500  Train_loss : 2199.6513672  Test_loss : 2189.1513672\n",
      "[./nets/net-10.ckpt] SAVED\n",
      "Epoch : 011/500  Train_loss : 2280.5869141  Test_loss : 2218.4072266\n",
      "Epoch : 012/500  Train_loss : 2221.2485352  Test_loss : 2119.8720703\n",
      "Epoch : 013/500  Train_loss : 1957.3471680  Test_loss : 2038.6566162\n",
      "Epoch : 014/500  Train_loss : 1915.5449219  Test_loss : 2039.2832031\n",
      "Epoch : 015/500  Train_loss : 1643.8928223  Test_loss : 1884.0380859\n",
      "[./nets/net-15.ckpt] SAVED\n",
      "Epoch : 016/500  Train_loss : 1651.1398926  Test_loss : 2130.2409668\n",
      "Epoch : 017/500  Train_loss : 1901.7851562  Test_loss : 1864.8208008\n",
      "Epoch : 018/500  Train_loss : 1822.0590820  Test_loss : 1775.9675293\n",
      "Epoch : 019/500  Train_loss : 1947.2064209  Test_loss : 1801.8964844\n",
      "Epoch : 020/500  Train_loss : 1740.2770996  Test_loss : 1808.4501953\n",
      "[./nets/net-20.ckpt] SAVED\n",
      "Epoch : 021/500  Train_loss : 1721.3227539  Test_loss : 1794.1069336\n",
      "Epoch : 022/500  Train_loss : 1905.7772217  Test_loss : 1654.0275879\n",
      "Epoch : 023/500  Train_loss : 1641.5463867  Test_loss : 1813.5676270\n",
      "Epoch : 024/500  Train_loss : 1720.1956787  Test_loss : 1677.6656494\n",
      "Epoch : 025/500  Train_loss : 1578.4119873  Test_loss : 1616.3144531\n",
      "[./nets/net-25.ckpt] SAVED\n",
      "Epoch : 026/500  Train_loss : 1802.4521484  Test_loss : 1890.9340820\n",
      "Epoch : 027/500  Train_loss : 1834.4377441  Test_loss : 1825.2592773\n",
      "Epoch : 028/500  Train_loss : 1589.2034912  Test_loss : 1690.3903809\n",
      "Epoch : 029/500  Train_loss : 1615.9733887  Test_loss : 1430.9237061\n",
      "Epoch : 030/500  Train_loss : 1482.9786377  Test_loss : 1674.7800293\n",
      "[./nets/net-30.ckpt] SAVED\n",
      "Epoch : 031/500  Train_loss : 1529.8996582  Test_loss : 1633.0059814\n",
      "Epoch : 032/500  Train_loss : 1538.1574707  Test_loss : 1403.3948975\n",
      "Epoch : 033/500  Train_loss : 1568.0712891  Test_loss : 1552.5273438\n",
      "Epoch : 034/500  Train_loss : 1602.8876953  Test_loss : 1489.6905518\n",
      "Epoch : 035/500  Train_loss : 1466.7259521  Test_loss : 1610.4747314\n",
      "[./nets/net-35.ckpt] SAVED\n",
      "Epoch : 036/500  Train_loss : 1270.1791992  Test_loss : 1390.2958984\n",
      "Epoch : 037/500  Train_loss : 1524.1906738  Test_loss : 1471.2800293\n",
      "Epoch : 038/500  Train_loss : 1438.1152344  Test_loss : 1552.6557617\n",
      "Epoch : 039/500  Train_loss : 1581.5651855  Test_loss : 1307.9416504\n",
      "Epoch : 040/500  Train_loss : 1476.6557617  Test_loss : 1410.4448242\n",
      "[./nets/net-40.ckpt] SAVED\n",
      "Epoch : 041/500  Train_loss : 1630.8173828  Test_loss : 1349.0065918\n",
      "Epoch : 042/500  Train_loss : 1549.4973145  Test_loss : 1275.2554932\n",
      "Epoch : 043/500  Train_loss : 1438.7382812  Test_loss : 1461.9161377\n",
      "Epoch : 044/500  Train_loss : 1404.2264404  Test_loss : 1403.4283447\n",
      "Epoch : 045/500  Train_loss : 1397.7866211  Test_loss : 1406.1322021\n",
      "[./nets/net-45.ckpt] SAVED\n",
      "Epoch : 046/500  Train_loss : 1264.6156006  Test_loss : 1366.1452637\n",
      "Epoch : 047/500  Train_loss : 1618.1076660  Test_loss : 1379.9263916\n",
      "Epoch : 048/500  Train_loss : 1216.2490234  Test_loss : 1377.4807129\n",
      "Epoch : 049/500  Train_loss : 1267.7327881  Test_loss : 1433.7790527\n",
      "Epoch : 050/500  Train_loss : 1249.1688232  Test_loss : 1406.7991943\n",
      "[./nets/net-50.ckpt] SAVED\n",
      "Epoch : 051/500  Train_loss : 1432.5600586  Test_loss : 1273.3381348\n",
      "Epoch : 052/500  Train_loss : 1277.1391602  Test_loss : 1427.2484131\n",
      "Epoch : 053/500  Train_loss : 1351.7302246  Test_loss : 1417.9686279\n",
      "Epoch : 054/500  Train_loss : 1299.2087402  Test_loss : 1557.5081787\n",
      "Epoch : 055/500  Train_loss : 1356.9182129  Test_loss : 1330.3538818\n",
      "[./nets/net-55.ckpt] SAVED\n",
      "Epoch : 056/500  Train_loss : 1457.0500488  Test_loss : 1351.6650391\n",
      "Epoch : 057/500  Train_loss : 1122.3796387  Test_loss : 1210.8640137\n",
      "Epoch : 058/500  Train_loss : 1268.0800781  Test_loss : 1344.4951172\n",
      "Epoch : 059/500  Train_loss : 1252.4598389  Test_loss : 1423.1341553\n",
      "Epoch : 060/500  Train_loss : 1391.7175293  Test_loss : 1316.1435547\n",
      "[./nets/net-60.ckpt] SAVED\n",
      "Epoch : 061/500  Train_loss : 1218.8533936  Test_loss : 1222.8530273\n",
      "Epoch : 062/500  Train_loss : 1377.7369385  Test_loss : 1344.3209229\n",
      "Epoch : 063/500  Train_loss : 1282.5842285  Test_loss : 1312.7863770\n",
      "Epoch : 064/500  Train_loss : 1304.4130859  Test_loss : 1331.1101074\n",
      "Epoch : 065/500  Train_loss : 1304.8577881  Test_loss : 1297.3433838\n",
      "[./nets/net-65.ckpt] SAVED\n",
      "Epoch : 066/500  Train_loss : 1328.9234619  Test_loss : 1243.8806152\n",
      "Epoch : 067/500  Train_loss : 1346.0893555  Test_loss : 1229.6685791\n",
      "Epoch : 068/500  Train_loss : 1161.4681396  Test_loss : 1351.5397949\n",
      "Epoch : 069/500  Train_loss : 1401.5148926  Test_loss : 1544.6065674\n",
      "Epoch : 070/500  Train_loss : 1207.0098877  Test_loss : 1392.6215820\n",
      "[./nets/net-70.ckpt] SAVED\n",
      "Epoch : 071/500  Train_loss : 1049.5093994  Test_loss : 1306.0036621\n",
      "Epoch : 072/500  Train_loss : 1440.8413086  Test_loss : 1148.1097412\n",
      "Epoch : 073/500  Train_loss : 1204.1235352  Test_loss : 1097.2011719\n",
      "Epoch : 074/500  Train_loss : 1232.1997070  Test_loss : 1335.9716797\n",
      "Epoch : 075/500  Train_loss : 1370.5241699  Test_loss : 1218.5321045\n",
      "[./nets/net-75.ckpt] SAVED\n",
      "Epoch : 076/500  Train_loss : 1190.8267822  Test_loss : 1242.7884521\n",
      "Epoch : 077/500  Train_loss : 1137.1370850  Test_loss : 1235.1309814\n",
      "Epoch : 078/500  Train_loss : 1160.2644043  Test_loss : 1223.3828125\n",
      "Epoch : 079/500  Train_loss : 1391.4803467  Test_loss : 1235.3676758\n",
      "Epoch : 080/500  Train_loss : 1217.8237305  Test_loss : 1211.3717041\n",
      "[./nets/net-80.ckpt] SAVED\n",
      "Epoch : 081/500  Train_loss : 1112.7161865  Test_loss : 1368.2102051\n",
      "Epoch : 082/500  Train_loss : 1393.2556152  Test_loss : 1321.9916992\n",
      "Epoch : 083/500  Train_loss : 1297.9536133  Test_loss : 1221.8933105\n",
      "Epoch : 084/500  Train_loss : 1100.6647949  Test_loss : 1164.2630615\n",
      "Epoch : 085/500  Train_loss : 1221.1303711  Test_loss : 1402.4293213\n",
      "[./nets/net-85.ckpt] SAVED\n",
      "Epoch : 086/500  Train_loss : 1144.5299072  Test_loss : 1030.5413818\n",
      "Epoch : 087/500  Train_loss : 1104.6920166  Test_loss : 1228.3300781\n",
      "Epoch : 088/500  Train_loss : 1164.7648926  Test_loss : 1294.0616455\n",
      "Epoch : 089/500  Train_loss : 1227.0007324  Test_loss : 1159.6353760\n",
      "Epoch : 090/500  Train_loss : 1104.3009033  Test_loss : 1148.6798096\n",
      "[./nets/net-90.ckpt] SAVED\n",
      "Epoch : 091/500  Train_loss : 1066.0766602  Test_loss : 1028.7460938\n",
      "Epoch : 092/500  Train_loss : 1330.6516113  Test_loss : 1273.6391602\n",
      "Epoch : 093/500  Train_loss : 974.5915527  Test_loss : 1138.6188965\n",
      "Epoch : 094/500  Train_loss : 1065.3016357  Test_loss : 1143.6727295\n",
      "Epoch : 095/500  Train_loss : 1111.0828857  Test_loss : 1065.9241943\n",
      "[./nets/net-95.ckpt] SAVED\n",
      "Epoch : 096/500  Train_loss : 1219.7664795  Test_loss : 1228.1931152\n",
      "Epoch : 097/500  Train_loss : 1149.3635254  Test_loss : 1108.4951172\n",
      "Epoch : 098/500  Train_loss : 1117.3750000  Test_loss : 1255.4492188\n",
      "Epoch : 099/500  Train_loss : 1193.3140869  Test_loss : 1058.5650635\n",
      "Epoch : 100/500  Train_loss : 1138.2360840  Test_loss : 947.9067383\n",
      "[./nets/net-100.ckpt] SAVED\n",
      "Epoch : 101/500  Train_loss : 1116.4908447  Test_loss : 1131.5119629\n",
      "Epoch : 102/500  Train_loss : 1065.1104736  Test_loss : 1243.9095459\n",
      "Epoch : 103/500  Train_loss : 1257.5012207  Test_loss : 1031.0581055\n",
      "Epoch : 104/500  Train_loss : 1029.1673584  Test_loss : 1458.4184570\n",
      "Epoch : 105/500  Train_loss : 1031.8099365  Test_loss : 1014.0800781\n",
      "[./nets/net-105.ckpt] SAVED\n",
      "Epoch : 106/500  Train_loss : 1099.5605469  Test_loss : 1082.1949463\n",
      "Epoch : 107/500  Train_loss : 1061.7171631  Test_loss : 1092.8713379\n",
      "Epoch : 108/500  Train_loss : 1032.1456299  Test_loss : 1248.4418945\n",
      "Epoch : 109/500  Train_loss : 1227.4652100  Test_loss : 1227.2437744\n",
      "Epoch : 110/500  Train_loss : 1228.6743164  Test_loss : 1284.4489746\n",
      "[./nets/net-110.ckpt] SAVED\n",
      "Epoch : 111/500  Train_loss : 1026.9204102  Test_loss : 1276.4133301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 112/500  Train_loss : 1077.6881104  Test_loss : 1032.1082764\n",
      "Epoch : 113/500  Train_loss : 1102.9626465  Test_loss : 1272.9294434\n",
      "Epoch : 114/500  Train_loss : 904.7679443  Test_loss : 936.6815796\n",
      "Epoch : 115/500  Train_loss : 1098.3111572  Test_loss : 1179.1013184\n",
      "[./nets/net-115.ckpt] SAVED\n",
      "Epoch : 116/500  Train_loss : 1182.3854980  Test_loss : 971.2183228\n",
      "Epoch : 117/500  Train_loss : 1158.1480713  Test_loss : 1091.7724609\n",
      "Epoch : 118/500  Train_loss : 1034.0587158  Test_loss : 941.5605469\n",
      "Epoch : 119/500  Train_loss : 1029.7268066  Test_loss : 1029.6093750\n",
      "Epoch : 120/500  Train_loss : 971.2425537  Test_loss : 1046.6250000\n",
      "[./nets/net-120.ckpt] SAVED\n",
      "Epoch : 121/500  Train_loss : 1156.6398926  Test_loss : 1107.2523193\n",
      "Epoch : 122/500  Train_loss : 1066.5659180  Test_loss : 1189.3062744\n",
      "Epoch : 123/500  Train_loss : 1046.5021973  Test_loss : 1066.5864258\n",
      "Epoch : 124/500  Train_loss : 1150.6079102  Test_loss : 1110.7536621\n",
      "Epoch : 125/500  Train_loss : 1015.1950684  Test_loss : 1029.8343506\n",
      "[./nets/net-125.ckpt] SAVED\n",
      "Epoch : 126/500  Train_loss : 1067.2333984  Test_loss : 1139.2478027\n",
      "Epoch : 127/500  Train_loss : 1009.7493896  Test_loss : 1083.5372314\n",
      "Epoch : 128/500  Train_loss : 1068.6116943  Test_loss : 1168.5212402\n",
      "Epoch : 129/500  Train_loss : 1016.0065918  Test_loss : 1011.0458374\n",
      "Epoch : 130/500  Train_loss : 964.2740479  Test_loss : 1091.6492920\n",
      "[./nets/net-130.ckpt] SAVED\n",
      "Epoch : 131/500  Train_loss : 887.0934448  Test_loss : 1087.0228271\n",
      "Epoch : 132/500  Train_loss : 1379.4603271  Test_loss : 1004.1170044\n",
      "Epoch : 133/500  Train_loss : 966.1882324  Test_loss : 994.6003418\n",
      "Epoch : 134/500  Train_loss : 1125.4034424  Test_loss : 1194.6204834\n",
      "Epoch : 135/500  Train_loss : 1173.3740234  Test_loss : 964.0050659\n",
      "[./nets/net-135.ckpt] SAVED\n",
      "Epoch : 136/500  Train_loss : 889.5499878  Test_loss : 1025.0194092\n",
      "Epoch : 137/500  Train_loss : 1118.6494141  Test_loss : 1121.7286377\n",
      "Epoch : 138/500  Train_loss : 1106.3071289  Test_loss : 940.0384521\n",
      "Epoch : 139/500  Train_loss : 967.0943604  Test_loss : 1093.9542236\n",
      "Epoch : 140/500  Train_loss : 1057.4761963  Test_loss : 921.1740112\n",
      "[./nets/net-140.ckpt] SAVED\n",
      "Epoch : 141/500  Train_loss : 1002.8031006  Test_loss : 1130.8005371\n",
      "Epoch : 142/500  Train_loss : 982.8067017  Test_loss : 893.4639282\n",
      "Epoch : 143/500  Train_loss : 1032.2604980  Test_loss : 1082.0842285\n",
      "Epoch : 144/500  Train_loss : 1043.6217041  Test_loss : 1137.0012207\n",
      "Epoch : 145/500  Train_loss : 1181.0881348  Test_loss : 1151.1716309\n",
      "[./nets/net-145.ckpt] SAVED\n",
      "Epoch : 146/500  Train_loss : 1020.9240723  Test_loss : 1078.2253418\n",
      "Epoch : 147/500  Train_loss : 1073.2899170  Test_loss : 1005.3474731\n",
      "Epoch : 148/500  Train_loss : 1069.4798584  Test_loss : 1091.1657715\n",
      "Epoch : 149/500  Train_loss : 897.0617676  Test_loss : 1042.0736084\n",
      "Epoch : 150/500  Train_loss : 1167.7470703  Test_loss : 962.9981689\n",
      "[./nets/net-150.ckpt] SAVED\n",
      "Epoch : 151/500  Train_loss : 1042.3508301  Test_loss : 1123.0454102\n",
      "Epoch : 152/500  Train_loss : 943.6901245  Test_loss : 934.0330811\n",
      "Epoch : 153/500  Train_loss : 1047.1242676  Test_loss : 1100.5244141\n",
      "Epoch : 154/500  Train_loss : 1040.2058105  Test_loss : 974.8129272\n",
      "Epoch : 155/500  Train_loss : 1051.3963623  Test_loss : 967.3984985\n",
      "[./nets/net-155.ckpt] SAVED\n",
      "Epoch : 156/500  Train_loss : 949.8441162  Test_loss : 1050.4235840\n",
      "Epoch : 157/500  Train_loss : 977.1260986  Test_loss : 952.7803955\n",
      "Epoch : 158/500  Train_loss : 994.5213013  Test_loss : 906.0238647\n",
      "Epoch : 159/500  Train_loss : 991.6710205  Test_loss : 1217.6661377\n",
      "Epoch : 160/500  Train_loss : 1111.0294189  Test_loss : 880.4305420\n",
      "[./nets/net-160.ckpt] SAVED\n",
      "Epoch : 161/500  Train_loss : 1000.0413818  Test_loss : 981.8258057\n",
      "Epoch : 162/500  Train_loss : 980.7318115  Test_loss : 928.9400635\n",
      "Epoch : 163/500  Train_loss : 1063.1040039  Test_loss : 987.0779419\n",
      "Epoch : 164/500  Train_loss : 947.9317017  Test_loss : 858.2791138\n",
      "Epoch : 165/500  Train_loss : 991.3612671  Test_loss : 977.7117920\n",
      "[./nets/net-165.ckpt] SAVED\n",
      "Epoch : 166/500  Train_loss : 1066.5451660  Test_loss : 926.2117310\n",
      "Epoch : 167/500  Train_loss : 1005.6917725  Test_loss : 1111.2038574\n",
      "Epoch : 168/500  Train_loss : 892.5223389  Test_loss : 986.5540161\n",
      "Epoch : 169/500  Train_loss : 1153.9532471  Test_loss : 884.3962402\n",
      "Epoch : 170/500  Train_loss : 948.1303711  Test_loss : 1022.7401733\n",
      "[./nets/net-170.ckpt] SAVED\n",
      "Epoch : 171/500  Train_loss : 910.8408203  Test_loss : 1014.8240967\n",
      "Epoch : 172/500  Train_loss : 943.3940430  Test_loss : 859.3344727\n",
      "Epoch : 173/500  Train_loss : 1070.3732910  Test_loss : 1112.8175049\n",
      "Epoch : 174/500  Train_loss : 987.2569580  Test_loss : 997.6033936\n",
      "Epoch : 175/500  Train_loss : 1001.4853516  Test_loss : 735.6301270\n",
      "[./nets/net-175.ckpt] SAVED\n",
      "Epoch : 176/500  Train_loss : 871.1229858  Test_loss : 973.7889404\n",
      "Epoch : 177/500  Train_loss : 859.5380859  Test_loss : 1063.2207031\n",
      "Epoch : 178/500  Train_loss : 891.7711182  Test_loss : 918.0778198\n",
      "Epoch : 179/500  Train_loss : 909.1768799  Test_loss : 998.3665161\n",
      "Epoch : 180/500  Train_loss : 850.2626953  Test_loss : 954.3918457\n",
      "[./nets/net-180.ckpt] SAVED\n",
      "Epoch : 181/500  Train_loss : 854.4168091  Test_loss : 1049.5227051\n",
      "Epoch : 182/500  Train_loss : 973.3370361  Test_loss : 1021.5070801\n",
      "Epoch : 183/500  Train_loss : 977.1744995  Test_loss : 1131.0122070\n",
      "Epoch : 184/500  Train_loss : 1045.2783203  Test_loss : 864.7836914\n",
      "Epoch : 185/500  Train_loss : 748.0822144  Test_loss : 940.2174072\n",
      "[./nets/net-185.ckpt] SAVED\n",
      "Epoch : 186/500  Train_loss : 828.5264282  Test_loss : 1017.7946777\n",
      "Epoch : 187/500  Train_loss : 891.2968750  Test_loss : 988.5240479\n",
      "Epoch : 188/500  Train_loss : 968.5510254  Test_loss : 878.2875366\n",
      "Epoch : 189/500  Train_loss : 983.5570679  Test_loss : 995.0958862\n",
      "Epoch : 190/500  Train_loss : 854.1287842  Test_loss : 1074.2645264\n",
      "[./nets/net-190.ckpt] SAVED\n",
      "Epoch : 191/500  Train_loss : 860.6379395  Test_loss : 1095.2039795\n",
      "Epoch : 192/500  Train_loss : 912.8004150  Test_loss : 1124.2707520\n",
      "Epoch : 193/500  Train_loss : 901.8555298  Test_loss : 890.3009644\n",
      "Epoch : 194/500  Train_loss : 978.9375610  Test_loss : 848.3206177\n",
      "Epoch : 195/500  Train_loss : 953.8995361  Test_loss : 980.3578491\n",
      "[./nets/net-195.ckpt] SAVED\n",
      "Epoch : 196/500  Train_loss : 908.0661011  Test_loss : 895.8546753\n",
      "Epoch : 197/500  Train_loss : 872.3898926  Test_loss : 948.1484375\n",
      "Epoch : 198/500  Train_loss : 1073.4984131  Test_loss : 807.2360229\n",
      "Epoch : 199/500  Train_loss : 883.0153198  Test_loss : 910.7410889\n",
      "Epoch : 200/500  Train_loss : 908.4470215  Test_loss : 1014.7208862\n",
      "[./nets/net-200.ckpt] SAVED\n",
      "Epoch : 201/500  Train_loss : 824.8250122  Test_loss : 1012.0637817\n",
      "Epoch : 202/500  Train_loss : 876.4671021  Test_loss : 1019.1462402\n",
      "Epoch : 203/500  Train_loss : 842.7864990  Test_loss : 982.2882690\n",
      "Epoch : 204/500  Train_loss : 941.5913086  Test_loss : 961.2515259\n",
      "Epoch : 205/500  Train_loss : 836.3893433  Test_loss : 709.8198242\n",
      "[./nets/net-205.ckpt] SAVED\n",
      "Epoch : 206/500  Train_loss : 894.7591553  Test_loss : 886.6254883\n",
      "Epoch : 207/500  Train_loss : 909.1137695  Test_loss : 767.0870361\n",
      "Epoch : 208/500  Train_loss : 882.2979126  Test_loss : 918.8353271\n",
      "Epoch : 209/500  Train_loss : 841.4512939  Test_loss : 854.3348389\n",
      "Epoch : 210/500  Train_loss : 784.9425659  Test_loss : 972.3235474\n",
      "[./nets/net-210.ckpt] SAVED\n",
      "Epoch : 211/500  Train_loss : 822.3309937  Test_loss : 979.5359497\n",
      "Epoch : 212/500  Train_loss : 884.5216064  Test_loss : 906.9282837\n",
      "Epoch : 213/500  Train_loss : 973.7164917  Test_loss : 998.0594482\n",
      "Epoch : 214/500  Train_loss : 860.9759521  Test_loss : 963.1797485\n",
      "Epoch : 215/500  Train_loss : 852.7282104  Test_loss : 947.8795166\n",
      "[./nets/net-215.ckpt] SAVED\n",
      "Epoch : 216/500  Train_loss : 941.0834961  Test_loss : 989.1285400\n",
      "Epoch : 217/500  Train_loss : 807.3597412  Test_loss : 906.4589844\n",
      "Epoch : 218/500  Train_loss : 965.1190186  Test_loss : 933.4982910\n",
      "Epoch : 219/500  Train_loss : 856.6878662  Test_loss : 904.1775513\n",
      "Epoch : 220/500  Train_loss : 881.4181519  Test_loss : 1073.6999512\n",
      "[./nets/net-220.ckpt] SAVED\n",
      "Epoch : 221/500  Train_loss : 877.2886963  Test_loss : 1026.4594727\n",
      "Epoch : 222/500  Train_loss : 971.3933105  Test_loss : 925.2193604\n",
      "Epoch : 223/500  Train_loss : 834.5301514  Test_loss : 1093.7381592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 224/500  Train_loss : 879.1410522  Test_loss : 1033.1687012\n",
      "Epoch : 225/500  Train_loss : 885.6442871  Test_loss : 865.0101318\n",
      "[./nets/net-225.ckpt] SAVED\n",
      "Epoch : 226/500  Train_loss : 897.6739502  Test_loss : 1002.0284424\n",
      "Epoch : 227/500  Train_loss : 779.9050293  Test_loss : 772.3883667\n",
      "Epoch : 228/500  Train_loss : 824.7344360  Test_loss : 897.4357910\n",
      "Epoch : 229/500  Train_loss : 927.2681274  Test_loss : 893.1401367\n",
      "Epoch : 230/500  Train_loss : 926.9558105  Test_loss : 839.1156616\n",
      "[./nets/net-230.ckpt] SAVED\n",
      "Epoch : 231/500  Train_loss : 863.4737549  Test_loss : 952.4473877\n",
      "Epoch : 232/500  Train_loss : 844.1868896  Test_loss : 1030.5745850\n",
      "Epoch : 233/500  Train_loss : 882.4113770  Test_loss : 906.6337891\n",
      "Epoch : 234/500  Train_loss : 756.8988037  Test_loss : 973.7802734\n",
      "Epoch : 235/500  Train_loss : 911.7322998  Test_loss : 868.0529175\n",
      "[./nets/net-235.ckpt] SAVED\n",
      "Epoch : 236/500  Train_loss : 795.0942383  Test_loss : 902.7725830\n",
      "Epoch : 237/500  Train_loss : 870.0540161  Test_loss : 898.8277588\n",
      "Epoch : 238/500  Train_loss : 758.9038086  Test_loss : 866.8665161\n",
      "Epoch : 239/500  Train_loss : 830.6783447  Test_loss : 1027.3790283\n",
      "Epoch : 240/500  Train_loss : 818.6250610  Test_loss : 711.3943481\n",
      "[./nets/net-240.ckpt] SAVED\n",
      "Epoch : 241/500  Train_loss : 894.8826294  Test_loss : 825.4230957\n",
      "Epoch : 242/500  Train_loss : 818.6023560  Test_loss : 735.4664917\n",
      "Epoch : 243/500  Train_loss : 802.2191162  Test_loss : 876.1864014\n",
      "Epoch : 244/500  Train_loss : 859.7030029  Test_loss : 896.7193604\n",
      "Epoch : 245/500  Train_loss : 757.5195312  Test_loss : 982.7685547\n",
      "[./nets/net-245.ckpt] SAVED\n",
      "Epoch : 246/500  Train_loss : 784.1049805  Test_loss : 918.0803223\n",
      "Epoch : 247/500  Train_loss : 791.6948242  Test_loss : 977.2286377\n",
      "Epoch : 248/500  Train_loss : 630.0314331  Test_loss : 858.8024292\n",
      "Epoch : 249/500  Train_loss : 803.3799438  Test_loss : 957.8529053\n",
      "Epoch : 250/500  Train_loss : 964.2800293  Test_loss : 855.9291992\n",
      "[./nets/net-250.ckpt] SAVED\n",
      "Epoch : 251/500  Train_loss : 766.0802612  Test_loss : 939.0166016\n",
      "Epoch : 252/500  Train_loss : 816.5141602  Test_loss : 1003.3920898\n",
      "Epoch : 253/500  Train_loss : 963.4137573  Test_loss : 856.3082886\n",
      "Epoch : 254/500  Train_loss : 818.9792480  Test_loss : 925.9754028\n",
      "Epoch : 255/500  Train_loss : 849.5198975  Test_loss : 765.5834351\n",
      "[./nets/net-255.ckpt] SAVED\n",
      "Epoch : 256/500  Train_loss : 900.1225586  Test_loss : 869.4508667\n",
      "Epoch : 257/500  Train_loss : 1063.8470459  Test_loss : 846.0264282\n",
      "Epoch : 258/500  Train_loss : 969.4819336  Test_loss : 833.6249390\n",
      "Epoch : 259/500  Train_loss : 953.2784424  Test_loss : 847.8524170\n",
      "Epoch : 260/500  Train_loss : 883.4708252  Test_loss : 893.7457275\n",
      "[./nets/net-260.ckpt] SAVED\n",
      "Epoch : 261/500  Train_loss : 904.6812744  Test_loss : 922.1094971\n",
      "Epoch : 262/500  Train_loss : 708.8808594  Test_loss : 827.8199463\n",
      "Epoch : 263/500  Train_loss : 811.2704468  Test_loss : 1030.5034180\n",
      "Epoch : 264/500  Train_loss : 736.6730347  Test_loss : 943.1975098\n",
      "Epoch : 265/500  Train_loss : 715.4927979  Test_loss : 801.9864502\n",
      "[./nets/net-265.ckpt] SAVED\n",
      "Epoch : 266/500  Train_loss : 896.0882568  Test_loss : 846.0776978\n",
      "Epoch : 267/500  Train_loss : 810.0003052  Test_loss : 901.7830811\n",
      "Epoch : 268/500  Train_loss : 820.9953613  Test_loss : 837.0169678\n",
      "Epoch : 269/500  Train_loss : 760.7424316  Test_loss : 856.7829590\n",
      "Epoch : 270/500  Train_loss : 868.3306274  Test_loss : 752.7257080\n",
      "[./nets/net-270.ckpt] SAVED\n",
      "Epoch : 271/500  Train_loss : 729.2034912  Test_loss : 808.5433960\n",
      "Epoch : 272/500  Train_loss : 990.6087646  Test_loss : 896.7510986\n",
      "Epoch : 273/500  Train_loss : 715.6087036  Test_loss : 813.7429810\n",
      "Epoch : 274/500  Train_loss : 962.8321533  Test_loss : 919.3190308\n",
      "Epoch : 275/500  Train_loss : 875.4606934  Test_loss : 775.3775024\n",
      "[./nets/net-275.ckpt] SAVED\n",
      "Epoch : 276/500  Train_loss : 822.1892700  Test_loss : 844.6943970\n",
      "Epoch : 277/500  Train_loss : 901.5935669  Test_loss : 787.1571045\n",
      "Epoch : 278/500  Train_loss : 756.0105591  Test_loss : 877.6768799\n",
      "Epoch : 279/500  Train_loss : 850.8061523  Test_loss : 743.5887451\n",
      "Epoch : 280/500  Train_loss : 824.8115234  Test_loss : 873.6096802\n",
      "[./nets/net-280.ckpt] SAVED\n",
      "Epoch : 281/500  Train_loss : 870.5965576  Test_loss : 708.0612793\n",
      "Epoch : 282/500  Train_loss : 767.7374268  Test_loss : 743.5610962\n",
      "Epoch : 283/500  Train_loss : 777.2899170  Test_loss : 1019.7458496\n",
      "Epoch : 284/500  Train_loss : 730.0333252  Test_loss : 795.5195923\n",
      "Epoch : 285/500  Train_loss : 711.4345093  Test_loss : 825.4365234\n",
      "[./nets/net-285.ckpt] SAVED\n",
      "Epoch : 286/500  Train_loss : 847.6130981  Test_loss : 914.9146729\n",
      "Epoch : 287/500  Train_loss : 749.3817139  Test_loss : 872.9501953\n",
      "Epoch : 288/500  Train_loss : 838.1701660  Test_loss : 899.9477539\n",
      "Epoch : 289/500  Train_loss : 763.7565918  Test_loss : 848.4669800\n",
      "Epoch : 290/500  Train_loss : 890.2427979  Test_loss : 724.3109131\n",
      "[./nets/net-290.ckpt] SAVED\n",
      "Epoch : 291/500  Train_loss : 727.8515015  Test_loss : 888.3248291\n",
      "Epoch : 292/500  Train_loss : 732.0758057  Test_loss : 884.0462036\n",
      "Epoch : 293/500  Train_loss : 775.4970093  Test_loss : 763.4022217\n",
      "Epoch : 294/500  Train_loss : 802.6385498  Test_loss : 739.6423340\n",
      "Epoch : 295/500  Train_loss : 943.3466797  Test_loss : 782.0395508\n",
      "[./nets/net-295.ckpt] SAVED\n",
      "Epoch : 296/500  Train_loss : 707.4765625  Test_loss : 829.4295654\n",
      "Epoch : 297/500  Train_loss : 816.3593750  Test_loss : 904.4103394\n",
      "Epoch : 298/500  Train_loss : 763.0986328  Test_loss : 736.4721680\n",
      "Epoch : 299/500  Train_loss : 770.6702271  Test_loss : 813.5128784\n",
      "Epoch : 300/500  Train_loss : 724.2259521  Test_loss : 833.6475830\n",
      "[./nets/net-300.ckpt] SAVED\n",
      "Epoch : 301/500  Train_loss : 1065.1210938  Test_loss : 711.3132324\n",
      "Epoch : 302/500  Train_loss : 834.1350098  Test_loss : 828.4756470\n",
      "Epoch : 303/500  Train_loss : 867.2149658  Test_loss : 828.8773804\n",
      "Epoch : 304/500  Train_loss : 715.1343994  Test_loss : 842.5721436\n",
      "Epoch : 305/500  Train_loss : 818.8483276  Test_loss : 823.3184814\n",
      "[./nets/net-305.ckpt] SAVED\n",
      "Epoch : 306/500  Train_loss : 789.6055298  Test_loss : 853.9167480\n",
      "Epoch : 307/500  Train_loss : 677.1663208  Test_loss : 846.0057373\n",
      "Epoch : 308/500  Train_loss : 846.3765869  Test_loss : 863.5015869\n",
      "Epoch : 309/500  Train_loss : 833.5811768  Test_loss : 866.6157837\n",
      "Epoch : 310/500  Train_loss : 838.7214966  Test_loss : 968.9217529\n",
      "[./nets/net-310.ckpt] SAVED\n",
      "Epoch : 311/500  Train_loss : 759.0545654  Test_loss : 869.7040405\n",
      "Epoch : 312/500  Train_loss : 795.2319336  Test_loss : 862.3887939\n",
      "Epoch : 313/500  Train_loss : 785.6012573  Test_loss : 719.2060547\n",
      "Epoch : 314/500  Train_loss : 739.5460205  Test_loss : 859.2910156\n",
      "Epoch : 315/500  Train_loss : 626.5006104  Test_loss : 881.2587891\n",
      "[./nets/net-315.ckpt] SAVED\n",
      "Epoch : 316/500  Train_loss : 675.7523193  Test_loss : 807.8143921\n",
      "Epoch : 317/500  Train_loss : 754.9812622  Test_loss : 715.5268555\n",
      "Epoch : 318/500  Train_loss : 789.9283447  Test_loss : 778.0642090\n",
      "Epoch : 319/500  Train_loss : 855.9844971  Test_loss : 787.4057007\n",
      "Epoch : 320/500  Train_loss : 772.8450317  Test_loss : 828.4761963\n",
      "[./nets/net-320.ckpt] SAVED\n",
      "Epoch : 321/500  Train_loss : 724.8938599  Test_loss : 787.0861816\n",
      "Epoch : 322/500  Train_loss : 776.4967041  Test_loss : 839.0181885\n",
      "Epoch : 323/500  Train_loss : 824.8742676  Test_loss : 774.2817383\n",
      "Epoch : 324/500  Train_loss : 670.5187988  Test_loss : 708.1276855\n",
      "Epoch : 325/500  Train_loss : 671.9508057  Test_loss : 778.0045166\n",
      "[./nets/net-325.ckpt] SAVED\n",
      "Epoch : 326/500  Train_loss : 759.3691406  Test_loss : 906.6312256\n",
      "Epoch : 327/500  Train_loss : 700.9624023  Test_loss : 803.3325806\n",
      "Epoch : 328/500  Train_loss : 862.2385254  Test_loss : 801.1881104\n",
      "Epoch : 329/500  Train_loss : 876.3664551  Test_loss : 837.3438721\n",
      "Epoch : 330/500  Train_loss : 698.0562744  Test_loss : 860.9732666\n",
      "[./nets/net-330.ckpt] SAVED\n",
      "Epoch : 331/500  Train_loss : 683.3017578  Test_loss : 723.8787842\n",
      "Epoch : 332/500  Train_loss : 675.2135010  Test_loss : 764.6309204\n",
      "Epoch : 333/500  Train_loss : 904.8618774  Test_loss : 813.2855225\n",
      "Epoch : 334/500  Train_loss : 742.7025757  Test_loss : 854.1612549\n",
      "Epoch : 335/500  Train_loss : 734.2690430  Test_loss : 785.3906860\n",
      "[./nets/net-335.ckpt] SAVED\n",
      "Epoch : 336/500  Train_loss : 696.0195312  Test_loss : 847.9599609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 337/500  Train_loss : 812.5161743  Test_loss : 892.0006104\n",
      "Epoch : 338/500  Train_loss : 809.5455322  Test_loss : 741.8823242\n",
      "Epoch : 339/500  Train_loss : 811.6099243  Test_loss : 875.9949951\n",
      "Epoch : 340/500  Train_loss : 763.8227539  Test_loss : 778.9764404\n",
      "[./nets/net-340.ckpt] SAVED\n",
      "Epoch : 341/500  Train_loss : 776.5987549  Test_loss : 895.3458862\n",
      "Epoch : 342/500  Train_loss : 710.0176392  Test_loss : 781.1658936\n",
      "Epoch : 343/500  Train_loss : 785.3560791  Test_loss : 777.9759521\n",
      "Epoch : 344/500  Train_loss : 602.7263794  Test_loss : 834.8350830\n",
      "Epoch : 345/500  Train_loss : 691.8328857  Test_loss : 958.4066162\n",
      "[./nets/net-345.ckpt] SAVED\n",
      "Epoch : 346/500  Train_loss : 811.8183594  Test_loss : 886.8152466\n",
      "Epoch : 347/500  Train_loss : 878.2510376  Test_loss : 791.9460449\n",
      "Epoch : 348/500  Train_loss : 724.7451782  Test_loss : 717.8255615\n",
      "Epoch : 349/500  Train_loss : 751.8057861  Test_loss : 817.1591797\n",
      "Epoch : 350/500  Train_loss : 844.8973999  Test_loss : 876.9810181\n",
      "[./nets/net-350.ckpt] SAVED\n",
      "Epoch : 351/500  Train_loss : 773.9971313  Test_loss : 873.2089233\n",
      "Epoch : 352/500  Train_loss : 814.5759888  Test_loss : 799.2218628\n",
      "Epoch : 353/500  Train_loss : 717.5386963  Test_loss : 947.5375366\n",
      "Epoch : 354/500  Train_loss : 788.6867676  Test_loss : 775.6625977\n",
      "Epoch : 355/500  Train_loss : 937.2849731  Test_loss : 910.1791382\n",
      "[./nets/net-355.ckpt] SAVED\n",
      "Epoch : 356/500  Train_loss : 665.7420654  Test_loss : 762.8272705\n",
      "Epoch : 357/500  Train_loss : 638.3275146  Test_loss : 776.0487671\n",
      "Epoch : 358/500  Train_loss : 741.5167847  Test_loss : 669.6769409\n",
      "Epoch : 359/500  Train_loss : 799.5338135  Test_loss : 765.7534790\n",
      "Epoch : 360/500  Train_loss : 706.8702393  Test_loss : 800.6693726\n",
      "[./nets/net-360.ckpt] SAVED\n",
      "Epoch : 361/500  Train_loss : 843.7513428  Test_loss : 790.0491943\n",
      "Epoch : 362/500  Train_loss : 865.3735352  Test_loss : 823.5240479\n",
      "Epoch : 363/500  Train_loss : 685.1548462  Test_loss : 908.0875244\n",
      "Epoch : 364/500  Train_loss : 782.0108643  Test_loss : 821.4859619\n",
      "Epoch : 365/500  Train_loss : 722.1123047  Test_loss : 935.0149536\n",
      "[./nets/net-365.ckpt] SAVED\n",
      "Epoch : 366/500  Train_loss : 758.8607788  Test_loss : 734.2832031\n",
      "Epoch : 367/500  Train_loss : 776.9160767  Test_loss : 777.5540161\n",
      "Epoch : 368/500  Train_loss : 772.9655762  Test_loss : 917.2413330\n",
      "Epoch : 369/500  Train_loss : 613.6511230  Test_loss : 682.8696899\n",
      "Epoch : 370/500  Train_loss : 856.8600464  Test_loss : 894.8760986\n",
      "[./nets/net-370.ckpt] SAVED\n",
      "Epoch : 371/500  Train_loss : 782.9265137  Test_loss : 743.8173828\n",
      "Epoch : 372/500  Train_loss : 687.0863647  Test_loss : 708.8115845\n",
      "Epoch : 373/500  Train_loss : 608.2864380  Test_loss : 715.2719727\n",
      "Epoch : 374/500  Train_loss : 716.7268677  Test_loss : 738.0099487\n",
      "Epoch : 375/500  Train_loss : 754.6835938  Test_loss : 777.0241089\n",
      "[./nets/net-375.ckpt] SAVED\n",
      "Epoch : 376/500  Train_loss : 696.6762085  Test_loss : 738.4479370\n",
      "Epoch : 377/500  Train_loss : 692.5978394  Test_loss : 920.9867554\n",
      "Epoch : 378/500  Train_loss : 729.6147461  Test_loss : 974.8888550\n",
      "Epoch : 379/500  Train_loss : 601.3181763  Test_loss : 765.1659546\n",
      "Epoch : 380/500  Train_loss : 732.0393677  Test_loss : 687.3428955\n",
      "[./nets/net-380.ckpt] SAVED\n",
      "Epoch : 381/500  Train_loss : 618.8594971  Test_loss : 818.5375977\n",
      "Epoch : 382/500  Train_loss : 561.1430664  Test_loss : 760.8625488\n",
      "Epoch : 383/500  Train_loss : 656.6887817  Test_loss : 758.7993164\n",
      "Epoch : 384/500  Train_loss : 685.0731812  Test_loss : 667.9791870\n",
      "Epoch : 385/500  Train_loss : 676.6699829  Test_loss : 751.6951294\n",
      "[./nets/net-385.ckpt] SAVED\n",
      "Epoch : 386/500  Train_loss : 745.8787842  Test_loss : 743.5202637\n",
      "Epoch : 387/500  Train_loss : 672.0775757  Test_loss : 863.9879150\n",
      "Epoch : 388/500  Train_loss : 841.4942627  Test_loss : 699.0044556\n",
      "Epoch : 389/500  Train_loss : 639.0015259  Test_loss : 774.0318604\n",
      "Epoch : 390/500  Train_loss : 785.7295532  Test_loss : 703.9580688\n",
      "[./nets/net-390.ckpt] SAVED\n",
      "Epoch : 391/500  Train_loss : 761.7843018  Test_loss : 762.0847778\n",
      "Epoch : 392/500  Train_loss : 796.8316650  Test_loss : 961.0173340\n",
      "Epoch : 393/500  Train_loss : 852.6956177  Test_loss : 711.4224854\n",
      "Epoch : 394/500  Train_loss : 707.6077271  Test_loss : 846.1198730\n",
      "Epoch : 395/500  Train_loss : 711.9680176  Test_loss : 629.9600220\n",
      "[./nets/net-395.ckpt] SAVED\n",
      "Epoch : 396/500  Train_loss : 702.7393188  Test_loss : 844.0460205\n",
      "Epoch : 397/500  Train_loss : 803.2581177  Test_loss : 945.6829224\n",
      "Epoch : 398/500  Train_loss : 653.3284302  Test_loss : 691.2747803\n",
      "Epoch : 399/500  Train_loss : 623.3978882  Test_loss : 645.0014038\n",
      "Epoch : 400/500  Train_loss : 755.9591064  Test_loss : 776.7941895\n",
      "[./nets/net-400.ckpt] SAVED\n",
      "Epoch : 401/500  Train_loss : 705.5581055  Test_loss : 800.2722168\n",
      "Epoch : 402/500  Train_loss : 732.4338379  Test_loss : 864.0344849\n",
      "Epoch : 403/500  Train_loss : 787.3272705  Test_loss : 778.4014893\n",
      "Epoch : 404/500  Train_loss : 685.6355591  Test_loss : 745.6240845\n",
      "Epoch : 405/500  Train_loss : 647.3643799  Test_loss : 729.1342163\n",
      "[./nets/net-405.ckpt] SAVED\n",
      "Epoch : 406/500  Train_loss : 720.2334595  Test_loss : 853.6391602\n",
      "Epoch : 407/500  Train_loss : 716.0808105  Test_loss : 772.1034546\n",
      "Epoch : 408/500  Train_loss : 824.0437012  Test_loss : 745.4124146\n",
      "Epoch : 409/500  Train_loss : 806.6013794  Test_loss : 743.2078247\n",
      "Epoch : 410/500  Train_loss : 932.3436279  Test_loss : 911.6721191\n",
      "[./nets/net-410.ckpt] SAVED\n",
      "Epoch : 411/500  Train_loss : 650.4400635  Test_loss : 697.0775146\n",
      "Epoch : 412/500  Train_loss : 751.3397217  Test_loss : 775.8311768\n",
      "Epoch : 413/500  Train_loss : 624.3715820  Test_loss : 865.8334351\n",
      "Epoch : 414/500  Train_loss : 770.2711182  Test_loss : 827.6824951\n",
      "Epoch : 415/500  Train_loss : 636.1765137  Test_loss : 791.2145996\n",
      "[./nets/net-415.ckpt] SAVED\n",
      "Epoch : 416/500  Train_loss : 831.7896118  Test_loss : 651.4036865\n",
      "Epoch : 417/500  Train_loss : 758.3208008  Test_loss : 710.5906372\n",
      "Epoch : 418/500  Train_loss : 694.9415283  Test_loss : 926.3393555\n",
      "Epoch : 419/500  Train_loss : 726.7436523  Test_loss : 931.2812500\n",
      "Epoch : 420/500  Train_loss : 785.9201660  Test_loss : 747.0182495\n",
      "[./nets/net-420.ckpt] SAVED\n",
      "Epoch : 421/500  Train_loss : 678.4158936  Test_loss : 675.0035400\n",
      "Epoch : 422/500  Train_loss : 706.1305542  Test_loss : 628.0435791\n",
      "Epoch : 423/500  Train_loss : 804.6550293  Test_loss : 690.5574951\n",
      "Epoch : 424/500  Train_loss : 724.0641479  Test_loss : 818.3061523\n",
      "Epoch : 425/500  Train_loss : 697.9974976  Test_loss : 899.8654785\n",
      "[./nets/net-425.ckpt] SAVED\n",
      "Epoch : 426/500  Train_loss : 719.3922729  Test_loss : 807.9844360\n",
      "Epoch : 427/500  Train_loss : 706.8598633  Test_loss : 707.2257080\n",
      "Epoch : 428/500  Train_loss : 781.6143188  Test_loss : 872.3297729\n",
      "Epoch : 429/500  Train_loss : 642.1282959  Test_loss : 809.9935913\n",
      "Epoch : 430/500  Train_loss : 665.1941528  Test_loss : 691.2352905\n",
      "[./nets/net-430.ckpt] SAVED\n",
      "Epoch : 431/500  Train_loss : 707.3120117  Test_loss : 882.6920776\n",
      "Epoch : 432/500  Train_loss : 680.1325073  Test_loss : 769.8982544\n",
      "Epoch : 433/500  Train_loss : 811.6715088  Test_loss : 660.8095703\n",
      "Epoch : 434/500  Train_loss : 740.7316284  Test_loss : 672.2389526\n",
      "Epoch : 435/500  Train_loss : 692.5488281  Test_loss : 705.6494751\n",
      "[./nets/net-435.ckpt] SAVED\n",
      "Epoch : 436/500  Train_loss : 729.6479492  Test_loss : 841.9108276\n",
      "Epoch : 437/500  Train_loss : 699.2205200  Test_loss : 871.2402954\n",
      "Epoch : 438/500  Train_loss : 655.6714478  Test_loss : 966.1833496\n",
      "Epoch : 439/500  Train_loss : 684.9727783  Test_loss : 626.9089355\n",
      "Epoch : 440/500  Train_loss : 621.2210693  Test_loss : 664.3562012\n",
      "[./nets/net-440.ckpt] SAVED\n",
      "Epoch : 441/500  Train_loss : 656.6026611  Test_loss : 724.2643433\n",
      "Epoch : 442/500  Train_loss : 697.7796021  Test_loss : 676.5740967\n",
      "Epoch : 443/500  Train_loss : 850.9405518  Test_loss : 686.9433594\n",
      "Epoch : 444/500  Train_loss : 834.4489136  Test_loss : 691.6130371\n",
      "Epoch : 445/500  Train_loss : 803.2598877  Test_loss : 727.2250366\n",
      "[./nets/net-445.ckpt] SAVED\n",
      "Epoch : 446/500  Train_loss : 721.8811646  Test_loss : 851.2698364\n",
      "Epoch : 447/500  Train_loss : 667.2154541  Test_loss : 653.5507812\n",
      "Epoch : 448/500  Train_loss : 797.0244141  Test_loss : 814.6641846\n",
      "Epoch : 449/500  Train_loss : 681.2643433  Test_loss : 666.9398804\n",
      "Epoch : 450/500  Train_loss : 698.0816040  Test_loss : 794.8852539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[./nets/net-450.ckpt] SAVED\n",
      "Epoch : 451/500  Train_loss : 715.9310913  Test_loss : 829.0169678\n",
      "Epoch : 452/500  Train_loss : 775.9942627  Test_loss : 807.6539917\n",
      "Epoch : 453/500  Train_loss : 637.9610596  Test_loss : 622.7006836\n",
      "Epoch : 454/500  Train_loss : 784.5960693  Test_loss : 706.9374390\n",
      "Epoch : 455/500  Train_loss : 719.5009766  Test_loss : 655.6868286\n",
      "[./nets/net-455.ckpt] SAVED\n",
      "Epoch : 456/500  Train_loss : 684.6130981  Test_loss : 757.6189575\n",
      "Epoch : 457/500  Train_loss : 681.7459106  Test_loss : 653.5174561\n",
      "Epoch : 458/500  Train_loss : 663.4273682  Test_loss : 668.8614502\n",
      "Epoch : 459/500  Train_loss : 579.7672119  Test_loss : 818.6263428\n",
      "Epoch : 460/500  Train_loss : 629.8052979  Test_loss : 723.0996704\n",
      "[./nets/net-460.ckpt] SAVED\n",
      "Epoch : 461/500  Train_loss : 602.3751831  Test_loss : 650.1722412\n",
      "Epoch : 462/500  Train_loss : 641.3475952  Test_loss : 774.3695068\n",
      "Epoch : 463/500  Train_loss : 905.0561523  Test_loss : 700.2164307\n",
      "Epoch : 464/500  Train_loss : 833.9270020  Test_loss : 682.1580200\n",
      "Epoch : 465/500  Train_loss : 737.7545166  Test_loss : 786.5610352\n",
      "[./nets/net-465.ckpt] SAVED\n",
      "Epoch : 466/500  Train_loss : 566.7492676  Test_loss : 745.5263062\n",
      "Epoch : 467/500  Train_loss : 831.8378906  Test_loss : 594.5842896\n",
      "Epoch : 468/500  Train_loss : 781.4121094  Test_loss : 706.6231689\n",
      "Epoch : 469/500  Train_loss : 611.8852539  Test_loss : 699.7443237\n",
      "Epoch : 470/500  Train_loss : 597.3214111  Test_loss : 694.5648193\n",
      "[./nets/net-470.ckpt] SAVED\n",
      "Epoch : 471/500  Train_loss : 700.6508789  Test_loss : 635.5281982\n",
      "Epoch : 472/500  Train_loss : 749.1021729  Test_loss : 721.9854736\n",
      "Epoch : 473/500  Train_loss : 670.5904541  Test_loss : 773.3742676\n",
      "Epoch : 474/500  Train_loss : 697.4816895  Test_loss : 765.4324341\n",
      "Epoch : 475/500  Train_loss : 687.1094971  Test_loss : 663.0479736\n",
      "[./nets/net-475.ckpt] SAVED\n",
      "Epoch : 476/500  Train_loss : 746.7821655  Test_loss : 651.8089600\n",
      "Epoch : 477/500  Train_loss : 677.1068726  Test_loss : 739.3032227\n",
      "Epoch : 478/500  Train_loss : 852.8010254  Test_loss : 697.4079590\n",
      "Epoch : 479/500  Train_loss : 733.4028931  Test_loss : 756.1875000\n",
      "Epoch : 480/500  Train_loss : 730.1939087  Test_loss : 814.6950073\n",
      "[./nets/net-480.ckpt] SAVED\n",
      "Epoch : 481/500  Train_loss : 707.0914307  Test_loss : 804.7748413\n",
      "Epoch : 482/500  Train_loss : 708.8222656  Test_loss : 519.8563232\n",
      "Epoch : 483/500  Train_loss : 622.5404663  Test_loss : 693.3988037\n",
      "Epoch : 484/500  Train_loss : 679.8748779  Test_loss : 762.6315918\n",
      "Epoch : 485/500  Train_loss : 795.3669434  Test_loss : 652.4517822\n",
      "[./nets/net-485.ckpt] SAVED\n",
      "Epoch : 486/500  Train_loss : 729.3019409  Test_loss : 737.0834961\n",
      "Epoch : 487/500  Train_loss : 620.2468872  Test_loss : 668.5241699\n",
      "Epoch : 488/500  Train_loss : 624.3503418  Test_loss : 680.7925415\n",
      "Epoch : 489/500  Train_loss : 632.3290405  Test_loss : 745.6998901\n",
      "Epoch : 490/500  Train_loss : 654.2576294  Test_loss : 594.7030640\n",
      "[./nets/net-490.ckpt] SAVED\n",
      "Epoch : 491/500  Train_loss : 780.0601807  Test_loss : 641.0648804\n",
      "Epoch : 492/500  Train_loss : 705.7987671  Test_loss : 689.2462769\n",
      "Epoch : 493/500  Train_loss : 734.0856323  Test_loss : 725.5426025\n",
      "Epoch : 494/500  Train_loss : 671.8546143  Test_loss : 770.3848877\n",
      "Epoch : 495/500  Train_loss : 706.8084106  Test_loss : 778.0697021\n",
      "[./nets/net-495.ckpt] SAVED\n",
      "Epoch : 496/500  Train_loss : 760.5108643  Test_loss : 694.5324097\n",
      "Epoch : 497/500  Train_loss : 645.0307617  Test_loss : 715.5498657\n",
      "Epoch : 498/500  Train_loss : 722.4999390  Test_loss : 650.8034058\n",
      "Epoch : 499/500  Train_loss : 673.8725586  Test_loss : 729.1382446\n",
      "Epoch : 500/500  Train_loss : 714.1022949  Test_loss : 575.5933838\n",
      "[./nets/net-500.ckpt] SAVED\n",
      "Optimization Finished\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "training_epochs = FLAGS.training_epochs\n",
    "batch_size = FLAGS.batch_size\n",
    "display_step = FLAGS.display_step\n",
    "# Plot\n",
    "n_plot = 5    # plot 5 images\n",
    "train_disp_idx = np.random.randint(mnist.train.num_examples, size=n_plot)    # fixed during train time\n",
    "test_disp_idx = np.random.randint(mnist.test.num_examples, size=n_plot)\n",
    "\n",
    "# Initialize\n",
    "sess = tf.Session(config=config)\n",
    "sess.run(init)\n",
    "\n",
    "# Optimize\n",
    "for epoch in range(training_epochs):\n",
    "    total_cost = 0.\n",
    "    n_total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    rand_train_idx = np.random.randint(mnist.train.num_examples, size=batch_size)\n",
    "    rand_test_idx = np.random.randint(mnist.test.num_examples, size=batch_size)\n",
    "    \n",
    "    # Iteration\n",
    "    for i in range(n_total_batch):\n",
    "        batch_pure, _ = mnist.train.next_batch(batch_size)    # pure image\n",
    "        noise = noise_batch(batch_size)    # random noise\n",
    "        batch_crpt = occl(batch_pure, noise)   # corrupted image \n",
    "        feeds = {ph_pure: batch_pure, ph_noise: noise, ph_crpt: batch_crpt}\n",
    "        sess.run(optm, feed_dict=feeds)\n",
    "        #total_cost += sess.run(loss, feed_dict=feeds)\n",
    "    #total_cost = total_cost / mnist.train.num_examples\n",
    "    \n",
    "    train_pure = train_img[rand_train_idx]    # pure image\n",
    "    train_noise = noise_batch(batch_size)    # random noise\n",
    "    train_crpt = occl(train_pure,train_noise)   # corrupted image\n",
    "    train_feeds = {ph_pure: train_pure, ph_noise: train_noise, ph_crpt: train_crpt}\n",
    "    train_loss, tb_train_loss = sess.run([loss,_train_loss], feed_dict=train_feeds)\n",
    "    \n",
    "    test_pure = test_img[rand_test_idx]    # pure image\n",
    "    test_noise = noise_batch(batch_size)    # random noise\n",
    "    test_crpt = occl(test_pure,test_noise)   # corrupted image\n",
    "    test_feeds = {ph_pure: test_pure, ph_noise: test_noise, ph_crpt: test_crpt}\n",
    "    test_loss, tb_test_loss = sess.run([loss,_test_loss], feed_dict=test_feeds)\n",
    "\n",
    "    writer.add_summary(tb_train_loss, epoch)\n",
    "    writer.add_summary(tb_test_loss, epoch)\n",
    "    print(\"Epoch : %03d/%03d  Train_loss : %.7f  Test_loss : %.7f\" \n",
    "          % (epoch+1, training_epochs, train_loss, test_loss))   \n",
    "        \n",
    "    # Display\n",
    "    if (epoch+1) % display_step == 0:\n",
    "        train_gt_pure = train_img[train_disp_idx]    # pure image\n",
    "        train_gt_noise = noise_batch(n_plot)    # random noise\n",
    "        train_gt_crpt = occl(train_gt_pure,train_gt_noise)   # corrupted image\n",
    "        train_gt_feeds = {ph_pure: train_gt_pure, ph_noise: train_gt_noise, ph_crpt: train_gt_crpt}\n",
    "        \n",
    "        test_gt_pure = test_img[test_disp_idx]    # pure image\n",
    "        test_gt_noise = noise_batch(n_plot)    # random noise\n",
    "        test_gt_crpt = occl(test_gt_pure,test_gt_noise)   # corrupted image\n",
    "        test_gt_feeds = {ph_pure: test_gt_pure, ph_noise: test_gt_noise, ph_crpt: test_gt_crpt}\n",
    "        \n",
    "        # generated images\n",
    "        train_gen_pure, train_gen_noise, train_gen_crpt = sess.run([core_gen, shell2_gen, full_gen], \\\n",
    "                                        feed_dict=train_gt_feeds)  # 784-d vector\n",
    "        test_gen_pure, test_gen_noise, test_gen_crpt = sess.run([core_gen, shell2_gen, full_gen], \\\n",
    "                                        feed_dict=test_gt_feeds)  # 784-d vector\n",
    "        \n",
    "        # plotting results from training data\n",
    "        fig, axes = plt.subplots(nrows=4, ncols=n_plot, figsize=(10,2*n_plot))   # displaying 4*n_plot images\n",
    "        plt.setp(axes, xticks=np.arange(0,27,7), yticks=np.arange(0,27,7)) \n",
    "        for j in range(n_plot):\n",
    "            train_disp_gt_crpt = np.reshape(train_gt_crpt[j], [28,28])    # 28x28\n",
    "            axes[0, j].imshow(train_disp_gt_crpt, cmap='gray')   \n",
    "            axes[0, j].set(ylabel='gt_crpt')\n",
    "            axes[0, j].label_outer()\n",
    "            \n",
    "            train_disp_gen_pure = np.reshape(train_gen_pure[j], [28,28])    # 28x28\n",
    "            axes[1, j].imshow(train_disp_gen_pure, cmap='gray')   \n",
    "            axes[1, j].set(ylabel='gen_pure')\n",
    "            axes[1, j].label_outer()\n",
    "            \n",
    "            train_disp_gen_noise = np.reshape(train_gen_noise[j], [28,28])    # 28x28\n",
    "            axes[2, j].imshow(train_disp_gen_noise, cmap='gray')   \n",
    "            axes[2, j].set(ylabel='gen_noise')\n",
    "            axes[2, j].label_outer()\n",
    "            \n",
    "            train_disp_gen_crpt = np.reshape(train_gen_crpt[j], [28,28])    # 28x28\n",
    "            axes[3, j].imshow(train_disp_gen_crpt, cmap='gray')   \n",
    "            axes[3, j].set(ylabel='gen_crpt')\n",
    "            axes[3, j].label_outer()\n",
    "            \n",
    "#             train_disp_gt_pure = np.reshape(train_gt_pure[j], [28,28])\n",
    "#             axes[0, j].imshow(train_disp_gt_pure, cmap='gray')\n",
    "#             axes[0, j].set(ylabel='gt_pure')\n",
    "#             axes[0, j].label_outer()\n",
    "            \n",
    "#             train_disp_gt_noise = np.reshape(train_gt_noise[j], [28,28])    # 28x28\n",
    "#             axes[2, j].imshow(train_disp_gt_noise, cmap='gray')   \n",
    "#             axes[2, j].set(ylabel='gt_noise')\n",
    "#             axes[2, j].label_outer()\n",
    "                    \n",
    "        plt.savefig(outputdir+'/train/epoch %03d' %(epoch+1))    \n",
    "        plt.close(fig)\n",
    "        \n",
    "        # plotting results from testing data\n",
    "        fig, axes = plt.subplots(nrows=4, ncols=n_plot, figsize=(10,2*n_plot))   # displaying 4*n_plot images\n",
    "        plt.setp(axes, xticks=np.arange(0,27,7), yticks=np.arange(0,27,7)) \n",
    "        for k in range(n_plot):\n",
    "            test_disp_gt_crpt = np.reshape(test_gt_crpt[k], [28,28])    # 28x28\n",
    "            axes[0, k].imshow(test_disp_gt_crpt, cmap='gray')   \n",
    "            axes[0, k].set(ylabel='gt_crpt')\n",
    "            axes[0, k].label_outer()\n",
    "            \n",
    "            test_disp_gen_pure = np.reshape(test_gen_pure[k], [28,28])    # 28x28\n",
    "            axes[1, k].imshow(test_disp_gen_pure, cmap='gray')   \n",
    "            axes[1, k].set(ylabel='gen_pure')\n",
    "            axes[1, k].label_outer()           \n",
    "            \n",
    "            test_disp_gen_noise = np.reshape(test_gen_noise[k], [28,28])    # 28x28\n",
    "            axes[2, k].imshow(test_disp_gen_noise, cmap='gray')   \n",
    "            axes[2, k].set(ylabel='gen_noise')\n",
    "            axes[2, k].label_outer()\n",
    "            \n",
    "            test_disp_gen_crpt = np.reshape(test_gen_crpt[k], [28,28])    # 28x28\n",
    "            axes[3, k].imshow(test_disp_gen_crpt, cmap='gray')   \n",
    "            axes[3, k].set(ylabel='gen_crpt')\n",
    "            axes[3, k].label_outer()\n",
    "            \n",
    "#             test_disp_gt_pure = np.reshape(test_gt_pure[k], [28,28])\n",
    "#             axes[0, k].imshow(test_disp_gt_pure, cmap='gray')\n",
    "#             axes[0, k].set(ylabel='gt_pure')\n",
    "#             axes[0, k].label_outer()\n",
    "            \n",
    "#             test_disp_gt_noise = np.reshape(test_gt_noise[k], [28,28])    # 28x28\n",
    "#             axes[2, k].imshow(test_disp_gt_noise, cmap='gray')   \n",
    "#             axes[2, k].set(ylabel='gt_noise')\n",
    "#             axes[2, k].label_outer()\n",
    "                    \n",
    "        plt.savefig(outputdir+'/test/epoch %03d' %(epoch+1))    \n",
    "        plt.close(fig)\n",
    "        \n",
    "        # Save\n",
    "        if (epoch+1) % FLAGS.save_step ==0:\n",
    "            savename = savedir+\"/net-\"+str(epoch+1)+\".ckpt\"\n",
    "            saver.save(sess, savename)\n",
    "            print(\"[%s] SAVED\" % (savename))\n",
    "\n",
    "print(\"Optimization Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
