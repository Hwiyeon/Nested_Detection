{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current version [1.3.1]\n",
      "Packages Loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)  ## just for ignore DeprcationWarning message\n",
    "print(\"Current version [%s]\" %(tf.__version__))\n",
    "print(\"Packages Loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLAGS READY\n"
     ]
    }
   ],
   "source": [
    "# Dataset Configurations\n",
    "tf.app.flags.DEFINE_integer('img_size', 28, \"\"\"Image size of MNIST dataset\"\"\")\n",
    "\n",
    "# Network Configurations\n",
    "tf.app.flags.DEFINE_integer('batch_size', 100, \"\"\"Number of images to process in a batch\"\"\")\n",
    "tf.app.flags.DEFINE_float('l1_ratio', 0.5, \"\"\"Ratio of level1\"\"\")\n",
    "tf.app.flags.DEFINE_float('l2_ratio', 0.5, \"\"\"Ratio of level2\"\"\")\n",
    "\n",
    "# Optimization Configurations\n",
    "tf.app.flags.DEFINE_float('lr', 0.001, \"\"\"Learning rate\"\"\")\n",
    "\n",
    "# Training Configurations\n",
    "tf.app.flags.DEFINE_integer('training_epochs', 500, \"\"\"Number of epochs to run\"\"\")\n",
    "tf.app.flags.DEFINE_integer('display_step', 5, \"\"\"Number of iterations to display training output\"\"\")\n",
    "tf.app.flags.DEFINE_integer('save_step', 5, \"\"\"Number of interations to save checkpoint\"\"\")\n",
    "tf.app.flags.DEFINE_integer('save_max', 5, \"\"\"Number of checkpoints to remain\"\"\")\n",
    "\n",
    "\n",
    "# Save Configurations\n",
    "tf.app.flags.DEFINE_string('nets', './nets', \"\"\"Directory where to write the checkpoints\"\"\")\n",
    "tf.app.flags.DEFINE_string('outputs', './outputs', \"\"\"Directory where to save the output images\"\"\")\n",
    "tf.app.flags.DEFINE_string('tboard', './tensorboard', \"\"\"Directory where to save the tensorboard logs\"\"\")\n",
    "\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "print(\"FLAGS READY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.allow_soft_placement = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../data/train-images-idx3-ubyte.gz\n",
      "Extracting ../../data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../../data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../data/t10k-labels-idx1-ubyte.gz\n",
      "MNIST ready\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('../../data/', one_hot=True)\n",
    "train_img = mnist.train.images\n",
    "train_label = mnist.train.labels\n",
    "test_img = mnist.test.images\n",
    "test_label = mnist.test.labels\n",
    "print(\"MNIST ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating random noise mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise_mask(prob=0.5):\n",
    "    mask = np.zeros([FLAGS.img_size, FLAGS.img_size])\n",
    "    rd = np.random.random()\n",
    "    if rd > prob:\n",
    "        # threshold of the size of masks\n",
    "        uthd = FLAGS.img_size    \n",
    "        lthd = 0     \n",
    "        # mask size should be beween 14x14, 5x5\n",
    "        while(uthd>14 or lthd<5):\n",
    "            ver1 = np.random.random_integers(0, FLAGS.img_size-1, size= 2)   # vertex1\n",
    "            ver2 = np.random.random_integers(0, FLAGS.img_size-1, size= 2)    # vertex2\n",
    "            uthd = np.maximum(np.abs(ver1[0]-ver2[0]), np.abs(ver1[1]-ver2[1]))    # upperbound\n",
    "            lthd = np.minimum(np.abs(ver1[0]-ver2[0]), np.abs(ver1[1]-ver2[1]))    # lowerbound\n",
    "        xmin = np.minimum(ver1[0], ver2[0])    # left x value\n",
    "        xmax = np.maximum(ver1[0], ver2[0])    # right x value\n",
    "        ymin = np.minimum(ver1[1], ver2[1])    # top y value\n",
    "        ymax = np.maximum(ver1[1], ver2[1])    # bottom y value\n",
    "        noise = np.random.random((xmax-xmin+1, ymax-ymin+1))    # random sample in [0,1]\n",
    "        mask[xmin:xmax+1, ymin:ymax+1] = noise    # noise mask with location\n",
    "        mask_meta = [xmin, xmax, ymin, ymax, noise, mask]\n",
    "    mask = np.reshape(mask, [-1])\n",
    "    return mask\n",
    "\n",
    "def noise_batch(batch_num):\n",
    "    # make random noise batch\n",
    "    mask_batch = np.zeros([batch_num, FLAGS.img_size*FLAGS.img_size])\n",
    "    for i in range(batch_num):\n",
    "        mask_batch[i,:] = noise_mask()\n",
    "    return mask_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Occlusion generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def occl(target, disturb):\n",
    "    mask = (disturb==0).astype(float)\n",
    "    masked_target = np.multiply(target, mask)\n",
    "    crpt = np.add(masked_target, disturb)\n",
    "    return crpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nested MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _nested_mlp(l1, l2_s, l2, out_channel, name, stddev=0.1, is_init=False, is_last=False):\n",
    "    l1_shape = l1.get_shape()[1]\n",
    "    l2_shape = l2.get_shape()[1]\n",
    "    l2_s_shape = l2_s.get_shape()[1]\n",
    "    \n",
    "    if is_init:\n",
    "        # input is the input image\n",
    "        with tf.device('/CPU:0'):\n",
    "            with tf.variable_scope('level1'):\n",
    "                with tf.variable_scope(name):\n",
    "                    l1_weights = tf.get_variable('weights', \n",
    "                                                 [l1_shape, out_channel*FLAGS.l1_ratio], \n",
    "                                                 tf.float32, \n",
    "                                                 initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "                    l1_biases = tf.get_variable('biases', \n",
    "                                                [out_channel*FLAGS.l1_ratio],\n",
    "                                                tf.float32, \n",
    "                                                initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "            with tf.variable_scope('level2'):\n",
    "                with tf.variable_scope(name):\n",
    "                    l2_s_weights = tf.get_variable('weights_shell', \n",
    "                                                 [l2_s_shape, out_channel*FLAGS.l2_ratio], \n",
    "                                                 tf.float32, \n",
    "                                                 initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "                    l2_s_biases = tf.get_variable('biases_shell', \n",
    "                                                [out_channel*FLAGS.l2_ratio],\n",
    "                                                tf.float32, \n",
    "                                                initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "                    \n",
    "\n",
    "        l1_mlp = tf.nn.sigmoid(tf.add(tf.matmul(l1, l1_weights), l1_biases))\n",
    "        l2_s_mlp = tf.nn.sigmoid(tf.add(tf.matmul(l2_s, l2_s_weights), l2_s_biases))\n",
    "        l2_mlp = tf.concat((l1_mlp, l2_s_mlp), 1)\n",
    "    \n",
    "    elif is_last:\n",
    "        # output is the generated image\n",
    "        with tf.device('/CPU:0'):\n",
    "            with tf.variable_scope('level1'):\n",
    "                with tf.variable_scope(name):\n",
    "                    l1_weights = tf.get_variable('weights', \n",
    "                                                 [l1_shape, out_channel], \n",
    "                                                 tf.float32, \n",
    "                                                 initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "                    l1_biases = tf.get_variable('biases', \n",
    "                                                [out_channel],\n",
    "                                                tf.float32, \n",
    "                                                initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "            with tf.variable_scope('level2'):\n",
    "                with tf.variable_scope(name):\n",
    "                    l2_s_weights = tf.get_variable('weights_shell', \n",
    "                                                 [l2_s_shape, out_channel], \n",
    "                                                 tf.float32, \n",
    "                                                 initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "                    l2_s_biases = tf.get_variable('biases_shell', \n",
    "                                                [out_channel],\n",
    "                                                tf.float32, \n",
    "                                                initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "                    l2_weights = tf.get_variable('weights', \n",
    "                                                 [l2_shape, out_channel], \n",
    "                                                 tf.float32, \n",
    "                                                 initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "                    l2_biases = tf.get_variable('biases', \n",
    "                                                [out_channel],\n",
    "                                                tf.float32, \n",
    "                                                initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "                    \n",
    "\n",
    "        l1_mlp = tf.nn.sigmoid(tf.add(tf.matmul(l1, l1_weights), l1_biases))\n",
    "        l2_s_mlp = tf.nn.sigmoid(tf.add(tf.matmul(l2_s, l2_s_weights), l2_s_biases))\n",
    "        l2_mlp = tf.nn.sigmoid(tf.add(tf.matmul(l2, l2_weights), l2_biases))\n",
    "                                 \n",
    "    else:\n",
    "        with tf.device('/CPU:0'):\n",
    "            with tf.variable_scope('level1'):\n",
    "                with tf.variable_scope(name):\n",
    "                    l1_weights = tf.get_variable('weights', \n",
    "                                                 [l1_shape, out_channel*FLAGS.l1_ratio], \n",
    "                                                 tf.float32, \n",
    "                                                 initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "                    l1_biases = tf.get_variable('biases', \n",
    "                                                [out_channel*FLAGS.l1_ratio],\n",
    "                                                tf.float32, \n",
    "                                                initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "            with tf.variable_scope('level2'):\n",
    "                with tf.variable_scope(name):\n",
    "                    l2_s_weights = tf.get_variable('weights_shell', \n",
    "                                                   [l2_s_shape, out_channel*FLAGS.l2_ratio], \n",
    "                                                   tf.float32, \n",
    "                                                   initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "                    l2_s_biases = tf.get_variable('biases_shell', \n",
    "                                                  [out_channel*FLAGS.l2_ratio],\n",
    "                                                  tf.float32, \n",
    "                                                  initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "                    l2_weights_1 = tf.get_variable('weights_1', \n",
    "                                                   [l2_s_shape, out_channel*FLAGS.l1_ratio], \n",
    "                                                   tf.float32, \n",
    "                                                   initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "                    l2_biases_1 = tf.get_variable('biases_1', \n",
    "                                                  [out_channel*FLAGS.l1_ratio],\n",
    "                                                  tf.float32, \n",
    "                                                  initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "                    l2_weights_2 = tf.get_variable('weights_2', \n",
    "                                                   [l1_shape, out_channel*FLAGS.l2_ratio], \n",
    "                                                   tf.float32, \n",
    "                                                   initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "                    l2_biases_2 = tf.get_variable('biases_2', \n",
    "                                                  [out_channel*FLAGS.l2_ratio],\n",
    "                                                  tf.float32, \n",
    "                                                  initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "\n",
    "        l1_mlp_r = tf.add(tf.matmul(l1, l1_weights), l1_biases)\n",
    "        l1_mlp = tf.nn.sigmoid(l1_mlp_r)\n",
    "        \n",
    "        l2_s_mlp = tf.nn.sigmoid(tf.add(tf.matmul(l2_s, l2_s_weights), l2_s_biases))\n",
    "        \n",
    "        l2_mlp_1_r = tf.add(tf.matmul(l2[:,l1_shape:l2_shape], l2_weights_1), l2_biases_1)\n",
    "        l2_mlp_1 = tf.nn.sigmoid(tf.add(l1_mlp_r, l2_mlp_1_r))\n",
    "        l2_mlp_2_r = tf.add(tf.matmul(l2[:,:l1_shape], l2_weights_2), l2_biases_2)\n",
    "        l2_mlp_3_r = tf.add(tf.matmul(l2[:,l1_shape:l2_shape], l2_s_weights), l2_s_biases)\n",
    "        l2_mlp_2 = tf.nn.sigmoid(tf.add(l2_mlp_2_r, l2_mlp_3_r))\n",
    "        l2_mlp = tf.concat((l2_mlp_1, l2_mlp_2), 1)\n",
    "        \n",
    "        \n",
    "    return l1_mlp, l2_s_mlp, l2_mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graphs Ready\n"
     ]
    }
   ],
   "source": [
    "# Network Topology\n",
    "n_input = FLAGS.img_size*FLAGS.img_size\n",
    "n_enc1 = 1024\n",
    "n_enc2 = 512\n",
    "n_enc3 = 256\n",
    "n_dec1 = 512\n",
    "n_dec2 = 1024\n",
    "n_out = 784\n",
    "\n",
    "# Inputs and Outputs\n",
    "ph_pure = tf.placeholder(\"float\", [None, n_input])    # pure image --- core\n",
    "ph_noise= tf.placeholder(\"float\", [None, n_input])    # noise --- shell1\n",
    "ph_crpt = tf.placeholder(\"float\", [None, n_input])    # corrupted image   --- level2\n",
    "\n",
    "\n",
    "# Model\n",
    "def nested_ae_mlp(_X):\n",
    "    l1_enc1, l2_s_enc1, l2_enc1 = _nested_mlp(_X, _X, _X, n_enc1, name='enc1', is_init=True)\n",
    "    l1_enc2, l2_s_enc2, l2_enc2 = _nested_mlp(l1_enc1, l2_s_enc1, l2_enc1, n_enc2, name='enc2')\n",
    "    l1_enc3, l2_s_enc3, l2_enc3 = _nested_mlp(l1_enc2, l2_s_enc2, l2_enc2, n_enc3, name='enc3')\n",
    "    l1_dec1, l2_s_dec1, l2_dec1 = _nested_mlp(l1_enc3, l2_s_enc3, l2_enc3, n_dec1, name='dec1')\n",
    "    l1_dec2, l2_s_dec2, l2_dec2 = _nested_mlp(l1_dec1, l2_s_dec1, l2_dec1, n_dec2, name='dec2')\n",
    "    l1_out, l2_s_out, l2_out = _nested_mlp(l1_dec2, l2_s_dec2, l2_dec2, n_out, name='out',is_last=True)\n",
    "    return l1_out, l2_s_out, l2_out\n",
    "\n",
    "# Generation\n",
    "core_gen, shell2_gen, full_gen = nested_ae_mlp(ph_crpt)   # [None, n_input]\n",
    "\n",
    "# Loss & Optimizer\n",
    "with tf.name_scope(\"loss\") as scope:\n",
    "    loss = tf.reduce_mean(tf.nn.l2_loss(full_gen-ph_crpt)) + 2*tf.reduce_mean(tf.nn.l2_loss(core_gen-ph_pure))\\\n",
    "            + tf.reduce_mean(tf.nn.l2_loss(shell2_gen-ph_noise))\n",
    "    _train_loss = tf.summary.scalar(\"train_loss\", loss)\n",
    "    _test_loss = tf.summary.scalar(\"test_loss\", loss)\n",
    "\n",
    "optm = tf.train.AdamOptimizer(learning_rate=FLAGS.lr).minimize(loss)\n",
    "\n",
    "\n",
    "print(\"Graphs Ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize Ready\n"
     ]
    }
   ],
   "source": [
    "merged = tf.summary.merge_all()\n",
    "tensorboard_path = FLAGS.tboard\n",
    "if not os.path.exists(tensorboard_path):\n",
    "    os.makedirs(tensorboard_path)\n",
    "writer = tf.summary.FileWriter(tensorboard_path)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "print(\"Initialize Ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saver ready\n"
     ]
    }
   ],
   "source": [
    "outputdir = FLAGS.outputs\n",
    "if not os.path.exists(outputdir+'/train'):\n",
    "    os.makedirs(outputdir+'/train')\n",
    "\n",
    "if not os.path.exists(outputdir+'/test'):\n",
    "    os.makedirs(outputdir+'/test')\n",
    "    \n",
    "savedir = FLAGS.nets\n",
    "if not os.path.exists(savedir):\n",
    "    os.makedirs(savedir)\n",
    "    \n",
    "saver = tf.train.Saver(max_to_keep=FLAGS.save_max)\n",
    "print(\"Saver ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 001/500  Train_loss : 5454.3647461  Test_loss : 5012.3193359\n",
      "Epoch : 002/500  Train_loss : 4255.8417969  Test_loss : 3928.7670898\n",
      "Epoch : 003/500  Train_loss : 3372.7214355  Test_loss : 3534.4455566\n",
      "Epoch : 004/500  Train_loss : 3281.3063965  Test_loss : 3327.9497070\n",
      "Epoch : 005/500  Train_loss : 2920.5751953  Test_loss : 2762.7153320\n",
      "[./nets/net-5.ckpt] SAVED\n",
      "Epoch : 006/500  Train_loss : 2775.1901855  Test_loss : 2521.0156250\n",
      "Epoch : 007/500  Train_loss : 2335.6823730  Test_loss : 2475.6821289\n",
      "Epoch : 008/500  Train_loss : 2889.8679199  Test_loss : 2621.0732422\n",
      "Epoch : 009/500  Train_loss : 2148.3845215  Test_loss : 2323.3823242\n",
      "Epoch : 010/500  Train_loss : 2199.6513672  Test_loss : 2189.1513672\n",
      "[./nets/net-10.ckpt] SAVED\n",
      "Epoch : 011/500  Train_loss : 2280.5869141  Test_loss : 2218.4072266\n",
      "Epoch : 012/500  Train_loss : 2221.2485352  Test_loss : 2119.8720703\n",
      "Epoch : 013/500  Train_loss : 1957.3471680  Test_loss : 2038.6566162\n",
      "Epoch : 014/500  Train_loss : 1915.5449219  Test_loss : 2039.2832031\n",
      "Epoch : 015/500  Train_loss : 1643.8928223  Test_loss : 1884.0380859\n",
      "[./nets/net-15.ckpt] SAVED\n",
      "Epoch : 016/500  Train_loss : 1651.1398926  Test_loss : 2130.2409668\n",
      "Epoch : 017/500  Train_loss : 1901.7851562  Test_loss : 1864.8208008\n",
      "Epoch : 018/500  Train_loss : 1822.0590820  Test_loss : 1775.9675293\n",
      "Epoch : 019/500  Train_loss : 1947.2064209  Test_loss : 1801.8964844\n",
      "Epoch : 020/500  Train_loss : 1740.2770996  Test_loss : 1808.4501953\n",
      "[./nets/net-20.ckpt] SAVED\n",
      "Epoch : 021/500  Train_loss : 1721.3227539  Test_loss : 1794.1069336\n",
      "Epoch : 022/500  Train_loss : 1905.7772217  Test_loss : 1654.0275879\n",
      "Epoch : 023/500  Train_loss : 1641.5463867  Test_loss : 1813.5676270\n",
      "Epoch : 024/500  Train_loss : 1720.1956787  Test_loss : 1677.6656494\n",
      "Epoch : 025/500  Train_loss : 1578.4119873  Test_loss : 1616.3144531\n",
      "[./nets/net-25.ckpt] SAVED\n",
      "Epoch : 026/500  Train_loss : 1802.4521484  Test_loss : 1890.9340820\n",
      "Epoch : 027/500  Train_loss : 1834.4377441  Test_loss : 1825.2592773\n",
      "Epoch : 028/500  Train_loss : 1589.2034912  Test_loss : 1690.3903809\n",
      "Epoch : 029/500  Train_loss : 1615.9733887  Test_loss : 1430.9237061\n",
      "Epoch : 030/500  Train_loss : 1482.9786377  Test_loss : 1674.7800293\n",
      "[./nets/net-30.ckpt] SAVED\n",
      "Epoch : 031/500  Train_loss : 1529.8996582  Test_loss : 1633.0059814\n",
      "Epoch : 032/500  Train_loss : 1538.1574707  Test_loss : 1403.3948975\n",
      "Epoch : 033/500  Train_loss : 1568.0712891  Test_loss : 1552.5273438\n",
      "Epoch : 034/500  Train_loss : 1602.8876953  Test_loss : 1489.6905518\n",
      "Epoch : 035/500  Train_loss : 1466.7259521  Test_loss : 1610.4747314\n",
      "[./nets/net-35.ckpt] SAVED\n",
      "Epoch : 036/500  Train_loss : 1270.1791992  Test_loss : 1390.2958984\n",
      "Epoch : 037/500  Train_loss : 1524.1906738  Test_loss : 1471.2800293\n",
      "Epoch : 038/500  Train_loss : 1438.1152344  Test_loss : 1552.6557617\n",
      "Epoch : 039/500  Train_loss : 1581.5651855  Test_loss : 1307.9416504\n",
      "Epoch : 040/500  Train_loss : 1476.6557617  Test_loss : 1410.4448242\n",
      "[./nets/net-40.ckpt] SAVED\n",
      "Epoch : 041/500  Train_loss : 1630.8173828  Test_loss : 1349.0065918\n",
      "Epoch : 042/500  Train_loss : 1549.4973145  Test_loss : 1275.2554932\n",
      "Epoch : 043/500  Train_loss : 1438.7382812  Test_loss : 1461.9161377\n",
      "Epoch : 044/500  Train_loss : 1404.2264404  Test_loss : 1403.4283447\n",
      "Epoch : 045/500  Train_loss : 1397.7866211  Test_loss : 1406.1322021\n",
      "[./nets/net-45.ckpt] SAVED\n",
      "Epoch : 046/500  Train_loss : 1264.6156006  Test_loss : 1366.1452637\n",
      "Epoch : 047/500  Train_loss : 1618.1076660  Test_loss : 1379.9263916\n",
      "Epoch : 048/500  Train_loss : 1216.2490234  Test_loss : 1377.4807129\n",
      "Epoch : 049/500  Train_loss : 1267.7327881  Test_loss : 1433.7790527\n",
      "Epoch : 050/500  Train_loss : 1249.1688232  Test_loss : 1406.7991943\n",
      "[./nets/net-50.ckpt] SAVED\n",
      "Epoch : 051/500  Train_loss : 1432.5600586  Test_loss : 1273.3381348\n",
      "Epoch : 052/500  Train_loss : 1277.1391602  Test_loss : 1427.2484131\n",
      "Epoch : 053/500  Train_loss : 1351.7302246  Test_loss : 1417.9686279\n",
      "Epoch : 054/500  Train_loss : 1299.2087402  Test_loss : 1557.5081787\n",
      "Epoch : 055/500  Train_loss : 1356.9182129  Test_loss : 1330.3538818\n",
      "[./nets/net-55.ckpt] SAVED\n",
      "Epoch : 056/500  Train_loss : 1457.0500488  Test_loss : 1351.6650391\n",
      "Epoch : 057/500  Train_loss : 1122.3796387  Test_loss : 1210.8640137\n",
      "Epoch : 058/500  Train_loss : 1268.0800781  Test_loss : 1344.4951172\n",
      "Epoch : 059/500  Train_loss : 1252.4598389  Test_loss : 1423.1341553\n",
      "Epoch : 060/500  Train_loss : 1391.7175293  Test_loss : 1316.1435547\n",
      "[./nets/net-60.ckpt] SAVED\n",
      "Epoch : 061/500  Train_loss : 1218.8533936  Test_loss : 1222.8530273\n",
      "Epoch : 062/500  Train_loss : 1377.7369385  Test_loss : 1344.3209229\n",
      "Epoch : 063/500  Train_loss : 1282.5842285  Test_loss : 1312.7863770\n",
      "Epoch : 064/500  Train_loss : 1304.4130859  Test_loss : 1331.1101074\n",
      "Epoch : 065/500  Train_loss : 1304.8577881  Test_loss : 1297.3433838\n",
      "[./nets/net-65.ckpt] SAVED\n",
      "Epoch : 066/500  Train_loss : 1328.9234619  Test_loss : 1243.8806152\n",
      "Epoch : 067/500  Train_loss : 1346.0893555  Test_loss : 1229.6685791\n",
      "Epoch : 068/500  Train_loss : 1161.4681396  Test_loss : 1351.5397949\n",
      "Epoch : 069/500  Train_loss : 1401.5148926  Test_loss : 1544.6065674\n",
      "Epoch : 070/500  Train_loss : 1207.0098877  Test_loss : 1392.6215820\n",
      "[./nets/net-70.ckpt] SAVED\n",
      "Epoch : 071/500  Train_loss : 1049.5093994  Test_loss : 1306.0036621\n",
      "Epoch : 072/500  Train_loss : 1440.8413086  Test_loss : 1148.1097412\n",
      "Epoch : 073/500  Train_loss : 1204.1235352  Test_loss : 1097.2011719\n",
      "Epoch : 074/500  Train_loss : 1232.1997070  Test_loss : 1335.9716797\n",
      "Epoch : 075/500  Train_loss : 1370.5241699  Test_loss : 1218.5321045\n",
      "[./nets/net-75.ckpt] SAVED\n",
      "Epoch : 076/500  Train_loss : 1190.8267822  Test_loss : 1242.7884521\n",
      "Epoch : 077/500  Train_loss : 1137.1370850  Test_loss : 1235.1309814\n",
      "Epoch : 078/500  Train_loss : 1160.2644043  Test_loss : 1223.3828125\n",
      "Epoch : 079/500  Train_loss : 1391.4803467  Test_loss : 1235.3676758\n",
      "Epoch : 080/500  Train_loss : 1217.8237305  Test_loss : 1211.3717041\n",
      "[./nets/net-80.ckpt] SAVED\n",
      "Epoch : 081/500  Train_loss : 1112.7161865  Test_loss : 1368.2102051\n",
      "Epoch : 082/500  Train_loss : 1393.2556152  Test_loss : 1321.9916992\n",
      "Epoch : 083/500  Train_loss : 1297.9536133  Test_loss : 1221.8933105\n",
      "Epoch : 084/500  Train_loss : 1100.6647949  Test_loss : 1164.2630615\n",
      "Epoch : 085/500  Train_loss : 1221.1303711  Test_loss : 1402.4293213\n",
      "[./nets/net-85.ckpt] SAVED\n",
      "Epoch : 086/500  Train_loss : 1144.5299072  Test_loss : 1030.5413818\n",
      "Epoch : 087/500  Train_loss : 1104.6920166  Test_loss : 1228.3300781\n",
      "Epoch : 088/500  Train_loss : 1164.7648926  Test_loss : 1294.0616455\n",
      "Epoch : 089/500  Train_loss : 1227.0007324  Test_loss : 1159.6353760\n",
      "Epoch : 090/500  Train_loss : 1104.3009033  Test_loss : 1148.6798096\n",
      "[./nets/net-90.ckpt] SAVED\n",
      "Epoch : 091/500  Train_loss : 1066.0766602  Test_loss : 1028.7460938\n",
      "Epoch : 092/500  Train_loss : 1330.6516113  Test_loss : 1273.6391602\n",
      "Epoch : 093/500  Train_loss : 974.5915527  Test_loss : 1138.6188965\n",
      "Epoch : 094/500  Train_loss : 1065.3016357  Test_loss : 1143.6727295\n",
      "Epoch : 095/500  Train_loss : 1111.0828857  Test_loss : 1065.9241943\n",
      "[./nets/net-95.ckpt] SAVED\n",
      "Epoch : 096/500  Train_loss : 1219.7664795  Test_loss : 1228.1931152\n",
      "Epoch : 097/500  Train_loss : 1149.3635254  Test_loss : 1108.4951172\n",
      "Epoch : 098/500  Train_loss : 1117.3750000  Test_loss : 1255.4492188\n",
      "Epoch : 099/500  Train_loss : 1193.3140869  Test_loss : 1058.5650635\n",
      "Epoch : 100/500  Train_loss : 1138.2360840  Test_loss : 947.9067383\n",
      "[./nets/net-100.ckpt] SAVED\n",
      "Epoch : 101/500  Train_loss : 1116.4908447  Test_loss : 1131.5119629\n",
      "Epoch : 102/500  Train_loss : 1065.1104736  Test_loss : 1243.9095459\n",
      "Epoch : 103/500  Train_loss : 1257.5012207  Test_loss : 1031.0581055\n",
      "Epoch : 104/500  Train_loss : 1029.1673584  Test_loss : 1458.4184570\n",
      "Epoch : 105/500  Train_loss : 1031.8099365  Test_loss : 1014.0800781\n",
      "[./nets/net-105.ckpt] SAVED\n",
      "Epoch : 106/500  Train_loss : 1099.5605469  Test_loss : 1082.1949463\n",
      "Epoch : 107/500  Train_loss : 1061.7171631  Test_loss : 1092.8713379\n",
      "Epoch : 108/500  Train_loss : 1032.1456299  Test_loss : 1248.4418945\n",
      "Epoch : 109/500  Train_loss : 1227.4652100  Test_loss : 1227.2437744\n",
      "Epoch : 110/500  Train_loss : 1228.6743164  Test_loss : 1284.4489746\n",
      "[./nets/net-110.ckpt] SAVED\n",
      "Epoch : 111/500  Train_loss : 1026.9204102  Test_loss : 1276.4133301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 112/500  Train_loss : 1077.6881104  Test_loss : 1032.1082764\n",
      "Epoch : 113/500  Train_loss : 1102.9626465  Test_loss : 1272.9294434\n",
      "Epoch : 114/500  Train_loss : 904.7679443  Test_loss : 936.6815796\n",
      "Epoch : 115/500  Train_loss : 1098.3111572  Test_loss : 1179.1013184\n",
      "[./nets/net-115.ckpt] SAVED\n",
      "Epoch : 116/500  Train_loss : 1182.3854980  Test_loss : 971.2183228\n",
      "Epoch : 117/500  Train_loss : 1158.1480713  Test_loss : 1091.7724609\n",
      "Epoch : 118/500  Train_loss : 1034.0587158  Test_loss : 941.5605469\n",
      "Epoch : 119/500  Train_loss : 1029.7268066  Test_loss : 1029.6093750\n",
      "Epoch : 120/500  Train_loss : 971.2425537  Test_loss : 1046.6250000\n",
      "[./nets/net-120.ckpt] SAVED\n",
      "Epoch : 121/500  Train_loss : 1156.6398926  Test_loss : 1107.2523193\n",
      "Epoch : 122/500  Train_loss : 1066.5659180  Test_loss : 1189.3062744\n",
      "Epoch : 123/500  Train_loss : 1046.5021973  Test_loss : 1066.5864258\n",
      "Epoch : 124/500  Train_loss : 1150.6079102  Test_loss : 1110.7536621\n",
      "Epoch : 125/500  Train_loss : 1015.1950684  Test_loss : 1029.8343506\n",
      "[./nets/net-125.ckpt] SAVED\n",
      "Epoch : 126/500  Train_loss : 1067.2333984  Test_loss : 1139.2478027\n",
      "Epoch : 127/500  Train_loss : 1009.7493896  Test_loss : 1083.5372314\n",
      "Epoch : 128/500  Train_loss : 1068.6116943  Test_loss : 1168.5212402\n",
      "Epoch : 129/500  Train_loss : 1016.0065918  Test_loss : 1011.0458374\n",
      "Epoch : 130/500  Train_loss : 964.2740479  Test_loss : 1091.6492920\n",
      "[./nets/net-130.ckpt] SAVED\n",
      "Epoch : 131/500  Train_loss : 887.0934448  Test_loss : 1087.0228271\n",
      "Epoch : 132/500  Train_loss : 1379.4603271  Test_loss : 1004.1170044\n",
      "Epoch : 133/500  Train_loss : 966.1882324  Test_loss : 994.6003418\n",
      "Epoch : 134/500  Train_loss : 1125.4034424  Test_loss : 1194.6204834\n",
      "Epoch : 135/500  Train_loss : 1173.3740234  Test_loss : 964.0050659\n",
      "[./nets/net-135.ckpt] SAVED\n",
      "Epoch : 136/500  Train_loss : 889.5499878  Test_loss : 1025.0194092\n",
      "Epoch : 137/500  Train_loss : 1118.6494141  Test_loss : 1121.7286377\n",
      "Epoch : 138/500  Train_loss : 1106.3071289  Test_loss : 940.0384521\n",
      "Epoch : 139/500  Train_loss : 967.0943604  Test_loss : 1093.9542236\n",
      "Epoch : 140/500  Train_loss : 1057.4761963  Test_loss : 921.1740112\n",
      "[./nets/net-140.ckpt] SAVED\n",
      "Epoch : 141/500  Train_loss : 1002.8031006  Test_loss : 1130.8005371\n",
      "Epoch : 142/500  Train_loss : 982.8067017  Test_loss : 893.4639282\n",
      "Epoch : 143/500  Train_loss : 1032.2604980  Test_loss : 1082.0842285\n",
      "Epoch : 144/500  Train_loss : 1043.6217041  Test_loss : 1137.0012207\n",
      "Epoch : 145/500  Train_loss : 1181.0881348  Test_loss : 1151.1716309\n",
      "[./nets/net-145.ckpt] SAVED\n",
      "Epoch : 146/500  Train_loss : 1020.9240723  Test_loss : 1078.2253418\n",
      "Epoch : 147/500  Train_loss : 1073.2899170  Test_loss : 1005.3474731\n",
      "Epoch : 148/500  Train_loss : 1069.4798584  Test_loss : 1091.1657715\n",
      "Epoch : 149/500  Train_loss : 897.0617676  Test_loss : 1042.0736084\n",
      "Epoch : 150/500  Train_loss : 1167.7470703  Test_loss : 962.9981689\n",
      "[./nets/net-150.ckpt] SAVED\n",
      "Epoch : 151/500  Train_loss : 1042.3508301  Test_loss : 1123.0454102\n",
      "Epoch : 152/500  Train_loss : 943.6901245  Test_loss : 934.0330811\n",
      "Epoch : 153/500  Train_loss : 1047.1242676  Test_loss : 1100.5244141\n",
      "Epoch : 154/500  Train_loss : 1040.2058105  Test_loss : 974.8129272\n",
      "Epoch : 155/500  Train_loss : 1051.3963623  Test_loss : 967.3984985\n",
      "[./nets/net-155.ckpt] SAVED\n",
      "Epoch : 156/500  Train_loss : 949.8441162  Test_loss : 1050.4235840\n",
      "Epoch : 157/500  Train_loss : 977.1260986  Test_loss : 952.7803955\n",
      "Epoch : 158/500  Train_loss : 994.5213013  Test_loss : 906.0238647\n",
      "Epoch : 159/500  Train_loss : 991.6710205  Test_loss : 1217.6661377\n",
      "Epoch : 160/500  Train_loss : 1111.0294189  Test_loss : 880.4305420\n",
      "[./nets/net-160.ckpt] SAVED\n",
      "Epoch : 161/500  Train_loss : 1000.0413818  Test_loss : 981.8258057\n",
      "Epoch : 162/500  Train_loss : 980.7318115  Test_loss : 928.9400635\n",
      "Epoch : 163/500  Train_loss : 1063.1040039  Test_loss : 987.0779419\n",
      "Epoch : 164/500  Train_loss : 947.9317017  Test_loss : 858.2791138\n",
      "Epoch : 165/500  Train_loss : 991.3612671  Test_loss : 977.7117920\n",
      "[./nets/net-165.ckpt] SAVED\n",
      "Epoch : 166/500  Train_loss : 1066.5451660  Test_loss : 926.2117310\n",
      "Epoch : 167/500  Train_loss : 1005.6917725  Test_loss : 1111.2038574\n",
      "Epoch : 168/500  Train_loss : 892.5223389  Test_loss : 986.5540161\n",
      "Epoch : 169/500  Train_loss : 1153.9532471  Test_loss : 884.3962402\n",
      "Epoch : 170/500  Train_loss : 948.1303711  Test_loss : 1022.7401733\n",
      "[./nets/net-170.ckpt] SAVED\n",
      "Epoch : 171/500  Train_loss : 910.8408203  Test_loss : 1014.8240967\n",
      "Epoch : 172/500  Train_loss : 943.3940430  Test_loss : 859.3344727\n",
      "Epoch : 173/500  Train_loss : 1070.3732910  Test_loss : 1112.8175049\n",
      "Epoch : 174/500  Train_loss : 987.2569580  Test_loss : 997.6033936\n",
      "Epoch : 175/500  Train_loss : 1001.4853516  Test_loss : 735.6301270\n",
      "[./nets/net-175.ckpt] SAVED\n",
      "Epoch : 176/500  Train_loss : 871.1229858  Test_loss : 973.7889404\n",
      "Epoch : 177/500  Train_loss : 859.5380859  Test_loss : 1063.2207031\n",
      "Epoch : 178/500  Train_loss : 891.7711182  Test_loss : 918.0778198\n",
      "Epoch : 179/500  Train_loss : 909.1768799  Test_loss : 998.3665161\n",
      "Epoch : 180/500  Train_loss : 850.2626953  Test_loss : 954.3918457\n",
      "[./nets/net-180.ckpt] SAVED\n",
      "Epoch : 181/500  Train_loss : 854.4168091  Test_loss : 1049.5227051\n",
      "Epoch : 182/500  Train_loss : 973.3370361  Test_loss : 1021.5070801\n",
      "Epoch : 183/500  Train_loss : 977.1744995  Test_loss : 1131.0122070\n",
      "Epoch : 184/500  Train_loss : 1045.2783203  Test_loss : 864.7836914\n",
      "Epoch : 185/500  Train_loss : 748.0822144  Test_loss : 940.2174072\n",
      "[./nets/net-185.ckpt] SAVED\n",
      "Epoch : 186/500  Train_loss : 828.5264282  Test_loss : 1017.7946777\n",
      "Epoch : 187/500  Train_loss : 891.2968750  Test_loss : 988.5240479\n",
      "Epoch : 188/500  Train_loss : 968.5510254  Test_loss : 878.2875366\n",
      "Epoch : 189/500  Train_loss : 983.5570679  Test_loss : 995.0958862\n",
      "Epoch : 190/500  Train_loss : 854.1287842  Test_loss : 1074.2645264\n",
      "[./nets/net-190.ckpt] SAVED\n",
      "Epoch : 191/500  Train_loss : 860.6379395  Test_loss : 1095.2039795\n",
      "Epoch : 192/500  Train_loss : 912.8004150  Test_loss : 1124.2707520\n",
      "Epoch : 193/500  Train_loss : 901.8555298  Test_loss : 890.3009644\n",
      "Epoch : 194/500  Train_loss : 978.9375610  Test_loss : 848.3206177\n",
      "Epoch : 195/500  Train_loss : 953.8995361  Test_loss : 980.3578491\n",
      "[./nets/net-195.ckpt] SAVED\n",
      "Epoch : 196/500  Train_loss : 908.0661011  Test_loss : 895.8546753\n",
      "Epoch : 197/500  Train_loss : 872.3898926  Test_loss : 948.1484375\n",
      "Epoch : 198/500  Train_loss : 1073.4984131  Test_loss : 807.2360229\n",
      "Epoch : 199/500  Train_loss : 883.0153198  Test_loss : 910.7410889\n",
      "Epoch : 200/500  Train_loss : 908.4470215  Test_loss : 1014.7208862\n",
      "[./nets/net-200.ckpt] SAVED\n",
      "Epoch : 201/500  Train_loss : 824.8250122  Test_loss : 1012.0637817\n",
      "Epoch : 202/500  Train_loss : 876.4671021  Test_loss : 1019.1462402\n",
      "Epoch : 203/500  Train_loss : 842.7864990  Test_loss : 982.2882690\n",
      "Epoch : 204/500  Train_loss : 941.5913086  Test_loss : 961.2515259\n",
      "Epoch : 205/500  Train_loss : 836.3893433  Test_loss : 709.8198242\n",
      "[./nets/net-205.ckpt] SAVED\n",
      "Epoch : 206/500  Train_loss : 894.7591553  Test_loss : 886.6254883\n",
      "Epoch : 207/500  Train_loss : 909.1137695  Test_loss : 767.0870361\n",
      "Epoch : 208/500  Train_loss : 882.2979126  Test_loss : 918.8353271\n",
      "Epoch : 209/500  Train_loss : 841.4512939  Test_loss : 854.3348389\n",
      "Epoch : 210/500  Train_loss : 784.9425659  Test_loss : 972.3235474\n",
      "[./nets/net-210.ckpt] SAVED\n",
      "Epoch : 211/500  Train_loss : 822.3309937  Test_loss : 979.5359497\n",
      "Epoch : 212/500  Train_loss : 884.5216064  Test_loss : 906.9282837\n",
      "Epoch : 213/500  Train_loss : 973.7164917  Test_loss : 998.0594482\n",
      "Epoch : 214/500  Train_loss : 860.9759521  Test_loss : 963.1797485\n",
      "Epoch : 215/500  Train_loss : 852.7282104  Test_loss : 947.8795166\n",
      "[./nets/net-215.ckpt] SAVED\n",
      "Epoch : 216/500  Train_loss : 941.0834961  Test_loss : 989.1285400\n",
      "Epoch : 217/500  Train_loss : 807.3597412  Test_loss : 906.4589844\n",
      "Epoch : 218/500  Train_loss : 965.1190186  Test_loss : 933.4982910\n",
      "Epoch : 219/500  Train_loss : 856.6878662  Test_loss : 904.1775513\n",
      "Epoch : 220/500  Train_loss : 881.4181519  Test_loss : 1073.6999512\n",
      "[./nets/net-220.ckpt] SAVED\n",
      "Epoch : 221/500  Train_loss : 877.2886963  Test_loss : 1026.4594727\n",
      "Epoch : 222/500  Train_loss : 971.3933105  Test_loss : 925.2193604\n",
      "Epoch : 223/500  Train_loss : 834.5301514  Test_loss : 1093.7381592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 224/500  Train_loss : 879.1410522  Test_loss : 1033.1687012\n",
      "Epoch : 225/500  Train_loss : 885.6442871  Test_loss : 865.0101318\n",
      "[./nets/net-225.ckpt] SAVED\n",
      "Epoch : 226/500  Train_loss : 897.6739502  Test_loss : 1002.0284424\n",
      "Epoch : 227/500  Train_loss : 779.9050293  Test_loss : 772.3883667\n",
      "Epoch : 228/500  Train_loss : 824.7344360  Test_loss : 897.4357910\n",
      "Epoch : 229/500  Train_loss : 927.2681274  Test_loss : 893.1401367\n",
      "Epoch : 230/500  Train_loss : 926.9558105  Test_loss : 839.1156616\n",
      "[./nets/net-230.ckpt] SAVED\n",
      "Epoch : 231/500  Train_loss : 863.4737549  Test_loss : 952.4473877\n",
      "Epoch : 232/500  Train_loss : 844.1868896  Test_loss : 1030.5745850\n",
      "Epoch : 233/500  Train_loss : 882.4113770  Test_loss : 906.6337891\n",
      "Epoch : 234/500  Train_loss : 756.8988037  Test_loss : 973.7802734\n",
      "Epoch : 235/500  Train_loss : 911.7322998  Test_loss : 868.0529175\n",
      "[./nets/net-235.ckpt] SAVED\n",
      "Epoch : 236/500  Train_loss : 795.0942383  Test_loss : 902.7725830\n",
      "Epoch : 237/500  Train_loss : 870.0540161  Test_loss : 898.8277588\n",
      "Epoch : 238/500  Train_loss : 758.9038086  Test_loss : 866.8665161\n",
      "Epoch : 239/500  Train_loss : 830.6783447  Test_loss : 1027.3790283\n",
      "Epoch : 240/500  Train_loss : 818.6250610  Test_loss : 711.3943481\n",
      "[./nets/net-240.ckpt] SAVED\n",
      "Epoch : 241/500  Train_loss : 894.8826294  Test_loss : 825.4230957\n",
      "Epoch : 242/500  Train_loss : 818.6023560  Test_loss : 735.4664917\n",
      "Epoch : 243/500  Train_loss : 802.2191162  Test_loss : 876.1864014\n",
      "Epoch : 244/500  Train_loss : 859.7030029  Test_loss : 896.7193604\n",
      "Epoch : 245/500  Train_loss : 757.5195312  Test_loss : 982.7685547\n",
      "[./nets/net-245.ckpt] SAVED\n",
      "Epoch : 246/500  Train_loss : 784.1049805  Test_loss : 918.0803223\n",
      "Epoch : 247/500  Train_loss : 791.6948242  Test_loss : 977.2286377\n",
      "Epoch : 248/500  Train_loss : 630.0314331  Test_loss : 858.8024292\n",
      "Epoch : 249/500  Train_loss : 803.3799438  Test_loss : 957.8529053\n",
      "Epoch : 250/500  Train_loss : 964.2800293  Test_loss : 855.9291992\n",
      "[./nets/net-250.ckpt] SAVED\n",
      "Epoch : 251/500  Train_loss : 766.0802612  Test_loss : 939.0166016\n",
      "Epoch : 252/500  Train_loss : 816.5141602  Test_loss : 1003.3920898\n",
      "Epoch : 253/500  Train_loss : 963.4137573  Test_loss : 856.3082886\n",
      "Epoch : 254/500  Train_loss : 818.9792480  Test_loss : 925.9754028\n",
      "Epoch : 255/500  Train_loss : 849.5198975  Test_loss : 765.5834351\n",
      "[./nets/net-255.ckpt] SAVED\n",
      "Epoch : 256/500  Train_loss : 900.1225586  Test_loss : 869.4508667\n",
      "Epoch : 257/500  Train_loss : 1063.8470459  Test_loss : 846.0264282\n",
      "Epoch : 258/500  Train_loss : 969.4819336  Test_loss : 833.6249390\n",
      "Epoch : 259/500  Train_loss : 953.2784424  Test_loss : 847.8524170\n",
      "Epoch : 260/500  Train_loss : 883.4708252  Test_loss : 893.7457275\n",
      "[./nets/net-260.ckpt] SAVED\n",
      "Epoch : 261/500  Train_loss : 904.6812744  Test_loss : 922.1094971\n",
      "Epoch : 262/500  Train_loss : 708.8808594  Test_loss : 827.8199463\n",
      "Epoch : 263/500  Train_loss : 811.2704468  Test_loss : 1030.5034180\n",
      "Epoch : 264/500  Train_loss : 736.6730347  Test_loss : 943.1975098\n",
      "Epoch : 265/500  Train_loss : 715.4927979  Test_loss : 801.9864502\n",
      "[./nets/net-265.ckpt] SAVED\n",
      "Epoch : 266/500  Train_loss : 896.0882568  Test_loss : 846.0776978\n",
      "Epoch : 267/500  Train_loss : 810.0003052  Test_loss : 901.7830811\n",
      "Epoch : 268/500  Train_loss : 820.9953613  Test_loss : 837.0169678\n",
      "Epoch : 269/500  Train_loss : 760.7424316  Test_loss : 856.7829590\n",
      "Epoch : 270/500  Train_loss : 868.3306274  Test_loss : 752.7257080\n",
      "[./nets/net-270.ckpt] SAVED\n",
      "Epoch : 271/500  Train_loss : 729.2034912  Test_loss : 808.5433960\n",
      "Epoch : 272/500  Train_loss : 990.6087646  Test_loss : 896.7510986\n",
      "Epoch : 273/500  Train_loss : 715.6087036  Test_loss : 813.7429810\n",
      "Epoch : 274/500  Train_loss : 962.8321533  Test_loss : 919.3190308\n",
      "Epoch : 275/500  Train_loss : 875.4606934  Test_loss : 775.3775024\n",
      "[./nets/net-275.ckpt] SAVED\n",
      "Epoch : 276/500  Train_loss : 822.1892700  Test_loss : 844.6943970\n",
      "Epoch : 277/500  Train_loss : 901.5935669  Test_loss : 787.1571045\n",
      "Epoch : 278/500  Train_loss : 756.0105591  Test_loss : 877.6768799\n",
      "Epoch : 279/500  Train_loss : 850.8061523  Test_loss : 743.5887451\n",
      "Epoch : 280/500  Train_loss : 824.8115234  Test_loss : 873.6096802\n",
      "[./nets/net-280.ckpt] SAVED\n",
      "Epoch : 281/500  Train_loss : 870.5965576  Test_loss : 708.0612793\n",
      "Epoch : 282/500  Train_loss : 767.7374268  Test_loss : 743.5610962\n",
      "Epoch : 283/500  Train_loss : 777.2899170  Test_loss : 1019.7458496\n",
      "Epoch : 284/500  Train_loss : 730.0333252  Test_loss : 795.5195923\n",
      "Epoch : 285/500  Train_loss : 711.4345093  Test_loss : 825.4365234\n",
      "[./nets/net-285.ckpt] SAVED\n",
      "Epoch : 286/500  Train_loss : 847.6130981  Test_loss : 914.9146729\n",
      "Epoch : 287/500  Train_loss : 749.3817139  Test_loss : 872.9501953\n",
      "Epoch : 288/500  Train_loss : 838.1701660  Test_loss : 899.9477539\n",
      "Epoch : 289/500  Train_loss : 763.7565918  Test_loss : 848.4669800\n",
      "Epoch : 290/500  Train_loss : 890.2427979  Test_loss : 724.3109131\n",
      "[./nets/net-290.ckpt] SAVED\n",
      "Epoch : 291/500  Train_loss : 727.8515015  Test_loss : 888.3248291\n",
      "Epoch : 292/500  Train_loss : 732.0758057  Test_loss : 884.0462036\n",
      "Epoch : 293/500  Train_loss : 775.4970093  Test_loss : 763.4022217\n",
      "Epoch : 294/500  Train_loss : 802.6385498  Test_loss : 739.6423340\n",
      "Epoch : 295/500  Train_loss : 943.3466797  Test_loss : 782.0395508\n",
      "[./nets/net-295.ckpt] SAVED\n",
      "Epoch : 296/500  Train_loss : 707.4765625  Test_loss : 829.4295654\n",
      "Epoch : 297/500  Train_loss : 816.3593750  Test_loss : 904.4103394\n",
      "Epoch : 298/500  Train_loss : 763.0986328  Test_loss : 736.4721680\n",
      "Epoch : 299/500  Train_loss : 770.6702271  Test_loss : 813.5128784\n",
      "Epoch : 300/500  Train_loss : 724.2259521  Test_loss : 833.6475830\n",
      "[./nets/net-300.ckpt] SAVED\n",
      "Epoch : 301/500  Train_loss : 1065.1210938  Test_loss : 711.3132324\n",
      "Epoch : 302/500  Train_loss : 834.1350098  Test_loss : 828.4756470\n",
      "Epoch : 303/500  Train_loss : 867.2149658  Test_loss : 828.8773804\n",
      "Epoch : 304/500  Train_loss : 715.1343994  Test_loss : 842.5721436\n",
      "Epoch : 305/500  Train_loss : 818.8483276  Test_loss : 823.3184814\n",
      "[./nets/net-305.ckpt] SAVED\n",
      "Epoch : 306/500  Train_loss : 789.6055298  Test_loss : 853.9167480\n",
      "Epoch : 307/500  Train_loss : 677.1663208  Test_loss : 846.0057373\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "training_epochs = FLAGS.training_epochs\n",
    "batch_size = FLAGS.batch_size\n",
    "display_step = FLAGS.display_step\n",
    "# Plot\n",
    "n_plot = 5    # plot 5 images\n",
    "train_disp_idx = np.random.randint(mnist.train.num_examples, size=n_plot)    # fixed during train time\n",
    "test_disp_idx = np.random.randint(mnist.test.num_examples, size=n_plot)\n",
    "\n",
    "# Initialize\n",
    "sess = tf.Session(config=config)\n",
    "sess.run(init)\n",
    "\n",
    "# Optimize\n",
    "for epoch in range(training_epochs):\n",
    "    total_cost = 0.\n",
    "    n_total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    rand_train_idx = np.random.randint(mnist.train.num_examples, size=batch_size)\n",
    "    rand_test_idx = np.random.randint(mnist.test.num_examples, size=batch_size)\n",
    "    \n",
    "    # Iteration\n",
    "    for i in range(n_total_batch):\n",
    "        batch_pure, _ = mnist.train.next_batch(batch_size)    # pure image\n",
    "        noise = noise_batch(batch_size)    # random noise\n",
    "        batch_crpt = occl(batch_pure, noise)   # corrupted image \n",
    "        feeds = {ph_pure: batch_pure, ph_noise: noise, ph_crpt: batch_crpt}\n",
    "        sess.run(optm, feed_dict=feeds)\n",
    "        #total_cost += sess.run(loss, feed_dict=feeds)\n",
    "    #total_cost = total_cost / mnist.train.num_examples\n",
    "    \n",
    "    train_pure = train_img[rand_train_idx]    # pure image\n",
    "    train_noise = noise_batch(batch_size)    # random noise\n",
    "    train_crpt = occl(train_pure,train_noise)   # corrupted image\n",
    "    train_feeds = {ph_pure: train_pure, ph_noise: train_noise, ph_crpt: train_crpt}\n",
    "    train_loss, tb_train_loss = sess.run([loss,_train_loss], feed_dict=train_feeds)\n",
    "    \n",
    "    test_pure = test_img[rand_test_idx]    # pure image\n",
    "    test_noise = noise_batch(batch_size)    # random noise\n",
    "    test_crpt = occl(test_pure,test_noise)   # corrupted image\n",
    "    test_feeds = {ph_pure: test_pure, ph_noise: test_noise, ph_crpt: test_crpt}\n",
    "    test_loss, tb_test_loss = sess.run([loss,_test_loss], feed_dict=test_feeds)\n",
    "\n",
    "    writer.add_summary(tb_train_loss, epoch)\n",
    "    writer.add_summary(tb_test_loss, epoch)\n",
    "    print(\"Epoch : %03d/%03d  Train_loss : %.7f  Test_loss : %.7f\" \n",
    "          % (epoch+1, training_epochs, train_loss, test_loss))   \n",
    "        \n",
    "    # Display\n",
    "    if (epoch+1) % display_step == 0:\n",
    "        train_gt_pure = train_img[train_disp_idx]    # pure image\n",
    "        train_gt_noise = noise_batch(n_plot)    # random noise\n",
    "        train_gt_crpt = occl(train_gt_pure,train_gt_noise)   # corrupted image\n",
    "        train_gt_feeds = {ph_pure: train_gt_pure, ph_noise: train_gt_noise, ph_crpt: train_gt_crpt}\n",
    "        \n",
    "        test_gt_pure = test_img[test_disp_idx]    # pure image\n",
    "        test_gt_noise = noise_batch(n_plot)    # random noise\n",
    "        test_gt_crpt = occl(test_gt_pure,test_gt_noise)   # corrupted image\n",
    "        test_gt_feeds = {ph_pure: test_gt_pure, ph_noise: test_gt_noise, ph_crpt: test_gt_crpt}\n",
    "        \n",
    "        # generated images\n",
    "        train_gen_pure, train_gen_noise, train_gen_crpt = sess.run([core_gen, shell2_gen, full_gen], \\\n",
    "                                        feed_dict=train_gt_feeds)  # 784-d vector\n",
    "        test_gen_pure, test_gen_noise, test_gen_crpt = sess.run([core_gen, shell2_gen, full_gen], \\\n",
    "                                        feed_dict=test_gt_feeds)  # 784-d vector\n",
    "        \n",
    "        # plotting results from training data\n",
    "        fig, axes = plt.subplots(nrows=4, ncols=n_plot, figsize=(10,2*n_plot))   # displaying 4*n_plot images\n",
    "        plt.setp(axes, xticks=np.arange(0,27,7), yticks=np.arange(0,27,7)) \n",
    "        for j in range(n_plot):\n",
    "            train_disp_gt_crpt = np.reshape(train_gt_crpt[j], [28,28])    # 28x28\n",
    "            axes[0, j].imshow(train_disp_gt_crpt, cmap='gray')   \n",
    "            axes[0, j].set(ylabel='gt_crpt')\n",
    "            axes[0, j].label_outer()\n",
    "            \n",
    "            train_disp_gen_pure = np.reshape(train_gen_pure[j], [28,28])    # 28x28\n",
    "            axes[1, j].imshow(train_disp_gen_pure, cmap='gray')   \n",
    "            axes[1, j].set(ylabel='gen_pure')\n",
    "            axes[1, j].label_outer()\n",
    "            \n",
    "            train_disp_gen_noise = np.reshape(train_gen_noise[j], [28,28])    # 28x28\n",
    "            axes[2, j].imshow(train_disp_gen_noise, cmap='gray')   \n",
    "            axes[2, j].set(ylabel='gen_noise')\n",
    "            axes[2, j].label_outer()\n",
    "            \n",
    "            train_disp_gen_crpt = np.reshape(train_gen_crpt[j], [28,28])    # 28x28\n",
    "            axes[3, j].imshow(train_disp_gen_crpt, cmap='gray')   \n",
    "            axes[3, j].set(ylabel='gen_crpt')\n",
    "            axes[3, j].label_outer()\n",
    "            \n",
    "#             train_disp_gt_pure = np.reshape(train_gt_pure[j], [28,28])\n",
    "#             axes[0, j].imshow(train_disp_gt_pure, cmap='gray')\n",
    "#             axes[0, j].set(ylabel='gt_pure')\n",
    "#             axes[0, j].label_outer()\n",
    "            \n",
    "#             train_disp_gt_noise = np.reshape(train_gt_noise[j], [28,28])    # 28x28\n",
    "#             axes[2, j].imshow(train_disp_gt_noise, cmap='gray')   \n",
    "#             axes[2, j].set(ylabel='gt_noise')\n",
    "#             axes[2, j].label_outer()\n",
    "                    \n",
    "        plt.savefig(outputdir+'/train/epoch %03d' %(epoch+1))    \n",
    "        plt.close(fig)\n",
    "        \n",
    "        # plotting results from testing data\n",
    "        fig, axes = plt.subplots(nrows=4, ncols=n_plot, figsize=(10,2*n_plot))   # displaying 4*n_plot images\n",
    "        plt.setp(axes, xticks=np.arange(0,27,7), yticks=np.arange(0,27,7)) \n",
    "        for k in range(n_plot):\n",
    "            test_disp_gt_crpt = np.reshape(test_gt_crpt[k], [28,28])    # 28x28\n",
    "            axes[0, k].imshow(test_disp_gt_crpt, cmap='gray')   \n",
    "            axes[0, k].set(ylabel='gt_crpt')\n",
    "            axes[0, k].label_outer()\n",
    "            \n",
    "            test_disp_gen_pure = np.reshape(test_gen_pure[k], [28,28])    # 28x28\n",
    "            axes[1, k].imshow(test_disp_gen_pure, cmap='gray')   \n",
    "            axes[1, k].set(ylabel='gen_pure')\n",
    "            axes[1, k].label_outer()           \n",
    "            \n",
    "            test_disp_gen_noise = np.reshape(test_gen_noise[k], [28,28])    # 28x28\n",
    "            axes[2, k].imshow(test_disp_gen_noise, cmap='gray')   \n",
    "            axes[2, k].set(ylabel='gen_noise')\n",
    "            axes[2, k].label_outer()\n",
    "            \n",
    "            test_disp_gen_crpt = np.reshape(test_gen_crpt[k], [28,28])    # 28x28\n",
    "            axes[3, k].imshow(test_disp_gen_crpt, cmap='gray')   \n",
    "            axes[3, k].set(ylabel='gen_crpt')\n",
    "            axes[3, k].label_outer()\n",
    "            \n",
    "#             test_disp_gt_pure = np.reshape(test_gt_pure[k], [28,28])\n",
    "#             axes[0, k].imshow(test_disp_gt_pure, cmap='gray')\n",
    "#             axes[0, k].set(ylabel='gt_pure')\n",
    "#             axes[0, k].label_outer()\n",
    "            \n",
    "#             test_disp_gt_noise = np.reshape(test_gt_noise[k], [28,28])    # 28x28\n",
    "#             axes[2, k].imshow(test_disp_gt_noise, cmap='gray')   \n",
    "#             axes[2, k].set(ylabel='gt_noise')\n",
    "#             axes[2, k].label_outer()\n",
    "                    \n",
    "        plt.savefig(outputdir+'/test/epoch %03d' %(epoch+1))    \n",
    "        plt.close(fig)\n",
    "        \n",
    "        # Save\n",
    "        if (epoch+1) % FLAGS.save_step ==0:\n",
    "            savename = savedir+\"/net-\"+str(epoch+1)+\".ckpt\"\n",
    "            saver.save(sess, savename)\n",
    "            print(\"[%s] SAVED\" % (savename))\n",
    "\n",
    "print(\"Optimization Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
