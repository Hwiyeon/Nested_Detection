{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current version [1.3.1]\n",
      "Packages Loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)  ## just for ignore DeprcationWarning message\n",
    "print(\"Current version [%s]\" %(tf.__version__))\n",
    "print(\"Packages Loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLAGS READY\n"
     ]
    }
   ],
   "source": [
    "# Dataset Configurations\n",
    "tf.app.flags.DEFINE_integer('img_size', 28, \"\"\"Image size of MNIST dataset\"\"\")\n",
    "\n",
    "# Network Configurations\n",
    "tf.app.flags.DEFINE_integer('batch_size', 100, \"\"\"Number of images to process in a batch\"\"\")\n",
    "tf.app.flags.DEFINE_float('l1_ratio', 0.5, \"\"\"Ratio of level1\"\"\")\n",
    "tf.app.flags.DEFINE_float('l2_ratio', 0.5, \"\"\"Ratio of level2\"\"\")\n",
    "\n",
    "# Optimization Configurations\n",
    "tf.app.flags.DEFINE_float('lr', 0.001, \"\"\"Learning rate\"\"\")\n",
    "\n",
    "# Training Configurations\n",
    "tf.app.flags.DEFINE_integer('training_epochs', 500, \"\"\"Number of epochs to run\"\"\")\n",
    "tf.app.flags.DEFINE_integer('display_step', 5, \"\"\"Number of iterations to display training output\"\"\")\n",
    "tf.app.flags.DEFINE_integer('save_step', 5, \"\"\"Number of interations to save checkpoint\"\"\")\n",
    "tf.app.flags.DEFINE_integer('save_max', 5, \"\"\"Number of checkpoints to remain\"\"\")\n",
    "\n",
    "\n",
    "# Save Configurations\n",
    "tf.app.flags.DEFINE_string('nets', './nets', \"\"\"Directory where to write the checkpoints\"\"\")\n",
    "tf.app.flags.DEFINE_string('outputs', './outputs', \"\"\"Directory where to save the output images\"\"\")\n",
    "tf.app.flags.DEFINE_string('tboard', './tensorboard', \"\"\"Directory where to save the tensorboard logs\"\"\")\n",
    "\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "print(\"FLAGS READY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.allow_soft_placement = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../data/train-images-idx3-ubyte.gz\n",
      "Extracting ../../data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../../data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../data/t10k-labels-idx1-ubyte.gz\n",
      "MNIST ready\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('../../data/', one_hot=True)\n",
    "train_img = mnist.train.images\n",
    "train_label = mnist.train.labels\n",
    "test_img = mnist.test.images\n",
    "test_label = mnist.test.labels\n",
    "print(\"MNIST ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating random noise mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise_mask(prob=0.5):\n",
    "    mask = np.zeros([FLAGS.img_size, FLAGS.img_size])\n",
    "    rd = np.random.random()\n",
    "    if rd > prob:\n",
    "        # threshold of the size of masks\n",
    "        uthd = FLAGS.img_size    \n",
    "        lthd = 0     \n",
    "        # mask size should be beween 14x14, 5x5\n",
    "        while(uthd>14 or lthd<5):\n",
    "            ver1 = np.random.random_integers(0, FLAGS.img_size-1, size= 2)   # vertex1\n",
    "            ver2 = np.random.random_integers(0, FLAGS.img_size-1, size= 2)    # vertex2\n",
    "            uthd = np.maximum(np.abs(ver1[0]-ver2[0]), np.abs(ver1[1]-ver2[1]))    # upperbound\n",
    "            lthd = np.minimum(np.abs(ver1[0]-ver2[0]), np.abs(ver1[1]-ver2[1]))    # lowerbound\n",
    "        xmin = np.minimum(ver1[0], ver2[0])    # left x value\n",
    "        xmax = np.maximum(ver1[0], ver2[0])    # right x value\n",
    "        ymin = np.minimum(ver1[1], ver2[1])    # top y value\n",
    "        ymax = np.maximum(ver1[1], ver2[1])    # bottom y value\n",
    "        noise = np.random.random((xmax-xmin+1, ymax-ymin+1))    # random sample in [0,1]\n",
    "        mask[xmin:xmax+1, ymin:ymax+1] = noise    # noise mask with location\n",
    "        mask_meta = [xmin, xmax, ymin, ymax, noise, mask]\n",
    "    mask = np.reshape(mask, [-1])\n",
    "    return mask\n",
    "\n",
    "def noise_batch(batch_num):\n",
    "    # make random noise batch\n",
    "    mask_batch = np.zeros([batch_num, FLAGS.img_size*FLAGS.img_size])\n",
    "    for i in range(batch_num):\n",
    "        mask_batch[i,:] = noise_mask()\n",
    "    return mask_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Occlusion generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def occl(target, disturb):\n",
    "    mask = (disturb==0).astype(float)\n",
    "    masked_target = np.multiply(target, mask)\n",
    "    crpt = np.add(masked_target, disturb)\n",
    "    return crpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nested MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _nested_mlp(l1, l2_s, l2, out_channel, name, stddev=0.1, is_init=False, is_last=False):\n",
    "    l1_shape = l1.get_shape()[1]\n",
    "    l2_shape = l2.get_shape()[1]\n",
    "    l2_s_shape = l2_s.get_shape()[1]\n",
    "    \n",
    "    if is_init:\n",
    "        # input is the input image\n",
    "        with tf.device('/CPU:0'):\n",
    "            with tf.variable_scope('level1'):\n",
    "                with tf.variable_scope(name):\n",
    "                    l1_weights = tf.get_variable('weights', \n",
    "                                                 [l1_shape, out_channel*FLAGS.l1_ratio], \n",
    "                                                 tf.float32, \n",
    "                                                 initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "                    l1_biases = tf.get_variable('biases', \n",
    "                                                [out_channel*FLAGS.l1_ratio],\n",
    "                                                tf.float32, \n",
    "                                                initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "            with tf.variable_scope('level2'):\n",
    "                with tf.variable_scope(name):\n",
    "                    l2_s_weights = tf.get_variable('weights_shell', \n",
    "                                                 [l2_s_shape, out_channel*FLAGS.l2_ratio], \n",
    "                                                 tf.float32, \n",
    "                                                 initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "                    l2_s_biases = tf.get_variable('biases_shell', \n",
    "                                                [out_channel*FLAGS.l2_ratio],\n",
    "                                                tf.float32, \n",
    "                                                initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "                    \n",
    "\n",
    "        l1_mlp = tf.nn.sigmoid(tf.add(tf.matmul(l1, l1_weights), l1_biases))\n",
    "        l2_s_mlp = tf.nn.sigmoid(tf.add(tf.matmul(l2_s, l2_s_weights), l2_s_biases))\n",
    "        l2_mlp = tf.concat((l1_mlp, l2_s_mlp), 1)\n",
    "    \n",
    "    elif is_last:\n",
    "        # output is the generated image\n",
    "        with tf.device('/CPU:0'):\n",
    "            with tf.variable_scope('level1'):\n",
    "                with tf.variable_scope(name):\n",
    "                    l1_weights = tf.get_variable('weights', \n",
    "                                                 [l1_shape, out_channel], \n",
    "                                                 tf.float32, \n",
    "                                                 initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "                    l1_biases = tf.get_variable('biases', \n",
    "                                                [out_channel],\n",
    "                                                tf.float32, \n",
    "                                                initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "            with tf.variable_scope('level2'):\n",
    "                with tf.variable_scope(name):\n",
    "                    l2_s_weights = tf.get_variable('weights_shell', \n",
    "                                                 [l2_s_shape, out_channel], \n",
    "                                                 tf.float32, \n",
    "                                                 initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "                    l2_s_biases = tf.get_variable('biases_shell', \n",
    "                                                [out_channel],\n",
    "                                                tf.float32, \n",
    "                                                initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "                    l2_weights = tf.get_variable('weights', \n",
    "                                                 [l2_shape, out_channel], \n",
    "                                                 tf.float32, \n",
    "                                                 initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "                    l2_biases = tf.get_variable('biases', \n",
    "                                                [out_channel],\n",
    "                                                tf.float32, \n",
    "                                                initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "                    \n",
    "\n",
    "        l1_mlp = tf.nn.sigmoid(tf.add(tf.matmul(l1, l1_weights), l1_biases))\n",
    "        l2_s_mlp = tf.nn.sigmoid(tf.add(tf.matmul(l2_s, l2_s_weights), l2_s_biases))\n",
    "        l2_mlp = tf.nn.sigmoid(tf.add(tf.matmul(l2, l2_weights), l2_biases))\n",
    "                                 \n",
    "    else:\n",
    "        with tf.device('/CPU:0'):\n",
    "            with tf.variable_scope('level1'):\n",
    "                with tf.variable_scope(name):\n",
    "                    l1_weights = tf.get_variable('weights', \n",
    "                                                 [l1_shape, out_channel*FLAGS.l1_ratio], \n",
    "                                                 tf.float32, \n",
    "                                                 initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "                    l1_biases = tf.get_variable('biases', \n",
    "                                                [out_channel*FLAGS.l1_ratio],\n",
    "                                                tf.float32, \n",
    "                                                initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "            with tf.variable_scope('level2'):\n",
    "                with tf.variable_scope(name):\n",
    "                    l2_s_weights = tf.get_variable('weights_shell', \n",
    "                                                   [l2_s_shape, out_channel*FLAGS.l2_ratio], \n",
    "                                                   tf.float32, \n",
    "                                                   initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "                    l2_s_biases = tf.get_variable('biases_shell', \n",
    "                                                  [out_channel*FLAGS.l2_ratio],\n",
    "                                                  tf.float32, \n",
    "                                                  initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "                    l2_weights_1 = tf.get_variable('weights_1', \n",
    "                                                   [l2_s_shape, out_channel*FLAGS.l1_ratio], \n",
    "                                                   tf.float32, \n",
    "                                                   initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "                    l2_biases_1 = tf.get_variable('biases_1', \n",
    "                                                  [out_channel*FLAGS.l1_ratio],\n",
    "                                                  tf.float32, \n",
    "                                                  initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "                    l2_weights_2 = tf.get_variable('weights_2', \n",
    "                                                   [l1_shape, out_channel*FLAGS.l2_ratio], \n",
    "                                                   tf.float32, \n",
    "                                                   initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "                    l2_biases_2 = tf.get_variable('biases_2', \n",
    "                                                  [out_channel*FLAGS.l2_ratio],\n",
    "                                                  tf.float32, \n",
    "                                                  initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "\n",
    "        l1_mlp_r = tf.add(tf.matmul(l1, l1_weights), l1_biases)\n",
    "        l1_mlp = tf.nn.sigmoid(l1_mlp_r)\n",
    "        \n",
    "        l2_s_mlp = tf.nn.sigmoid(tf.add(tf.matmul(l2_s, l2_s_weights), l2_s_biases))\n",
    "        \n",
    "        l2_mlp_1_r = tf.add(tf.matmul(l2[:,l1_shape:l2_shape], l2_weights_1), l2_biases_1)\n",
    "        l2_mlp_1 = tf.nn.sigmoid(tf.add(l1_mlp_r, l2_mlp_1_r))\n",
    "        l2_mlp_2_r = tf.add(tf.matmul(l2[:,:l1_shape], l2_weights_2), l2_biases_2)\n",
    "        l2_mlp_3_r = tf.add(tf.matmul(l2[:,l1_shape:l2_shape], l2_s_weights), l2_s_biases)\n",
    "        l2_mlp_2 = tf.nn.sigmoid(tf.add(l2_mlp_2_r, l2_mlp_3_r))\n",
    "        l2_mlp = tf.concat((l2_mlp_1, l2_mlp_2), 1)\n",
    "        \n",
    "        \n",
    "    return l1_mlp, l2_s_mlp, l2_mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graphs Ready\n"
     ]
    }
   ],
   "source": [
    "# Network Topology\n",
    "n_input = FLAGS.img_size*FLAGS.img_size\n",
    "n_enc1 = 1024\n",
    "n_enc2 = 512\n",
    "n_enc3 = 256\n",
    "n_dec1 = 512\n",
    "n_dec2 = 1024\n",
    "n_out = 784\n",
    "\n",
    "# Inputs and Outputs\n",
    "ph_pure = tf.placeholder(\"float\", [None, n_input])    # pure image --- core\n",
    "ph_noise= tf.placeholder(\"float\", [None, n_input])    # noise --- shell1\n",
    "ph_crpt = tf.placeholder(\"float\", [None, n_input])    # corrupted image   --- level2\n",
    "\n",
    "\n",
    "# Model\n",
    "def nested_ae_mlp(_X):\n",
    "    l1_enc1, l2_s_enc1, l2_enc1 = _nested_mlp(_X, _X, _X, n_enc1, name='enc1', is_init=True)\n",
    "    l1_enc2, l2_s_enc2, l2_enc2 = _nested_mlp(l1_enc1, l2_s_enc1, l2_enc1, n_enc2, name='enc2')\n",
    "    l1_enc3, l2_s_enc3, l2_enc3 = _nested_mlp(l1_enc2, l2_s_enc2, l2_enc2, n_enc3, name='enc3')\n",
    "    l1_dec1, l2_s_dec1, l2_dec1 = _nested_mlp(l1_enc3, l2_s_enc3, l2_enc3, n_dec1, name='dec1')\n",
    "    l1_dec2, l2_s_dec2, l2_dec2 = _nested_mlp(l1_dec1, l2_s_dec1, l2_dec1, n_dec2, name='dec2')\n",
    "    l1_out, l2_s_out, l2_out = _nested_mlp(l1_dec2, l2_s_dec2, l2_dec2, n_out, name='out',is_last=True)\n",
    "    return l1_out, l2_s_out, l2_out\n",
    "\n",
    "# Generation\n",
    "core_gen, shell2_gen, full_gen = nested_ae_mlp(ph_crpt)   # [None, n_input]\n",
    "\n",
    "# Loss & Optimizer\n",
    "with tf.name_scope(\"loss\") as scope:\n",
    "    loss = tf.reduce_mean(tf.nn.l2_loss(full_gen-ph_crpt)) + 2*tf.reduce_mean(tf.nn.l2_loss(core_gen-ph_pure))\\\n",
    "            + tf.reduce_mean(tf.nn.l2_loss(shell2_gen-ph_noise))\n",
    "    _train_loss = tf.summary.scalar(\"train_loss\", loss)\n",
    "    _test_loss = tf.summary.scalar(\"test_loss\", loss)\n",
    "\n",
    "optm = tf.train.AdamOptimizer(learning_rate=FLAGS.lr).minimize(loss)\n",
    "\n",
    "\n",
    "print(\"Graphs Ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize Ready\n"
     ]
    }
   ],
   "source": [
    "merged = tf.summary.merge_all()\n",
    "tensorboard_path = FLAGS.tboard\n",
    "if not os.path.exists(tensorboard_path):\n",
    "    os.makedirs(tensorboard_path)\n",
    "writer = tf.summary.FileWriter(tensorboard_path)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "print(\"Initialize Ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saver ready\n"
     ]
    }
   ],
   "source": [
    "outputdir = FLAGS.outputs\n",
    "if not os.path.exists(outputdir+'/train'):\n",
    "    os.makedirs(outputdir+'/train')\n",
    "\n",
    "if not os.path.exists(outputdir+'/test'):\n",
    "    os.makedirs(outputdir+'/test')\n",
    "    \n",
    "savedir = FLAGS.nets\n",
    "if not os.path.exists(savedir):\n",
    "    os.makedirs(savedir)\n",
    "    \n",
    "saver = tf.train.Saver(max_to_keep=FLAGS.save_max)\n",
    "print(\"Saver ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "training_epochs = FLAGS.training_epochs\n",
    "batch_size = FLAGS.batch_size\n",
    "display_step = FLAGS.display_step\n",
    "# Plot\n",
    "n_plot = 5    # plot 5 images\n",
    "train_disp_idx = np.random.randint(mnist.train.num_examples, size=n_plot)    # fixed during train time\n",
    "test_disp_idx = np.random.randint(mnist.test.num_examples, size=n_plot)\n",
    "\n",
    "# Initialize\n",
    "sess = tf.Session(config=config)\n",
    "sess.run(init)\n",
    "\n",
    "# Optimize\n",
    "for epoch in range(training_epochs):\n",
    "    total_cost = 0.\n",
    "    n_total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    rand_train_idx = np.random.randint(mnist.train.num_examples, size=batch_size)\n",
    "    rand_test_idx = np.random.randint(mnist.test.num_examples, size=batch_size)\n",
    "    \n",
    "    # Iteration\n",
    "    for i in range(n_total_batch):\n",
    "        batch_pure, _ = mnist.train.next_batch(batch_size)    # pure image\n",
    "        noise = noise_batch(batch_size)    # random noise\n",
    "        batch_crpt = occl(batch_pure, noise)   # corrupted image \n",
    "        feeds = {ph_pure: batch_pure, ph_noise: noise, ph_crpt: batch_crpt}\n",
    "        sess.run(optm, feed_dict=feeds)\n",
    "        #total_cost += sess.run(loss, feed_dict=feeds)\n",
    "    #total_cost = total_cost / mnist.train.num_examples\n",
    "    \n",
    "    train_pure = train_img[rand_train_idx]    # pure image\n",
    "    train_noise = noise_batch(batch_size)    # random noise\n",
    "    train_crpt = occl(train_pure,train_noise)   # corrupted image\n",
    "    train_feeds = {ph_pure: train_pure, ph_noise: train_noise, ph_crpt: train_crpt}\n",
    "    train_loss, tb_train_loss = sess.run([loss,_train_loss], feed_dict=train_feeds)\n",
    "    \n",
    "    test_pure = test_img[rand_test_idx]    # pure image\n",
    "    test_noise = noise_batch(batch_size)    # random noise\n",
    "    test_crpt = occl(test_pure,test_noise)   # corrupted image\n",
    "    test_feeds = {ph_pure: test_pure, ph_noise: test_noise, ph_crpt: test_crpt}\n",
    "    test_loss, tb_test_loss = sess.run([loss,_test_loss], feed_dict=test_feeds)\n",
    "\n",
    "    writer.add_summary(tb_train_loss, epoch)\n",
    "    writer.add_summary(tb_test_loss, epoch)\n",
    "    print(\"Epoch : %03d/%03d  Train_loss : %.7f  Test_loss : %.7f\" \n",
    "          % (epoch+1, training_epochs, train_loss, test_loss))   \n",
    "        \n",
    "    # Display\n",
    "    if (epoch+1) % display_step == 0:\n",
    "        train_gt_pure = train_img[train_disp_idx]    # pure image\n",
    "        train_gt_noise = noise_batch(n_plot)    # random noise\n",
    "        train_gt_crpt = occl(train_gt_pure,train_gt_noise)   # corrupted image\n",
    "        train_gt_feeds = {ph_pure: train_gt_pure, ph_noise: train_gt_noise, ph_crpt: train_gt_crpt}\n",
    "        \n",
    "        test_gt_pure = test_img[test_disp_idx]    # pure image\n",
    "        test_gt_noise = noise_batch(n_plot)    # random noise\n",
    "        test_gt_crpt = occl(test_gt_pure,test_gt_noise)   # corrupted image\n",
    "        test_gt_feeds = {ph_pure: test_gt_pure, ph_noise: test_gt_noise, ph_crpt: test_gt_crpt}\n",
    "        \n",
    "        # generated images\n",
    "        train_gen_pure, train_gen_noise, train_gen_crpt = sess.run([core_gen, shell2_gen, full_gen], \\\n",
    "                                        feed_dict=train_gt_feeds)  # 784-d vector\n",
    "        test_gen_pure, test_gen_noise, test_gen_crpt = sess.run([core_gen, shell2_gen, full_gen], \\\n",
    "                                        feed_dict=test_gt_feeds)  # 784-d vector\n",
    "        \n",
    "        # plotting results from training data\n",
    "        fig, axes = plt.subplots(nrows=4, ncols=n_plot, figsize=(10,2*n_plot))   # displaying 4*n_plot images\n",
    "        plt.setp(axes, xticks=np.arange(0,27,7), yticks=np.arange(0,27,7)) \n",
    "        for j in range(n_plot):\n",
    "            train_disp_gt_crpt = np.reshape(train_gt_crpt[j], [28,28])    # 28x28\n",
    "            axes[0, j].imshow(train_disp_gt_crpt, cmap='gray')   \n",
    "            axes[0, j].set(ylabel='gt_crpt')\n",
    "            axes[0, j].label_outer()\n",
    "            \n",
    "            train_disp_gen_pure = np.reshape(train_gen_pure[j], [28,28])    # 28x28\n",
    "            axes[1, j].imshow(train_disp_gen_pure, cmap='gray')   \n",
    "            axes[1, j].set(ylabel='gen_pure')\n",
    "            axes[1, j].label_outer()\n",
    "            \n",
    "            train_disp_gen_noise = np.reshape(train_gen_noise[j], [28,28])    # 28x28\n",
    "            axes[2, j].imshow(train_disp_gen_noise, cmap='gray')   \n",
    "            axes[2, j].set(ylabel='gen_noise')\n",
    "            axes[2, j].label_outer()\n",
    "            \n",
    "            train_disp_gen_crpt = np.reshape(train_gen_crpt[j], [28,28])    # 28x28\n",
    "            axes[3, j].imshow(train_disp_gen_crpt, cmap='gray')   \n",
    "            axes[3, j].set(ylabel='gen_crpt')\n",
    "            axes[3, j].label_outer()\n",
    "            \n",
    "#             train_disp_gt_pure = np.reshape(train_gt_pure[j], [28,28])\n",
    "#             axes[0, j].imshow(train_disp_gt_pure, cmap='gray')\n",
    "#             axes[0, j].set(ylabel='gt_pure')\n",
    "#             axes[0, j].label_outer()\n",
    "            \n",
    "#             train_disp_gt_noise = np.reshape(train_gt_noise[j], [28,28])    # 28x28\n",
    "#             axes[2, j].imshow(train_disp_gt_noise, cmap='gray')   \n",
    "#             axes[2, j].set(ylabel='gt_noise')\n",
    "#             axes[2, j].label_outer()\n",
    "                    \n",
    "        plt.savefig(outputdir+'/train/epoch %03d' %(epoch+1))    \n",
    "        plt.close(fig)\n",
    "        \n",
    "        # plotting results from testing data\n",
    "        fig, axes = plt.subplots(nrows=4, ncols=n_plot, figsize=(10,2*n_plot))   # displaying 4*n_plot images\n",
    "        plt.setp(axes, xticks=np.arange(0,27,7), yticks=np.arange(0,27,7)) \n",
    "        for k in range(n_plot):\n",
    "            test_disp_gt_crpt = np.reshape(test_gt_crpt[k], [28,28])    # 28x28\n",
    "            axes[0, k].imshow(test_disp_gt_crpt, cmap='gray')   \n",
    "            axes[0, k].set(ylabel='gt_crpt')\n",
    "            axes[0, k].label_outer()\n",
    "            \n",
    "            test_disp_gen_pure = np.reshape(test_gen_pure[k], [28,28])    # 28x28\n",
    "            axes[1, k].imshow(test_disp_gen_pure, cmap='gray')   \n",
    "            axes[1, k].set(ylabel='gen_pure')\n",
    "            axes[1, k].label_outer()           \n",
    "            \n",
    "            test_disp_gen_noise = np.reshape(test_gen_noise[k], [28,28])    # 28x28\n",
    "            axes[2, k].imshow(test_disp_gen_noise, cmap='gray')   \n",
    "            axes[2, k].set(ylabel='gen_noise')\n",
    "            axes[2, k].label_outer()\n",
    "            \n",
    "            test_disp_gen_crpt = np.reshape(test_gen_crpt[k], [28,28])    # 28x28\n",
    "            axes[3, k].imshow(test_disp_gen_crpt, cmap='gray')   \n",
    "            axes[3, k].set(ylabel='gen_crpt')\n",
    "            axes[3, k].label_outer()\n",
    "            \n",
    "#             test_disp_gt_pure = np.reshape(test_gt_pure[k], [28,28])\n",
    "#             axes[0, k].imshow(test_disp_gt_pure, cmap='gray')\n",
    "#             axes[0, k].set(ylabel='gt_pure')\n",
    "#             axes[0, k].label_outer()\n",
    "            \n",
    "#             test_disp_gt_noise = np.reshape(test_gt_noise[k], [28,28])    # 28x28\n",
    "#             axes[2, k].imshow(test_disp_gt_noise, cmap='gray')   \n",
    "#             axes[2, k].set(ylabel='gt_noise')\n",
    "#             axes[2, k].label_outer()\n",
    "                    \n",
    "        plt.savefig(outputdir+'/test/epoch %03d' %(epoch+1))    \n",
    "        plt.close(fig)\n",
    "        \n",
    "        # Save\n",
    "        if (epoch+1) % FLAGS.save_step ==0:\n",
    "            savename = savedir+\"/net-\"+str(epoch+1)+\".ckpt\"\n",
    "            saver.save(sess, savename)\n",
    "            print(\"[%s] SAVED\" % (savename))\n",
    "\n",
    "print(\"Optimization Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
