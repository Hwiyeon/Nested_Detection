{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current version [1.3.1]\n",
      "Packages Loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)  ## just for ignore DeprcationWarning message\n",
    "print(\"Current version [%s]\" %(tf.__version__))\n",
    "print(\"Packages Loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLAGS READY\n"
     ]
    }
   ],
   "source": [
    "# Dataset Configurations\n",
    "tf.app.flags.DEFINE_integer('img_size', 28, \"\"\"Image size of MNIST dataset\"\"\")\n",
    "\n",
    "# Network Configurations\n",
    "tf.app.flags.DEFINE_integer('batch_size', 100, \"\"\"Number of images to process in a batch\"\"\")\n",
    "tf.app.flags.DEFINE_float('l1_ratio', 0.5, \"\"\"Ratio of level1\"\"\")\n",
    "tf.app.flags.DEFINE_float('l2_ratio', 0.5, \"\"\"Ratio of level2\"\"\")\n",
    "\n",
    "# Optimization Configurations\n",
    "tf.app.flags.DEFINE_float('lr', 0.001, \"\"\"Learning rate\"\"\")\n",
    "\n",
    "# Training Configurations\n",
    "tf.app.flags.DEFINE_integer('training_epochs', 500, \"\"\"Number of epochs to run\"\"\")\n",
    "tf.app.flags.DEFINE_integer('display_step', 5, \"\"\"Number of iterations to display training output\"\"\")\n",
    "tf.app.flags.DEFINE_integer('save_step', 5, \"\"\"Number of interations to save checkpoint\"\"\")\n",
    "tf.app.flags.DEFINE_integer('save_max', 5, \"\"\"Number of checkpoints to remain\"\"\")\n",
    "\n",
    "\n",
    "# Save Configurations\n",
    "tf.app.flags.DEFINE_string('nets', './nets', \"\"\"Directory where to write the checkpoints\"\"\")\n",
    "tf.app.flags.DEFINE_string('outputs', './outputs', \"\"\"Directory where to save the output images\"\"\")\n",
    "tf.app.flags.DEFINE_string('tboard', './tensorboard', \"\"\"Directory where to save the tensorboard logs\"\"\")\n",
    "\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "print(\"FLAGS READY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.allow_soft_placement = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../data/train-images-idx3-ubyte.gz\n",
      "Extracting ../../data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../../data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../data/t10k-labels-idx1-ubyte.gz\n",
      "MNIST ready\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('../../data/', one_hot=True)\n",
    "train_img = mnist.train.images\n",
    "train_label = mnist.train.labels\n",
    "test_img = mnist.test.images\n",
    "test_label = mnist.test.labels\n",
    "print(\"MNIST ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating random noise mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise_mask(prob=0.5):\n",
    "    mask = np.zeros([FLAGS.img_size, FLAGS.img_size])\n",
    "    rd = np.random.random()\n",
    "    if rd > prob:\n",
    "        # threshold of the size of masks\n",
    "        uthd = FLAGS.img_size    \n",
    "        lthd = 0     \n",
    "        # mask size should be beween 14x14, 5x5\n",
    "        while(uthd>14 or lthd<5):\n",
    "            ver1 = np.random.random_integers(0, FLAGS.img_size-1, size= 2)   # vertex1\n",
    "            ver2 = np.random.random_integers(0, FLAGS.img_size-1, size= 2)    # vertex2\n",
    "            uthd = np.maximum(np.abs(ver1[0]-ver2[0]), np.abs(ver1[1]-ver2[1]))    # upperbound\n",
    "            lthd = np.minimum(np.abs(ver1[0]-ver2[0]), np.abs(ver1[1]-ver2[1]))    # lowerbound\n",
    "        xmin = np.minimum(ver1[0], ver2[0])    # left x value\n",
    "        xmax = np.maximum(ver1[0], ver2[0])    # right x value\n",
    "        ymin = np.minimum(ver1[1], ver2[1])    # top y value\n",
    "        ymax = np.maximum(ver1[1], ver2[1])    # bottom y value\n",
    "        noise = np.random.random((xmax-xmin+1, ymax-ymin+1))    # random sample in [0,1]\n",
    "        mask[xmin:xmax+1, ymin:ymax+1] = noise    # noise mask with location\n",
    "        mask_meta = [xmin, xmax, ymin, ymax, noise, mask]\n",
    "    mask = np.reshape(mask, [-1])\n",
    "    return mask\n",
    "\n",
    "def noise_batch(batch_num):\n",
    "    # make random noise batch\n",
    "    mask_batch = np.zeros([batch_num, FLAGS.img_size*FLAGS.img_size])\n",
    "    for i in range(batch_num):\n",
    "        mask_batch[i,:] = noise_mask()\n",
    "    return mask_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Occlusion generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def occl(target, disturb):\n",
    "    mask = (disturb==0).astype(float)\n",
    "    masked_target = np.multiply(target, mask)\n",
    "    crpt = np.add(masked_target, disturb)\n",
    "    return crpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nested MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _nested_mlp(l1, l2_s, l2, out_channel, name, stddev=0.1, is_init=False, is_last=False):\n",
    "    l1_shape = l1.get_shape()[1]\n",
    "    l2_shape = l2.get_shape()[1]\n",
    "    l2_s_shape = l2_s.get_shape()[1]\n",
    "    \n",
    "    if is_init:\n",
    "        # input is the input image\n",
    "        with tf.device('/CPU:0'):\n",
    "            with tf.variable_scope('level1'):\n",
    "                with tf.variable_scope(name):\n",
    "                    l1_weights = tf.get_variable('weights', \n",
    "                                                 [l1_shape, out_channel*FLAGS.l1_ratio], \n",
    "                                                 tf.float32, \n",
    "                                                 initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "                    l1_biases = tf.get_variable('biases', \n",
    "                                                [out_channel*FLAGS.l1_ratio],\n",
    "                                                tf.float32, \n",
    "                                                initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "            with tf.variable_scope('level2'):\n",
    "                with tf.variable_scope(name):\n",
    "                    l2_s_weights = tf.get_variable('weights_shell', \n",
    "                                                 [l2_s_shape, out_channel*FLAGS.l2_ratio], \n",
    "                                                 tf.float32, \n",
    "                                                 initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "                    l2_s_biases = tf.get_variable('biases_shell', \n",
    "                                                [out_channel*FLAGS.l2_ratio],\n",
    "                                                tf.float32, \n",
    "                                                initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "                    \n",
    "\n",
    "        l1_mlp = tf.nn.sigmoid(tf.add(tf.matmul(l1, l1_weights), l1_biases))\n",
    "        l2_s_mlp = tf.nn.sigmoid(tf.add(tf.matmul(l2_s, l2_s_weights), l2_s_biases))\n",
    "        l2_mlp = tf.concat((l1_mlp, l2_s_mlp), 1)\n",
    "    \n",
    "    elif is_last:\n",
    "        # output is the generated image\n",
    "        with tf.device('/CPU:0'):\n",
    "            with tf.variable_scope('level1'):\n",
    "                with tf.variable_scope(name):\n",
    "                    l1_weights = tf.get_variable('weights', \n",
    "                                                 [l1_shape, out_channel], \n",
    "                                                 tf.float32, \n",
    "                                                 initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "                    l1_biases = tf.get_variable('biases', \n",
    "                                                [out_channel],\n",
    "                                                tf.float32, \n",
    "                                                initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "            with tf.variable_scope('level2'):\n",
    "                with tf.variable_scope(name):\n",
    "                    l2_s_weights = tf.get_variable('weights_shell', \n",
    "                                                 [l2_s_shape, out_channel], \n",
    "                                                 tf.float32, \n",
    "                                                 initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "                    l2_s_biases = tf.get_variable('biases_shell', \n",
    "                                                [out_channel],\n",
    "                                                tf.float32, \n",
    "                                                initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "                    l2_weights = tf.get_variable('weights', \n",
    "                                                 [l2_shape, out_channel], \n",
    "                                                 tf.float32, \n",
    "                                                 initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "                    l2_biases = tf.get_variable('biases', \n",
    "                                                [out_channel],\n",
    "                                                tf.float32, \n",
    "                                                initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "                    \n",
    "\n",
    "        l1_mlp = tf.nn.sigmoid(tf.add(tf.matmul(l1, l1_weights), l1_biases))\n",
    "        l2_s_mlp = tf.nn.sigmoid(tf.add(tf.matmul(l2_s, l2_s_weights), l2_s_biases))\n",
    "        l2_mlp = tf.nn.sigmoid(tf.add(tf.matmul(l2, l2_weights), l2_biases))\n",
    "                                 \n",
    "    else:\n",
    "        with tf.device('/CPU:0'):\n",
    "            with tf.variable_scope('level1'):\n",
    "                with tf.variable_scope(name):\n",
    "                    l1_weights = tf.get_variable('weights', \n",
    "                                                 [l1_shape, out_channel*FLAGS.l1_ratio], \n",
    "                                                 tf.float32, \n",
    "                                                 initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "                    l1_biases = tf.get_variable('biases', \n",
    "                                                [out_channel*FLAGS.l1_ratio],\n",
    "                                                tf.float32, \n",
    "                                                initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "            with tf.variable_scope('level2'):\n",
    "                with tf.variable_scope(name):\n",
    "                    l2_s_weights = tf.get_variable('weights_shell', \n",
    "                                                   [l2_s_shape, out_channel*FLAGS.l2_ratio], \n",
    "                                                   tf.float32, \n",
    "                                                   initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "                    l2_s_biases = tf.get_variable('biases_shell', \n",
    "                                                  [out_channel*FLAGS.l2_ratio],\n",
    "                                                  tf.float32, \n",
    "                                                  initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "                    l2_weights_1 = tf.get_variable('weights_1', \n",
    "                                                   [l2_s_shape, out_channel*FLAGS.l1_ratio], \n",
    "                                                   tf.float32, \n",
    "                                                   initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "                    l2_biases_1 = tf.get_variable('biases_1', \n",
    "                                                  [out_channel*FLAGS.l1_ratio],\n",
    "                                                  tf.float32, \n",
    "                                                  initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "                    l2_weights_2 = tf.get_variable('weights_2', \n",
    "                                                   [l1_shape, out_channel*FLAGS.l2_ratio], \n",
    "                                                   tf.float32, \n",
    "                                                   initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "                    l2_biases_2 = tf.get_variable('biases_2', \n",
    "                                                  [out_channel*FLAGS.l2_ratio],\n",
    "                                                  tf.float32, \n",
    "                                                  initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "\n",
    "        l1_mlp_r = tf.add(tf.matmul(l1, l1_weights), l1_biases)\n",
    "        l1_mlp = tf.nn.sigmoid(l1_mlp_r)\n",
    "        \n",
    "        l2_s_mlp = tf.nn.sigmoid(tf.add(tf.matmul(l2_s, l2_s_weights), l2_s_biases))\n",
    "        \n",
    "        l2_mlp_1_r = tf.add(tf.matmul(l2[:,l1_shape:l2_shape], l2_weights_1), l2_biases_1)\n",
    "        l2_mlp_1 = tf.nn.sigmoid(tf.add(l1_mlp_r, l2_mlp_1_r))\n",
    "        l2_mlp_2_r = tf.add(tf.matmul(l2[:,:l1_shape], l2_weights_2), l2_biases_2)\n",
    "        l2_mlp_3_r = tf.add(tf.matmul(l2[:,l1_shape:l2_shape], l2_s_weights), l2_s_biases)\n",
    "        l2_mlp_2 = tf.nn.sigmoid(tf.add(l2_mlp_2_r, l2_mlp_3_r))\n",
    "        l2_mlp = tf.concat((l2_mlp_1, l2_mlp_2), 1)\n",
    "        \n",
    "        \n",
    "    return l1_mlp, l2_s_mlp, l2_mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graphs Ready\n"
     ]
    }
   ],
   "source": [
    "# Network Topology\n",
    "n_input = FLAGS.img_size*FLAGS.img_size\n",
    "n_enc1 = 1024\n",
    "n_enc2 = 512\n",
    "n_enc3 = 256\n",
    "n_dec1 = 512\n",
    "n_dec2 = 1024\n",
    "n_out = 784\n",
    "\n",
    "# Inputs and Outputs\n",
    "ph_pure = tf.placeholder(\"float\", [None, n_input])    # pure image --- core\n",
    "ph_noise= tf.placeholder(\"float\", [None, n_input])    # noise --- shell1\n",
    "ph_crpt = tf.placeholder(\"float\", [None, n_input])    # corrupted image   --- level2\n",
    "\n",
    "\n",
    "# Model\n",
    "def nested_ae_mlp(_X):\n",
    "    l1_enc1, l2_s_enc1, l2_enc1 = _nested_mlp(_X, _X, _X, n_enc1, name='enc1', is_init=True)\n",
    "    l1_enc2, l2_s_enc2, l2_enc2 = _nested_mlp(l1_enc1, l2_s_enc1, l2_enc1, n_enc2, name='enc2')\n",
    "    l1_enc3, l2_s_enc3, l2_enc3 = _nested_mlp(l1_enc2, l2_s_enc2, l2_enc2, n_enc3, name='enc3')\n",
    "    l1_dec1, l2_s_dec1, l2_dec1 = _nested_mlp(l1_enc3, l2_s_enc3, l2_enc3, n_dec1, name='dec1')\n",
    "    l1_dec2, l2_s_dec2, l2_dec2 = _nested_mlp(l1_dec1, l2_s_dec1, l2_dec1, n_dec2, name='dec2')\n",
    "    l1_out, l2_s_out, l2_out = _nested_mlp(l1_dec2, l2_s_dec2, l2_dec2, n_out, name='out',is_last=True)\n",
    "    return l1_out, l2_s_out, l2_out\n",
    "\n",
    "# Generation\n",
    "core_gen, shell2_gen, full_gen = nested_ae_mlp(ph_crpt)   # [None, n_input]\n",
    "\n",
    "# Loss & Optimizer\n",
    "with tf.name_scope(\"loss\") as scope:\n",
    "    loss = tf.reduce_mean(tf.nn.l2_loss(full_gen-ph_crpt)) + tf.reduce_mean(tf.nn.l2_loss(core_gen-ph_pure))\\\n",
    "            + tf.reduce_mean(tf.nn.l2_loss(shell2_gen-ph_noise))\n",
    "    _train_loss = tf.summary.scalar(\"train_loss\", loss)\n",
    "    _test_loss = tf.summary.scalar(\"test_loss\", loss)\n",
    "\n",
    "optm = tf.train.AdamOptimizer(learning_rate=FLAGS.lr).minimize(loss)\n",
    "\n",
    "\n",
    "print(\"Graphs Ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize Ready\n"
     ]
    }
   ],
   "source": [
    "merged = tf.summary.merge_all()\n",
    "tensorboard_path = FLAGS.tboard\n",
    "if not os.path.exists(tensorboard_path):\n",
    "    os.makedirs(tensorboard_path)\n",
    "writer = tf.summary.FileWriter(tensorboard_path)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "print(\"Initialize Ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saver ready\n"
     ]
    }
   ],
   "source": [
    "outputdir = FLAGS.outputs\n",
    "if not os.path.exists(outputdir+'/train'):\n",
    "    os.makedirs(outputdir+'/train')\n",
    "\n",
    "if not os.path.exists(outputdir+'/test'):\n",
    "    os.makedirs(outputdir+'/test')\n",
    "    \n",
    "savedir = FLAGS.nets\n",
    "if not os.path.exists(savedir):\n",
    "    os.makedirs(savedir)\n",
    "    \n",
    "saver = tf.train.Saver(max_to_keep=FLAGS.save_max)\n",
    "print(\"Saver ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 001/200  Train_loss : 3750.4719238  Test_loss : 3981.7995605\n",
      "Epoch : 002/200  Train_loss : 2493.4909668  Test_loss : 2477.6020508\n",
      "Epoch : 003/200  Train_loss : 2373.2622070  Test_loss : 2255.8701172\n",
      "Epoch : 004/200  Train_loss : 2383.2734375  Test_loss : 1888.1566162\n",
      "Epoch : 005/200  Train_loss : 2086.0856934  Test_loss : 1882.8995361\n",
      "[./nets/net-5.ckpt] SAVED\n",
      "Epoch : 006/200  Train_loss : 2024.3115234  Test_loss : 1880.4287109\n",
      "Epoch : 007/200  Train_loss : 1761.0963135  Test_loss : 1669.1197510\n",
      "Epoch : 008/200  Train_loss : 1777.3590088  Test_loss : 1699.7231445\n",
      "Epoch : 009/200  Train_loss : 1676.4223633  Test_loss : 1633.1688232\n",
      "Epoch : 010/200  Train_loss : 1479.9143066  Test_loss : 1609.4501953\n",
      "[./nets/net-10.ckpt] SAVED\n",
      "Epoch : 011/200  Train_loss : 1664.3120117  Test_loss : 1494.8452148\n",
      "Epoch : 012/200  Train_loss : 1350.8072510  Test_loss : 1362.0762939\n",
      "Epoch : 013/200  Train_loss : 1333.7958984  Test_loss : 1432.0540771\n",
      "Epoch : 014/200  Train_loss : 1409.1516113  Test_loss : 1312.7562256\n",
      "Epoch : 015/200  Train_loss : 1379.2642822  Test_loss : 1425.0612793\n",
      "[./nets/net-15.ckpt] SAVED\n",
      "Epoch : 016/200  Train_loss : 1364.5759277  Test_loss : 1258.5772705\n",
      "Epoch : 017/200  Train_loss : 1157.5504150  Test_loss : 1401.5292969\n",
      "Epoch : 018/200  Train_loss : 1239.0823975  Test_loss : 1183.1889648\n",
      "Epoch : 019/200  Train_loss : 1122.3146973  Test_loss : 1343.0465088\n",
      "Epoch : 020/200  Train_loss : 1437.2906494  Test_loss : 1249.8679199\n",
      "[./nets/net-20.ckpt] SAVED\n",
      "Epoch : 021/200  Train_loss : 1078.0498047  Test_loss : 1400.7077637\n",
      "Epoch : 022/200  Train_loss : 1143.8609619  Test_loss : 1163.4218750\n",
      "Epoch : 023/200  Train_loss : 995.5229492  Test_loss : 1233.7316895\n",
      "Epoch : 024/200  Train_loss : 1077.2285156  Test_loss : 1053.1162109\n",
      "Epoch : 025/200  Train_loss : 1251.2863770  Test_loss : 1163.0283203\n",
      "[./nets/net-25.ckpt] SAVED\n",
      "Epoch : 026/200  Train_loss : 1001.0764160  Test_loss : 1097.0585938\n",
      "Epoch : 027/200  Train_loss : 1171.5653076  Test_loss : 1155.2687988\n",
      "Epoch : 028/200  Train_loss : 1195.9056396  Test_loss : 1289.7614746\n",
      "Epoch : 029/200  Train_loss : 1052.4975586  Test_loss : 1140.7913818\n",
      "Epoch : 030/200  Train_loss : 1056.0709229  Test_loss : 977.7113037\n",
      "[./nets/net-30.ckpt] SAVED\n",
      "Epoch : 031/200  Train_loss : 1085.7458496  Test_loss : 1043.1945801\n",
      "Epoch : 032/200  Train_loss : 1172.3260498  Test_loss : 1167.6901855\n",
      "Epoch : 033/200  Train_loss : 1046.3752441  Test_loss : 1024.2224121\n",
      "Epoch : 034/200  Train_loss : 1142.6047363  Test_loss : 1035.1740723\n",
      "Epoch : 035/200  Train_loss : 1039.2382812  Test_loss : 1157.6486816\n",
      "[./nets/net-35.ckpt] SAVED\n",
      "Epoch : 036/200  Train_loss : 1046.9776611  Test_loss : 1122.9726562\n",
      "Epoch : 037/200  Train_loss : 889.9200439  Test_loss : 1103.6645508\n",
      "Epoch : 038/200  Train_loss : 1159.0009766  Test_loss : 917.8031616\n",
      "Epoch : 039/200  Train_loss : 1085.1479492  Test_loss : 1254.5516357\n",
      "Epoch : 040/200  Train_loss : 976.6729736  Test_loss : 1068.3532715\n",
      "[./nets/net-40.ckpt] SAVED\n",
      "Epoch : 041/200  Train_loss : 1040.7089844  Test_loss : 979.9637451\n",
      "Epoch : 042/200  Train_loss : 1028.1708984  Test_loss : 1000.6603394\n",
      "Epoch : 043/200  Train_loss : 983.1551514  Test_loss : 1001.8327637\n",
      "Epoch : 044/200  Train_loss : 1063.6586914  Test_loss : 1053.5070801\n",
      "Epoch : 045/200  Train_loss : 1013.0937500  Test_loss : 888.2554932\n",
      "[./nets/net-45.ckpt] SAVED\n",
      "Epoch : 046/200  Train_loss : 921.8576660  Test_loss : 981.0281372\n",
      "Epoch : 047/200  Train_loss : 991.0922852  Test_loss : 920.8334351\n",
      "Epoch : 048/200  Train_loss : 1048.9938965  Test_loss : 860.8826904\n",
      "Epoch : 049/200  Train_loss : 972.8802490  Test_loss : 985.3754272\n",
      "Epoch : 050/200  Train_loss : 799.3009644  Test_loss : 964.6170654\n",
      "[./nets/net-50.ckpt] SAVED\n",
      "Epoch : 051/200  Train_loss : 862.0846558  Test_loss : 924.4659424\n",
      "Epoch : 052/200  Train_loss : 923.8112183  Test_loss : 1040.1474609\n",
      "Epoch : 053/200  Train_loss : 989.1149902  Test_loss : 961.0360107\n",
      "Epoch : 054/200  Train_loss : 854.7669678  Test_loss : 947.1141968\n",
      "Epoch : 055/200  Train_loss : 910.8597412  Test_loss : 917.0666504\n",
      "[./nets/net-55.ckpt] SAVED\n",
      "Epoch : 056/200  Train_loss : 916.7471924  Test_loss : 913.6570435\n",
      "Epoch : 057/200  Train_loss : 894.4129028  Test_loss : 903.5954590\n",
      "Epoch : 058/200  Train_loss : 781.5963135  Test_loss : 918.8240967\n",
      "Epoch : 059/200  Train_loss : 948.4104614  Test_loss : 943.5328979\n",
      "Epoch : 060/200  Train_loss : 1072.4736328  Test_loss : 899.5880127\n",
      "[./nets/net-60.ckpt] SAVED\n",
      "Epoch : 061/200  Train_loss : 827.9148560  Test_loss : 871.8862305\n",
      "Epoch : 062/200  Train_loss : 833.7419434  Test_loss : 1005.5344238\n",
      "Epoch : 063/200  Train_loss : 906.0966187  Test_loss : 888.9294434\n",
      "Epoch : 064/200  Train_loss : 861.3811646  Test_loss : 863.1181030\n",
      "Epoch : 065/200  Train_loss : 863.4884644  Test_loss : 1049.8964844\n",
      "[./nets/net-65.ckpt] SAVED\n",
      "Epoch : 066/200  Train_loss : 923.1821289  Test_loss : 895.2519531\n",
      "Epoch : 067/200  Train_loss : 942.7142334  Test_loss : 979.3472900\n",
      "Epoch : 068/200  Train_loss : 1019.3841553  Test_loss : 924.6977539\n",
      "Epoch : 069/200  Train_loss : 762.8990479  Test_loss : 970.3823242\n",
      "Epoch : 070/200  Train_loss : 850.1201172  Test_loss : 842.6529541\n",
      "[./nets/net-70.ckpt] SAVED\n",
      "Epoch : 071/200  Train_loss : 804.6811523  Test_loss : 875.2053833\n",
      "Epoch : 072/200  Train_loss : 976.8901367  Test_loss : 848.7604980\n",
      "Epoch : 073/200  Train_loss : 824.5603638  Test_loss : 913.2781372\n",
      "Epoch : 074/200  Train_loss : 710.6427002  Test_loss : 833.9053345\n",
      "Epoch : 075/200  Train_loss : 870.0007935  Test_loss : 843.7359009\n",
      "[./nets/net-75.ckpt] SAVED\n",
      "Epoch : 076/200  Train_loss : 821.1406250  Test_loss : 1011.3949585\n",
      "Epoch : 077/200  Train_loss : 860.2575684  Test_loss : 862.9880371\n",
      "Epoch : 078/200  Train_loss : 829.7965698  Test_loss : 798.6444702\n",
      "Epoch : 079/200  Train_loss : 776.1306152  Test_loss : 870.7294922\n",
      "Epoch : 080/200  Train_loss : 914.4985352  Test_loss : 920.7510376\n",
      "[./nets/net-80.ckpt] SAVED\n",
      "Epoch : 081/200  Train_loss : 847.5681152  Test_loss : 755.7918091\n",
      "Epoch : 082/200  Train_loss : 767.8171997  Test_loss : 772.8707886\n",
      "Epoch : 083/200  Train_loss : 748.2818604  Test_loss : 893.7276611\n",
      "Epoch : 084/200  Train_loss : 873.9650879  Test_loss : 894.8662720\n",
      "Epoch : 085/200  Train_loss : 767.3149414  Test_loss : 808.6754150\n",
      "[./nets/net-85.ckpt] SAVED\n",
      "Epoch : 086/200  Train_loss : 886.5066528  Test_loss : 711.5855713\n",
      "Epoch : 087/200  Train_loss : 749.5761108  Test_loss : 749.1514893\n",
      "Epoch : 088/200  Train_loss : 866.1641846  Test_loss : 858.6385498\n",
      "Epoch : 089/200  Train_loss : 1045.6959229  Test_loss : 843.0807495\n",
      "Epoch : 090/200  Train_loss : 794.1801758  Test_loss : 917.1458130\n",
      "[./nets/net-90.ckpt] SAVED\n",
      "Epoch : 091/200  Train_loss : 919.4973145  Test_loss : 740.6105957\n",
      "Epoch : 092/200  Train_loss : 888.1920166  Test_loss : 902.0617676\n",
      "Epoch : 093/200  Train_loss : 986.1052246  Test_loss : 935.0003052\n",
      "Epoch : 094/200  Train_loss : 852.2777100  Test_loss : 820.8625488\n",
      "Epoch : 095/200  Train_loss : 783.2736816  Test_loss : 792.0018311\n",
      "[./nets/net-95.ckpt] SAVED\n",
      "Epoch : 096/200  Train_loss : 846.1055298  Test_loss : 800.8612061\n",
      "Epoch : 097/200  Train_loss : 857.5371704  Test_loss : 822.3064575\n",
      "Epoch : 098/200  Train_loss : 678.9748535  Test_loss : 789.7193604\n",
      "Epoch : 099/200  Train_loss : 809.1354980  Test_loss : 906.3121338\n",
      "Epoch : 100/200  Train_loss : 884.5410156  Test_loss : 759.4587402\n",
      "[./nets/net-100.ckpt] SAVED\n",
      "Epoch : 101/200  Train_loss : 672.1863403  Test_loss : 846.5388184\n",
      "Epoch : 102/200  Train_loss : 828.3027344  Test_loss : 772.9082031\n",
      "Epoch : 103/200  Train_loss : 778.1871948  Test_loss : 898.1974487\n",
      "Epoch : 104/200  Train_loss : 892.7514648  Test_loss : 834.9590454\n",
      "Epoch : 105/200  Train_loss : 719.6881104  Test_loss : 767.1256104\n",
      "[./nets/net-105.ckpt] SAVED\n",
      "Epoch : 106/200  Train_loss : 774.7344971  Test_loss : 940.1573486\n",
      "Epoch : 107/200  Train_loss : 747.3873291  Test_loss : 791.9350586\n",
      "Epoch : 108/200  Train_loss : 768.2861938  Test_loss : 803.1437378\n",
      "Epoch : 109/200  Train_loss : 755.3615723  Test_loss : 673.7561646\n",
      "Epoch : 110/200  Train_loss : 863.5100708  Test_loss : 786.7033081\n",
      "[./nets/net-110.ckpt] SAVED\n",
      "Epoch : 111/200  Train_loss : 681.6876831  Test_loss : 743.3535156\n",
      "Epoch : 112/200  Train_loss : 725.5293579  Test_loss : 838.0836182\n",
      "Epoch : 113/200  Train_loss : 686.4456177  Test_loss : 635.9099121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 114/200  Train_loss : 810.7143555  Test_loss : 621.5209961\n",
      "Epoch : 115/200  Train_loss : 860.5964355  Test_loss : 743.0825195\n",
      "[./nets/net-115.ckpt] SAVED\n",
      "Epoch : 116/200  Train_loss : 760.8442383  Test_loss : 674.8624268\n",
      "Epoch : 117/200  Train_loss : 684.0225830  Test_loss : 684.7608643\n",
      "Epoch : 118/200  Train_loss : 735.5233154  Test_loss : 899.1375732\n",
      "Epoch : 119/200  Train_loss : 723.2479858  Test_loss : 739.5716553\n",
      "Epoch : 120/200  Train_loss : 800.5269165  Test_loss : 772.4423218\n",
      "[./nets/net-120.ckpt] SAVED\n",
      "Epoch : 121/200  Train_loss : 681.9224854  Test_loss : 753.9846191\n",
      "Epoch : 122/200  Train_loss : 759.7180786  Test_loss : 718.7767334\n",
      "Epoch : 123/200  Train_loss : 786.2077637  Test_loss : 766.0856934\n",
      "Epoch : 124/200  Train_loss : 688.8110962  Test_loss : 683.5870361\n",
      "Epoch : 125/200  Train_loss : 812.8361816  Test_loss : 774.3776245\n",
      "[./nets/net-125.ckpt] SAVED\n",
      "Epoch : 126/200  Train_loss : 774.7322998  Test_loss : 725.0283813\n",
      "Epoch : 127/200  Train_loss : 653.0882568  Test_loss : 777.9696045\n",
      "Epoch : 128/200  Train_loss : 717.3661499  Test_loss : 845.7945557\n",
      "Epoch : 129/200  Train_loss : 707.6998291  Test_loss : 710.3566895\n",
      "Epoch : 130/200  Train_loss : 570.0100708  Test_loss : 779.5930176\n",
      "[./nets/net-130.ckpt] SAVED\n",
      "Epoch : 131/200  Train_loss : 741.9143066  Test_loss : 753.3336182\n",
      "Epoch : 132/200  Train_loss : 779.8448486  Test_loss : 860.2406006\n",
      "Epoch : 133/200  Train_loss : 711.1853027  Test_loss : 732.4884033\n",
      "Epoch : 134/200  Train_loss : 797.5253906  Test_loss : 662.6677246\n",
      "Epoch : 135/200  Train_loss : 715.3092041  Test_loss : 821.5889893\n",
      "[./nets/net-135.ckpt] SAVED\n",
      "Epoch : 136/200  Train_loss : 628.3621826  Test_loss : 716.7874146\n",
      "Epoch : 137/200  Train_loss : 758.8129272  Test_loss : 722.6422119\n",
      "Epoch : 138/200  Train_loss : 665.2203369  Test_loss : 734.1375732\n",
      "Epoch : 139/200  Train_loss : 649.7751465  Test_loss : 670.6489868\n",
      "Epoch : 140/200  Train_loss : 593.6210327  Test_loss : 760.6798706\n",
      "[./nets/net-140.ckpt] SAVED\n",
      "Epoch : 141/200  Train_loss : 760.3089600  Test_loss : 641.2532959\n",
      "Epoch : 142/200  Train_loss : 620.5894775  Test_loss : 651.9176025\n",
      "Epoch : 143/200  Train_loss : 744.1564331  Test_loss : 687.9169922\n",
      "Epoch : 144/200  Train_loss : 706.6160889  Test_loss : 660.2526855\n",
      "Epoch : 145/200  Train_loss : 706.4078979  Test_loss : 667.3068848\n",
      "[./nets/net-145.ckpt] SAVED\n",
      "Epoch : 146/200  Train_loss : 649.0415039  Test_loss : 716.6658936\n",
      "Epoch : 147/200  Train_loss : 713.2438965  Test_loss : 659.9813232\n",
      "Epoch : 148/200  Train_loss : 756.0079346  Test_loss : 867.5242920\n",
      "Epoch : 149/200  Train_loss : 673.6772461  Test_loss : 751.6606445\n",
      "Epoch : 150/200  Train_loss : 680.1959839  Test_loss : 642.6179199\n",
      "[./nets/net-150.ckpt] SAVED\n",
      "Epoch : 151/200  Train_loss : 734.6181641  Test_loss : 636.9775391\n",
      "Epoch : 152/200  Train_loss : 755.4072266  Test_loss : 723.6906738\n",
      "Epoch : 153/200  Train_loss : 723.9495239  Test_loss : 642.1710205\n",
      "Epoch : 154/200  Train_loss : 648.1715698  Test_loss : 708.6411743\n",
      "Epoch : 155/200  Train_loss : 641.0711670  Test_loss : 629.3893433\n",
      "[./nets/net-155.ckpt] SAVED\n",
      "Epoch : 156/200  Train_loss : 687.7592773  Test_loss : 668.9705811\n",
      "Epoch : 157/200  Train_loss : 757.6534424  Test_loss : 547.7338867\n",
      "Epoch : 158/200  Train_loss : 596.1297607  Test_loss : 777.9375610\n",
      "Epoch : 159/200  Train_loss : 687.4458008  Test_loss : 618.6172485\n",
      "Epoch : 160/200  Train_loss : 767.8807983  Test_loss : 628.4908447\n",
      "[./nets/net-160.ckpt] SAVED\n",
      "Epoch : 161/200  Train_loss : 580.8583374  Test_loss : 620.9141846\n",
      "Epoch : 162/200  Train_loss : 721.0144043  Test_loss : 596.7321777\n",
      "Epoch : 163/200  Train_loss : 623.4651489  Test_loss : 749.5556641\n",
      "Epoch : 164/200  Train_loss : 708.9693604  Test_loss : 687.2340698\n",
      "Epoch : 165/200  Train_loss : 788.3299561  Test_loss : 574.8231201\n",
      "[./nets/net-165.ckpt] SAVED\n",
      "Epoch : 166/200  Train_loss : 745.0325928  Test_loss : 650.4667969\n",
      "Epoch : 167/200  Train_loss : 613.2232666  Test_loss : 691.3348389\n",
      "Epoch : 168/200  Train_loss : 726.1695557  Test_loss : 591.4538574\n",
      "Epoch : 169/200  Train_loss : 711.7075195  Test_loss : 577.3067627\n",
      "Epoch : 170/200  Train_loss : 656.0369873  Test_loss : 629.1639404\n",
      "[./nets/net-170.ckpt] SAVED\n",
      "Epoch : 171/200  Train_loss : 700.0455933  Test_loss : 633.5205078\n",
      "Epoch : 172/200  Train_loss : 605.5911865  Test_loss : 688.4849243\n",
      "Epoch : 173/200  Train_loss : 640.5692139  Test_loss : 656.6617432\n",
      "Epoch : 174/200  Train_loss : 639.6501465  Test_loss : 649.4061279\n",
      "Epoch : 175/200  Train_loss : 685.6718140  Test_loss : 594.5933838\n",
      "[./nets/net-175.ckpt] SAVED\n",
      "Epoch : 176/200  Train_loss : 682.2461548  Test_loss : 739.8110352\n",
      "Epoch : 177/200  Train_loss : 555.8064575  Test_loss : 639.4940186\n",
      "Epoch : 178/200  Train_loss : 659.5427246  Test_loss : 792.1515503\n",
      "Epoch : 179/200  Train_loss : 604.2376709  Test_loss : 639.6488037\n",
      "Epoch : 180/200  Train_loss : 584.9562988  Test_loss : 661.7218628\n",
      "[./nets/net-180.ckpt] SAVED\n",
      "Epoch : 181/200  Train_loss : 574.8367310  Test_loss : 720.8507690\n",
      "Epoch : 182/200  Train_loss : 513.1380615  Test_loss : 618.9382935\n",
      "Epoch : 183/200  Train_loss : 666.0352173  Test_loss : 678.0010986\n",
      "Epoch : 184/200  Train_loss : 582.1939697  Test_loss : 661.6566772\n",
      "Epoch : 185/200  Train_loss : 731.2173462  Test_loss : 564.0378418\n",
      "[./nets/net-185.ckpt] SAVED\n",
      "Epoch : 186/200  Train_loss : 617.6323853  Test_loss : 636.1364746\n",
      "Epoch : 187/200  Train_loss : 615.1687012  Test_loss : 573.4521484\n",
      "Epoch : 188/200  Train_loss : 561.6043091  Test_loss : 644.1127319\n",
      "Epoch : 189/200  Train_loss : 550.1352539  Test_loss : 669.2683105\n",
      "Epoch : 190/200  Train_loss : 737.9293213  Test_loss : 656.4436646\n",
      "[./nets/net-190.ckpt] SAVED\n",
      "Epoch : 191/200  Train_loss : 549.9218750  Test_loss : 646.5296021\n",
      "Epoch : 192/200  Train_loss : 624.9923706  Test_loss : 624.8692627\n",
      "Epoch : 193/200  Train_loss : 692.1284790  Test_loss : 677.0765381\n",
      "Epoch : 194/200  Train_loss : 708.8363647  Test_loss : 527.5691528\n",
      "Epoch : 195/200  Train_loss : 616.2886963  Test_loss : 569.5269165\n",
      "[./nets/net-195.ckpt] SAVED\n",
      "Epoch : 196/200  Train_loss : 693.8565063  Test_loss : 731.2092285\n",
      "Epoch : 197/200  Train_loss : 657.1248169  Test_loss : 557.4605713\n",
      "Epoch : 198/200  Train_loss : 665.1907959  Test_loss : 628.9787598\n",
      "Epoch : 199/200  Train_loss : 534.8152466  Test_loss : 626.8908081\n",
      "Epoch : 200/200  Train_loss : 641.0501709  Test_loss : 565.9516602\n",
      "[./nets/net-200.ckpt] SAVED\n",
      "Optimization Finished\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "training_epochs = FLAGS.training_epochs\n",
    "batch_size = FLAGS.batch_size\n",
    "display_step = FLAGS.display_step\n",
    "# Plot\n",
    "n_plot = 5    # plot 5 images\n",
    "train_disp_idx = np.random.randint(mnist.train.num_examples, size=n_plot)    # fixed during train time\n",
    "test_disp_idx = np.random.randint(mnist.test.num_examples, size=n_plot)\n",
    "\n",
    "# Initialize\n",
    "sess = tf.Session(config=config)\n",
    "sess.run(init)\n",
    "\n",
    "# Optimize\n",
    "for epoch in range(training_epochs):\n",
    "    total_cost = 0.\n",
    "    n_total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    rand_train_idx = np.random.randint(mnist.train.num_examples, size=batch_size)\n",
    "    rand_test_idx = np.random.randint(mnist.test.num_examples, size=batch_size)\n",
    "    \n",
    "    # Iteration\n",
    "    for i in range(n_total_batch):\n",
    "        batch_pure, _ = mnist.train.next_batch(batch_size)    # pure image\n",
    "        noise = noise_batch(batch_size)    # random noise\n",
    "        batch_crpt = occl(batch_pure, noise)   # corrupted image \n",
    "        feeds = {ph_pure: batch_pure, ph_noise: noise, ph_crpt: batch_crpt}\n",
    "        sess.run(optm, feed_dict=feeds)\n",
    "        #total_cost += sess.run(loss, feed_dict=feeds)\n",
    "    #total_cost = total_cost / mnist.train.num_examples\n",
    "    \n",
    "    train_pure = train_img[rand_train_idx]    # pure image\n",
    "    train_noise = noise_batch(batch_size)    # random noise\n",
    "    train_crpt = occl(train_pure,train_noise)   # corrupted image\n",
    "    train_feeds = {ph_pure: train_pure, ph_noise: train_noise, ph_crpt: train_crpt}\n",
    "    train_loss, tb_train_loss = sess.run([loss,_train_loss], feed_dict=train_feeds)\n",
    "    \n",
    "    test_pure = test_img[rand_test_idx]    # pure image\n",
    "    test_noise = noise_batch(batch_size)    # random noise\n",
    "    test_crpt = occl(test_pure,test_noise)   # corrupted image\n",
    "    test_feeds = {ph_pure: test_pure, ph_noise: test_noise, ph_crpt: test_crpt}\n",
    "    test_loss, tb_test_loss = sess.run([loss,_test_loss], feed_dict=test_feeds)\n",
    "\n",
    "    writer.add_summary(tb_train_loss, epoch)\n",
    "    writer.add_summary(tb_test_loss, epoch)\n",
    "    print(\"Epoch : %03d/%03d  Train_loss : %.7f  Test_loss : %.7f\" \n",
    "          % (epoch+1, training_epochs, train_loss, test_loss))   \n",
    "        \n",
    "    # Display\n",
    "    if (epoch+1) % display_step == 0:\n",
    "        train_gt_pure = train_img[train_disp_idx]    # pure image\n",
    "        train_gt_noise = noise_batch(n_plot)    # random noise\n",
    "        train_gt_crpt = occl(train_gt_pure,train_gt_noise)   # corrupted image\n",
    "        train_gt_feeds = {ph_pure: train_gt_pure, ph_noise: train_gt_noise, ph_crpt: train_gt_crpt}\n",
    "        \n",
    "        test_gt_pure = test_img[test_disp_idx]    # pure image\n",
    "        test_gt_noise = noise_batch(n_plot)    # random noise\n",
    "        test_gt_crpt = occl(test_gt_pure,test_gt_noise)   # corrupted image\n",
    "        test_gt_feeds = {ph_pure: test_gt_pure, ph_noise: test_gt_noise, ph_crpt: test_gt_crpt}\n",
    "        \n",
    "        # generated images\n",
    "        train_gen_pure, train_gen_noise, train_gen_crpt = sess.run([core_gen, shell2_gen, full_gen], \\\n",
    "                                        feed_dict=train_gt_feeds)  # 784-d vector\n",
    "        test_gen_pure, test_gen_noise, test_gen_crpt = sess.run([core_gen, shell2_gen, full_gen], \\\n",
    "                                        feed_dict=test_gt_feeds)  # 784-d vector\n",
    "        \n",
    "        # plotting results from training data\n",
    "        fig, axes = plt.subplots(nrows=4, ncols=n_plot, figsize=(10,2*n_plot))   # displaying 4*n_plot images\n",
    "        plt.setp(axes, xticks=np.arange(0,27,7), yticks=np.arange(0,27,7)) \n",
    "        for j in range(n_plot):\n",
    "            train_disp_gt_crpt = np.reshape(train_gt_crpt[j], [28,28])    # 28x28\n",
    "            axes[0, j].imshow(train_disp_gt_crpt, cmap='gray')   \n",
    "            axes[0, j].set(ylabel='gt_crpt')\n",
    "            axes[0, j].label_outer()\n",
    "            \n",
    "            train_disp_gen_pure = np.reshape(train_gen_pure[j], [28,28])    # 28x28\n",
    "            axes[1, j].imshow(train_disp_gen_pure, cmap='gray')   \n",
    "            axes[1, j].set(ylabel='gen_pure')\n",
    "            axes[1, j].label_outer()\n",
    "            \n",
    "            train_disp_gen_noise = np.reshape(train_gen_noise[j], [28,28])    # 28x28\n",
    "            axes[2, j].imshow(train_disp_gen_noise, cmap='gray')   \n",
    "            axes[2, j].set(ylabel='gen_noise')\n",
    "            axes[2, j].label_outer()\n",
    "            \n",
    "            train_disp_gen_crpt = np.reshape(train_gen_crpt[j], [28,28])    # 28x28\n",
    "            axes[3, j].imshow(train_disp_gen_crpt, cmap='gray')   \n",
    "            axes[3, j].set(ylabel='gen_crpt')\n",
    "            axes[3, j].label_outer()\n",
    "            \n",
    "#             train_disp_gt_pure = np.reshape(train_gt_pure[j], [28,28])\n",
    "#             axes[0, j].imshow(train_disp_gt_pure, cmap='gray')\n",
    "#             axes[0, j].set(ylabel='gt_pure')\n",
    "#             axes[0, j].label_outer()\n",
    "            \n",
    "#             train_disp_gt_noise = np.reshape(train_gt_noise[j], [28,28])    # 28x28\n",
    "#             axes[2, j].imshow(train_disp_gt_noise, cmap='gray')   \n",
    "#             axes[2, j].set(ylabel='gt_noise')\n",
    "#             axes[2, j].label_outer()\n",
    "                    \n",
    "        plt.savefig(outputdir+'/train/epoch %03d' %(epoch+1))    \n",
    "        plt.close(fig)\n",
    "        \n",
    "        # plotting results from testing data\n",
    "        fig, axes = plt.subplots(nrows=4, ncols=n_plot, figsize=(10,2*n_plot))   # displaying 4*n_plot images\n",
    "        plt.setp(axes, xticks=np.arange(0,27,7), yticks=np.arange(0,27,7)) \n",
    "        for k in range(n_plot):\n",
    "            test_disp_gt_crpt = np.reshape(test_gt_crpt[k], [28,28])    # 28x28\n",
    "            axes[0, k].imshow(test_disp_gt_crpt, cmap='gray')   \n",
    "            axes[0, k].set(ylabel='gt_crpt')\n",
    "            axes[0, k].label_outer()\n",
    "            \n",
    "            test_disp_gen_pure = np.reshape(test_gen_pure[k], [28,28])    # 28x28\n",
    "            axes[1, k].imshow(test_disp_gen_pure, cmap='gray')   \n",
    "            axes[1, k].set(ylabel='gen_pure')\n",
    "            axes[1, k].label_outer()           \n",
    "            \n",
    "            test_disp_gen_noise = np.reshape(test_gen_noise[k], [28,28])    # 28x28\n",
    "            axes[2, k].imshow(test_disp_gen_noise, cmap='gray')   \n",
    "            axes[2, k].set(ylabel='gen_noise')\n",
    "            axes[2, k].label_outer()\n",
    "            \n",
    "            test_disp_gen_crpt = np.reshape(test_gen_crpt[k], [28,28])    # 28x28\n",
    "            axes[3, k].imshow(test_disp_gen_crpt, cmap='gray')   \n",
    "            axes[3, k].set(ylabel='gen_crpt')\n",
    "            axes[3, k].label_outer()\n",
    "            \n",
    "#             test_disp_gt_pure = np.reshape(test_gt_pure[k], [28,28])\n",
    "#             axes[0, k].imshow(test_disp_gt_pure, cmap='gray')\n",
    "#             axes[0, k].set(ylabel='gt_pure')\n",
    "#             axes[0, k].label_outer()\n",
    "            \n",
    "#             test_disp_gt_noise = np.reshape(test_gt_noise[k], [28,28])    # 28x28\n",
    "#             axes[2, k].imshow(test_disp_gt_noise, cmap='gray')   \n",
    "#             axes[2, k].set(ylabel='gt_noise')\n",
    "#             axes[2, k].label_outer()\n",
    "                    \n",
    "        plt.savefig(outputdir+'/test/epoch %03d' %(epoch+1))    \n",
    "        plt.close(fig)\n",
    "        \n",
    "        # Save\n",
    "        if (epoch+1) % FLAGS.save_step ==0:\n",
    "            savename = savedir+\"/net-\"+str(epoch+1)+\".ckpt\"\n",
    "            saver.save(sess, savename)\n",
    "            print(\"[%s] SAVED\" % (savename))\n",
    "\n",
    "print(\"Optimization Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
