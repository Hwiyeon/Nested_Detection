{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current version [1.3.1]\n",
      "Packages Loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)  ## just for ignore DeprcationWarning message\n",
    "print(\"Current version [%s]\" %(tf.__version__))\n",
    "print(\"Packages Loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLAGS READY\n"
     ]
    }
   ],
   "source": [
    "# Dataset Configurations\n",
    "tf.app.flags.DEFINE_integer('img_size', 28, \"\"\"Image size of MNIST dataset\"\"\")\n",
    "\n",
    "# Network Configurations\n",
    "tf.app.flags.DEFINE_integer('batch_size', 100, \"\"\"Number of images to process in a batch\"\"\")\n",
    "tf.app.flags.DEFINE_float('l1_ratio', 0.5, \"\"\"Ratio of level1\"\"\")\n",
    "tf.app.flags.DEFINE_float('l2_ratio', 0.5, \"\"\"Ratio of level2\"\"\")\n",
    "\n",
    "# Optimization Configurations\n",
    "tf.app.flags.DEFINE_float('lr', 0.001, \"\"\"Learning rate\"\"\")\n",
    "\n",
    "# Training Configurations\n",
    "tf.app.flags.DEFINE_integer('training_epochs', 500, \"\"\"Number of epochs to run\"\"\")\n",
    "tf.app.flags.DEFINE_integer('display_step', 5, \"\"\"Number of iterations to display training output\"\"\")\n",
    "tf.app.flags.DEFINE_integer('save_step', 5, \"\"\"Number of interations to save checkpoint\"\"\")\n",
    "tf.app.flags.DEFINE_integer('save_max', 5, \"\"\"Number of checkpoints to remain\"\"\")\n",
    "\n",
    "\n",
    "# Save Configurations\n",
    "tf.app.flags.DEFINE_string('nets', './nets', \"\"\"Directory where to write the checkpoints\"\"\")\n",
    "tf.app.flags.DEFINE_string('outputs', './outputs', \"\"\"Directory where to save the output images\"\"\")\n",
    "tf.app.flags.DEFINE_string('tboard', './tensorboard', \"\"\"Directory where to save the tensorboard logs\"\"\")\n",
    "\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "print(\"FLAGS READY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.allow_soft_placement = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../../data/train-images-idx3-ubyte.gz\n",
      "Extracting ../../data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../../data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../../data/t10k-labels-idx1-ubyte.gz\n",
      "MNIST ready\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('../../data/', one_hot=True)\n",
    "train_img = mnist.train.images\n",
    "train_label = mnist.train.labels\n",
    "test_img = mnist.test.images\n",
    "test_label = mnist.test.labels\n",
    "print(\"MNIST ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating random noise mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise_mask(prob=0.5):\n",
    "    mask = np.zeros([FLAGS.img_size, FLAGS.img_size])\n",
    "    rd = np.random.random()\n",
    "    if rd > prob:\n",
    "        # threshold of the size of masks\n",
    "        uthd = FLAGS.img_size    \n",
    "        lthd = 0     \n",
    "        # mask size should be beween 14x14, 5x5\n",
    "        while(uthd>14 or lthd<5):\n",
    "            ver1 = np.random.random_integers(0, FLAGS.img_size-1, size= 2)   # vertex1\n",
    "            ver2 = np.random.random_integers(0, FLAGS.img_size-1, size= 2)    # vertex2\n",
    "            uthd = np.maximum(np.abs(ver1[0]-ver2[0]), np.abs(ver1[1]-ver2[1]))    # upperbound\n",
    "            lthd = np.minimum(np.abs(ver1[0]-ver2[0]), np.abs(ver1[1]-ver2[1]))    # lowerbound\n",
    "        xmin = np.minimum(ver1[0], ver2[0])    # left x value\n",
    "        xmax = np.maximum(ver1[0], ver2[0])    # right x value\n",
    "        ymin = np.minimum(ver1[1], ver2[1])    # top y value\n",
    "        ymax = np.maximum(ver1[1], ver2[1])    # bottom y value\n",
    "        noise = np.random.random((xmax-xmin+1, ymax-ymin+1))    # random sample in [0,1]\n",
    "        mask[xmin:xmax+1, ymin:ymax+1] = noise    # noise mask with location\n",
    "        mask_meta = [xmin, xmax, ymin, ymax, noise, mask]\n",
    "    mask = np.reshape(mask, [-1])\n",
    "    return mask\n",
    "\n",
    "def noise_batch(batch_num):\n",
    "    # make random noise batch\n",
    "    mask_batch = np.zeros([batch_num, FLAGS.img_size*FLAGS.img_size])\n",
    "    for i in range(batch_num):\n",
    "        mask_batch[i,:] = noise_mask()\n",
    "    return mask_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Occlusion generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def occl(target, disturb):\n",
    "    mask = (disturb==0).astype(float)\n",
    "    masked_target = np.multiply(target, mask)\n",
    "    crpt = np.add(masked_target, disturb)\n",
    "    return crpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nested MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _nested_mlp(l1, l2_s, l2, out_channel, name, stddev=0.1, is_init=False, is_last=False):\n",
    "    l1_shape = l1.get_shape()[1]\n",
    "    l2_shape = l2.get_shape()[1]\n",
    "    l2_s_shape = l2_s.get_shape()[1]\n",
    "    \n",
    "    if is_init:\n",
    "        # input is the input image\n",
    "        with tf.device('/CPU:0'):\n",
    "            with tf.variable_scope('level1'):\n",
    "                with tf.variable_scope(name):\n",
    "                    l1_weights = tf.get_variable('weights', \n",
    "                                                 [l1_shape, out_channel*FLAGS.l1_ratio], \n",
    "                                                 tf.float32, \n",
    "                                                 initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "                    l1_biases = tf.get_variable('biases', \n",
    "                                                [out_channel*FLAGS.l1_ratio],\n",
    "                                                tf.float32, \n",
    "                                                initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "            with tf.variable_scope('level2'):\n",
    "                with tf.variable_scope(name):\n",
    "                    l2_s_weights = tf.get_variable('weights_shell', \n",
    "                                                 [l2_s_shape, out_channel*FLAGS.l2_ratio], \n",
    "                                                 tf.float32, \n",
    "                                                 initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "                    l2_s_biases = tf.get_variable('biases_shell', \n",
    "                                                [out_channel*FLAGS.l2_ratio],\n",
    "                                                tf.float32, \n",
    "                                                initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "                    \n",
    "\n",
    "        l1_mlp = tf.nn.sigmoid(tf.add(tf.matmul(l1, l1_weights), l1_biases))\n",
    "        l2_s_mlp = tf.nn.sigmoid(tf.add(tf.matmul(l2_s, l2_s_weights), l2_s_biases))\n",
    "        l2_mlp = tf.concat((l1_mlp, l2_s_mlp), 1)\n",
    "    \n",
    "    elif is_last:\n",
    "        # output is the generated image\n",
    "        with tf.device('/CPU:0'):\n",
    "            with tf.variable_scope('level1'):\n",
    "                with tf.variable_scope(name):\n",
    "                    l1_weights = tf.get_variable('weights', \n",
    "                                                 [l1_shape, out_channel], \n",
    "                                                 tf.float32, \n",
    "                                                 initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "                    l1_biases = tf.get_variable('biases', \n",
    "                                                [out_channel],\n",
    "                                                tf.float32, \n",
    "                                                initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "            with tf.variable_scope('level2'):\n",
    "                with tf.variable_scope(name):\n",
    "                    l2_s_weights = tf.get_variable('weights_shell', \n",
    "                                                 [l2_s_shape, out_channel], \n",
    "                                                 tf.float32, \n",
    "                                                 initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "                    l2_s_biases = tf.get_variable('biases_shell', \n",
    "                                                [out_channel],\n",
    "                                                tf.float32, \n",
    "                                                initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "                    l2_weights = tf.get_variable('weights', \n",
    "                                                 [l2_shape, out_channel], \n",
    "                                                 tf.float32, \n",
    "                                                 initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "                    l2_biases = tf.get_variable('biases', \n",
    "                                                [out_channel],\n",
    "                                                tf.float32, \n",
    "                                                initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "                    \n",
    "\n",
    "        l1_mlp = tf.nn.sigmoid(tf.add(tf.matmul(l1, l1_weights), l1_biases))\n",
    "        l2_s_mlp = tf.nn.sigmoid(tf.add(tf.matmul(l2_s, l2_s_weights), l2_s_biases))\n",
    "        l2_mlp = tf.nn.sigmoid(tf.add(tf.matmul(l2, l2_weights), l2_biases))\n",
    "                                 \n",
    "    else:\n",
    "        with tf.device('/CPU:0'):\n",
    "            with tf.variable_scope('level1'):\n",
    "                with tf.variable_scope(name):\n",
    "                    l1_weights = tf.get_variable('weights', \n",
    "                                                 [l1_shape, out_channel*FLAGS.l1_ratio], \n",
    "                                                 tf.float32, \n",
    "                                                 initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "                    l1_biases = tf.get_variable('biases', \n",
    "                                                [out_channel*FLAGS.l1_ratio],\n",
    "                                                tf.float32, \n",
    "                                                initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "            with tf.variable_scope('level2'):\n",
    "                with tf.variable_scope(name):\n",
    "                    l2_s_weights = tf.get_variable('weights_shell', \n",
    "                                                   [l2_s_shape, out_channel*FLAGS.l2_ratio], \n",
    "                                                   tf.float32, \n",
    "                                                   initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "                    l2_s_biases = tf.get_variable('biases_shell', \n",
    "                                                  [out_channel*FLAGS.l2_ratio],\n",
    "                                                  tf.float32, \n",
    "                                                  initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "                    l2_weights_1 = tf.get_variable('weights_1', \n",
    "                                                   [l2_s_shape, out_channel*FLAGS.l1_ratio], \n",
    "                                                   tf.float32, \n",
    "                                                   initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "                    l2_biases_1 = tf.get_variable('biases_1', \n",
    "                                                  [out_channel*FLAGS.l1_ratio],\n",
    "                                                  tf.float32, \n",
    "                                                  initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "                    l2_weights_2 = tf.get_variable('weights_2', \n",
    "                                                   [l1_shape, out_channel*FLAGS.l2_ratio], \n",
    "                                                   tf.float32, \n",
    "                                                   initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "                    l2_biases_2 = tf.get_variable('biases_2', \n",
    "                                                  [out_channel*FLAGS.l2_ratio],\n",
    "                                                  tf.float32, \n",
    "                                                  initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "\n",
    "        l1_mlp_r = tf.add(tf.matmul(l1, l1_weights), l1_biases)\n",
    "        l1_mlp = tf.nn.sigmoid(l1_mlp_r)\n",
    "        \n",
    "        l2_s_mlp = tf.nn.sigmoid(tf.add(tf.matmul(l2_s, l2_s_weights), l2_s_biases))\n",
    "        \n",
    "        l2_mlp_1_r = tf.add(tf.matmul(l2[:,l1_shape:l2_shape], l2_weights_1), l2_biases_1)\n",
    "        l2_mlp_1 = tf.nn.sigmoid(tf.add(l1_mlp_r, l2_mlp_1_r))\n",
    "        l2_mlp_2_r = tf.add(tf.matmul(l2[:,:l1_shape], l2_weights_2), l2_biases_2)\n",
    "        l2_mlp_3_r = tf.add(tf.matmul(l2[:,l1_shape:l2_shape], l2_s_weights), l2_s_biases)\n",
    "        l2_mlp_2 = tf.nn.sigmoid(tf.add(l2_mlp_2_r, l2_mlp_3_r))\n",
    "        l2_mlp = tf.concat((l2_mlp_1, l2_mlp_2), 1)\n",
    "        \n",
    "        \n",
    "    return l1_mlp, l2_s_mlp, l2_mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graphs Ready\n"
     ]
    }
   ],
   "source": [
    "# Network Topology\n",
    "n_input = FLAGS.img_size*FLAGS.img_size\n",
    "n_enc1 = 1024\n",
    "n_enc2 = 512\n",
    "n_enc3 = 256\n",
    "n_dec1 = 512\n",
    "n_dec2 = 1024\n",
    "n_out = 784\n",
    "\n",
    "# Inputs and Outputs\n",
    "ph_pure = tf.placeholder(\"float\", [None, n_input])    # pure image --- core\n",
    "ph_noise= tf.placeholder(\"float\", [None, n_input])    # noise --- shell1\n",
    "ph_crpt = tf.placeholder(\"float\", [None, n_input])    # corrupted image   --- level2\n",
    "\n",
    "\n",
    "# Model\n",
    "def nested_ae_mlp(_X):\n",
    "    l1_enc1, l2_s_enc1, l2_enc1 = _nested_mlp(_X, _X, _X, n_enc1, name='enc1', is_init=True)\n",
    "    l1_enc2, l2_s_enc2, l2_enc2 = _nested_mlp(l1_enc1, l2_s_enc1, l2_enc1, n_enc2, name='enc2')\n",
    "    l1_enc3, l2_s_enc3, l2_enc3 = _nested_mlp(l1_enc2, l2_s_enc2, l2_enc2, n_enc3, name='enc3')\n",
    "    l1_dec1, l2_s_dec1, l2_dec1 = _nested_mlp(l1_enc3, l2_s_enc3, l2_enc3, n_dec1, name='dec1')\n",
    "    l1_dec2, l2_s_dec2, l2_dec2 = _nested_mlp(l1_dec1, l2_s_dec1, l2_dec1, n_dec2, name='dec2')\n",
    "    l1_out, l2_s_out, l2_out = _nested_mlp(l1_dec2, l2_s_dec2, l2_dec2, n_out, name='out',is_last=True)\n",
    "    return l1_out, l2_s_out, l2_out\n",
    "\n",
    "# Generation\n",
    "core_gen, shell2_gen, full_gen = nested_ae_mlp(ph_crpt)   # [None, n_input]\n",
    "\n",
    "# Loss & Optimizer\n",
    "with tf.name_scope(\"loss\") as scope:\n",
    "    loss = tf.reduce_mean(tf.nn.l2_loss(full_gen-ph_crpt)) + tf.reduce_mean(tf.nn.l2_loss(core_gen-ph_pure))\\\n",
    "            + tf.reduce_mean(tf.nn.l2_loss(shell2_gen-ph_noise))\n",
    "    _train_loss = tf.summary.scalar(\"train_loss\", loss)\n",
    "    _test_loss = tf.summary.scalar(\"test_loss\", loss)\n",
    "\n",
    "optm = tf.train.AdamOptimizer(learning_rate=FLAGS.lr).minimize(loss)\n",
    "\n",
    "\n",
    "print(\"Graphs Ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize Ready\n"
     ]
    }
   ],
   "source": [
    "merged = tf.summary.merge_all()\n",
    "tensorboard_path = FLAGS.tboard\n",
    "if not os.path.exists(tensorboard_path):\n",
    "    os.makedirs(tensorboard_path)\n",
    "writer = tf.summary.FileWriter(tensorboard_path)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "print(\"Initialize Ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saver ready\n"
     ]
    }
   ],
   "source": [
    "outputdir = FLAGS.outputs\n",
    "if not os.path.exists(outputdir+'/train'):\n",
    "    os.makedirs(outputdir+'/train')\n",
    "\n",
    "if not os.path.exists(outputdir+'/test'):\n",
    "    os.makedirs(outputdir+'/test')\n",
    "    \n",
    "savedir = FLAGS.nets\n",
    "if not os.path.exists(savedir):\n",
    "    os.makedirs(savedir)\n",
    "    \n",
    "saver = tf.train.Saver(max_to_keep=FLAGS.save_max)\n",
    "print(\"Saver ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 001/500  Train_loss : 3819.0908203  Test_loss : 3739.4855957\n",
      "Epoch : 002/500  Train_loss : 2593.7258301  Test_loss : 2761.6892090\n",
      "Epoch : 003/500  Train_loss : 2381.1601562  Test_loss : 2310.1501465\n",
      "Epoch : 004/500  Train_loss : 2201.3657227  Test_loss : 2121.6425781\n",
      "Epoch : 005/500  Train_loss : 2012.5334473  Test_loss : 1710.2414551\n",
      "[./nets/net-5.ckpt] SAVED\n",
      "Epoch : 006/500  Train_loss : 1802.4169922  Test_loss : 1955.2259521\n",
      "Epoch : 007/500  Train_loss : 1506.0917969  Test_loss : 1757.1834717\n",
      "Epoch : 008/500  Train_loss : 1511.6772461  Test_loss : 1666.9383545\n",
      "Epoch : 009/500  Train_loss : 1823.0939941  Test_loss : 1601.2878418\n",
      "Epoch : 010/500  Train_loss : 1616.0495605  Test_loss : 1522.4211426\n",
      "[./nets/net-10.ckpt] SAVED\n",
      "Epoch : 011/500  Train_loss : 1533.7916260  Test_loss : 1448.0152588\n",
      "Epoch : 012/500  Train_loss : 1471.2607422  Test_loss : 1503.5058594\n",
      "Epoch : 013/500  Train_loss : 1527.4277344  Test_loss : 1385.6750488\n",
      "Epoch : 014/500  Train_loss : 1233.4643555  Test_loss : 1564.6732178\n",
      "Epoch : 015/500  Train_loss : 1433.2265625  Test_loss : 1284.1235352\n",
      "[./nets/net-15.ckpt] SAVED\n",
      "Epoch : 016/500  Train_loss : 1365.5830078  Test_loss : 1180.3768311\n",
      "Epoch : 017/500  Train_loss : 1351.6506348  Test_loss : 1502.3201904\n",
      "Epoch : 018/500  Train_loss : 1262.5395508  Test_loss : 1378.9891357\n",
      "Epoch : 019/500  Train_loss : 1363.3295898  Test_loss : 1200.0681152\n",
      "Epoch : 020/500  Train_loss : 1272.0245361  Test_loss : 1119.9848633\n",
      "[./nets/net-20.ckpt] SAVED\n",
      "Epoch : 021/500  Train_loss : 1245.4877930  Test_loss : 1119.7966309\n",
      "Epoch : 022/500  Train_loss : 1102.6285400  Test_loss : 1055.1208496\n",
      "Epoch : 023/500  Train_loss : 1325.6625977  Test_loss : 1258.5279541\n",
      "Epoch : 024/500  Train_loss : 1095.5745850  Test_loss : 1152.0560303\n",
      "Epoch : 025/500  Train_loss : 1148.4208984  Test_loss : 1131.3022461\n",
      "[./nets/net-25.ckpt] SAVED\n",
      "Epoch : 026/500  Train_loss : 991.4836426  Test_loss : 1209.9313965\n",
      "Epoch : 027/500  Train_loss : 1312.9003906  Test_loss : 1031.1129150\n",
      "Epoch : 028/500  Train_loss : 1177.9516602  Test_loss : 1177.1440430\n",
      "Epoch : 029/500  Train_loss : 958.6187134  Test_loss : 1056.4848633\n",
      "Epoch : 030/500  Train_loss : 1054.3286133  Test_loss : 1206.4501953\n",
      "[./nets/net-30.ckpt] SAVED\n",
      "Epoch : 031/500  Train_loss : 1188.7011719  Test_loss : 1069.4390869\n",
      "Epoch : 032/500  Train_loss : 1101.6289062  Test_loss : 1221.7646484\n",
      "Epoch : 033/500  Train_loss : 1031.0805664  Test_loss : 983.9115601\n",
      "Epoch : 034/500  Train_loss : 1004.1835938  Test_loss : 1064.0964355\n",
      "Epoch : 035/500  Train_loss : 1034.4462891  Test_loss : 1058.9062500\n",
      "[./nets/net-35.ckpt] SAVED\n",
      "Epoch : 036/500  Train_loss : 1006.5411377  Test_loss : 1018.4851685\n",
      "Epoch : 037/500  Train_loss : 980.1267090  Test_loss : 1119.0435791\n",
      "Epoch : 038/500  Train_loss : 1079.9265137  Test_loss : 1069.7279053\n",
      "Epoch : 039/500  Train_loss : 998.2796021  Test_loss : 1005.4667358\n",
      "Epoch : 040/500  Train_loss : 970.7797852  Test_loss : 955.7718506\n",
      "[./nets/net-40.ckpt] SAVED\n",
      "Epoch : 041/500  Train_loss : 948.9445801  Test_loss : 1145.6644287\n",
      "Epoch : 042/500  Train_loss : 926.4750977  Test_loss : 910.0471191\n",
      "Epoch : 043/500  Train_loss : 968.3536377  Test_loss : 1026.3242188\n",
      "Epoch : 044/500  Train_loss : 1172.2326660  Test_loss : 1062.6723633\n",
      "Epoch : 045/500  Train_loss : 1042.5751953  Test_loss : 1009.4591064\n",
      "[./nets/net-45.ckpt] SAVED\n",
      "Epoch : 046/500  Train_loss : 892.5617676  Test_loss : 965.4038086\n",
      "Epoch : 047/500  Train_loss : 967.9011230  Test_loss : 989.1050415\n",
      "Epoch : 048/500  Train_loss : 978.9357910  Test_loss : 1018.5728760\n",
      "Epoch : 049/500  Train_loss : 933.0524902  Test_loss : 971.2154541\n",
      "Epoch : 050/500  Train_loss : 881.4509888  Test_loss : 1012.3654785\n",
      "[./nets/net-50.ckpt] SAVED\n",
      "Epoch : 051/500  Train_loss : 1040.7530518  Test_loss : 829.1451416\n",
      "Epoch : 052/500  Train_loss : 845.1781616  Test_loss : 951.5805054\n",
      "Epoch : 053/500  Train_loss : 851.4357910  Test_loss : 980.9163208\n",
      "Epoch : 054/500  Train_loss : 828.5082397  Test_loss : 888.6617432\n",
      "Epoch : 055/500  Train_loss : 998.2980957  Test_loss : 893.0911255\n",
      "[./nets/net-55.ckpt] SAVED\n",
      "Epoch : 056/500  Train_loss : 1035.7457275  Test_loss : 955.6752319\n",
      "Epoch : 057/500  Train_loss : 955.4135132  Test_loss : 890.4460449\n",
      "Epoch : 058/500  Train_loss : 913.2714233  Test_loss : 1102.7036133\n",
      "Epoch : 059/500  Train_loss : 977.5074463  Test_loss : 844.4508057\n",
      "Epoch : 060/500  Train_loss : 1013.4075928  Test_loss : 929.1738281\n",
      "[./nets/net-60.ckpt] SAVED\n",
      "Epoch : 061/500  Train_loss : 1029.9525146  Test_loss : 995.2655640\n",
      "Epoch : 062/500  Train_loss : 926.2583008  Test_loss : 871.6729126\n",
      "Epoch : 063/500  Train_loss : 864.5153198  Test_loss : 869.7197876\n",
      "Epoch : 064/500  Train_loss : 1014.9758301  Test_loss : 929.0630493\n",
      "Epoch : 065/500  Train_loss : 1015.7049561  Test_loss : 814.7872925\n",
      "[./nets/net-65.ckpt] SAVED\n",
      "Epoch : 066/500  Train_loss : 882.2136841  Test_loss : 823.7207031\n",
      "Epoch : 067/500  Train_loss : 1077.9160156  Test_loss : 912.6068726\n",
      "Epoch : 068/500  Train_loss : 828.4841309  Test_loss : 812.5887451\n",
      "Epoch : 069/500  Train_loss : 799.0412598  Test_loss : 848.9633789\n",
      "Epoch : 070/500  Train_loss : 802.0031738  Test_loss : 885.2808838\n",
      "[./nets/net-70.ckpt] SAVED\n",
      "Epoch : 071/500  Train_loss : 841.4289551  Test_loss : 936.7940674\n",
      "Epoch : 072/500  Train_loss : 797.1804810  Test_loss : 936.4541626\n",
      "Epoch : 073/500  Train_loss : 873.0155029  Test_loss : 884.1752319\n",
      "Epoch : 074/500  Train_loss : 1014.4923096  Test_loss : 947.4893188\n",
      "Epoch : 075/500  Train_loss : 988.9531860  Test_loss : 813.3203125\n",
      "[./nets/net-75.ckpt] SAVED\n",
      "Epoch : 076/500  Train_loss : 803.0492554  Test_loss : 870.1185303\n",
      "Epoch : 077/500  Train_loss : 849.0263672  Test_loss : 943.3713379\n",
      "Epoch : 078/500  Train_loss : 857.4353027  Test_loss : 918.9857788\n",
      "Epoch : 079/500  Train_loss : 966.5009766  Test_loss : 928.0975342\n",
      "Epoch : 080/500  Train_loss : 846.5136719  Test_loss : 776.5787354\n",
      "[./nets/net-80.ckpt] SAVED\n",
      "Epoch : 081/500  Train_loss : 728.0866089  Test_loss : 929.4571533\n",
      "Epoch : 082/500  Train_loss : 805.0980225  Test_loss : 1008.6398926\n",
      "Epoch : 083/500  Train_loss : 845.0975342  Test_loss : 812.8447876\n",
      "Epoch : 084/500  Train_loss : 770.6550293  Test_loss : 904.6549072\n",
      "Epoch : 085/500  Train_loss : 830.7545166  Test_loss : 887.4320068\n",
      "[./nets/net-85.ckpt] SAVED\n",
      "Epoch : 086/500  Train_loss : 826.5638428  Test_loss : 900.0150146\n",
      "Epoch : 087/500  Train_loss : 800.5264282  Test_loss : 803.0636597\n",
      "Epoch : 088/500  Train_loss : 827.7564697  Test_loss : 837.6618652\n",
      "Epoch : 089/500  Train_loss : 897.0017090  Test_loss : 813.7075806\n",
      "Epoch : 090/500  Train_loss : 789.0468750  Test_loss : 841.1549072\n",
      "[./nets/net-90.ckpt] SAVED\n",
      "Epoch : 091/500  Train_loss : 693.6520386  Test_loss : 828.0631104\n",
      "Epoch : 092/500  Train_loss : 817.5277710  Test_loss : 852.4572754\n",
      "Epoch : 093/500  Train_loss : 902.0786743  Test_loss : 842.3165283\n",
      "Epoch : 094/500  Train_loss : 746.6170654  Test_loss : 780.9057007\n",
      "Epoch : 095/500  Train_loss : 833.9907227  Test_loss : 839.8516846\n",
      "[./nets/net-95.ckpt] SAVED\n",
      "Epoch : 096/500  Train_loss : 719.9921265  Test_loss : 895.5223999\n",
      "Epoch : 097/500  Train_loss : 761.5989990  Test_loss : 903.5543213\n",
      "Epoch : 098/500  Train_loss : 740.7436523  Test_loss : 775.8975830\n",
      "Epoch : 099/500  Train_loss : 740.5159302  Test_loss : 674.9673462\n",
      "Epoch : 100/500  Train_loss : 759.5452881  Test_loss : 849.5266724\n",
      "[./nets/net-100.ckpt] SAVED\n",
      "Epoch : 101/500  Train_loss : 823.6793213  Test_loss : 889.5556030\n",
      "Epoch : 102/500  Train_loss : 894.2313232  Test_loss : 905.3085938\n",
      "Epoch : 103/500  Train_loss : 807.2655029  Test_loss : 888.6940308\n",
      "Epoch : 104/500  Train_loss : 811.4066772  Test_loss : 810.2876587\n",
      "Epoch : 105/500  Train_loss : 749.3331299  Test_loss : 832.6203613\n",
      "[./nets/net-105.ckpt] SAVED\n",
      "Epoch : 106/500  Train_loss : 708.3880005  Test_loss : 768.0035400\n",
      "Epoch : 107/500  Train_loss : 787.4655762  Test_loss : 640.2360840\n",
      "Epoch : 108/500  Train_loss : 719.4252930  Test_loss : 852.3473511\n",
      "Epoch : 109/500  Train_loss : 725.2030029  Test_loss : 792.5744629\n",
      "Epoch : 110/500  Train_loss : 692.0493164  Test_loss : 783.9265137\n",
      "[./nets/net-110.ckpt] SAVED\n",
      "Epoch : 111/500  Train_loss : 709.1214600  Test_loss : 942.7191162\n",
      "Epoch : 112/500  Train_loss : 733.1311035  Test_loss : 856.0545654\n",
      "Epoch : 113/500  Train_loss : 684.0522461  Test_loss : 800.1231689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 114/500  Train_loss : 860.2945557  Test_loss : 768.5015869\n",
      "Epoch : 115/500  Train_loss : 854.3847046  Test_loss : 576.4733887\n",
      "[./nets/net-115.ckpt] SAVED\n",
      "Epoch : 116/500  Train_loss : 845.9425049  Test_loss : 848.1143799\n",
      "Epoch : 117/500  Train_loss : 757.1018066  Test_loss : 789.9989014\n",
      "Epoch : 118/500  Train_loss : 852.3891602  Test_loss : 762.8665771\n",
      "Epoch : 119/500  Train_loss : 782.7112427  Test_loss : 807.6298218\n",
      "Epoch : 120/500  Train_loss : 753.2142334  Test_loss : 659.0598755\n",
      "[./nets/net-120.ckpt] SAVED\n",
      "Epoch : 121/500  Train_loss : 661.3334961  Test_loss : 783.3700562\n",
      "Epoch : 122/500  Train_loss : 694.2367554  Test_loss : 781.1566162\n",
      "Epoch : 123/500  Train_loss : 783.2733765  Test_loss : 662.8984985\n",
      "Epoch : 124/500  Train_loss : 728.9523315  Test_loss : 754.1362915\n",
      "Epoch : 125/500  Train_loss : 663.4403076  Test_loss : 817.8554688\n",
      "[./nets/net-125.ckpt] SAVED\n",
      "Epoch : 126/500  Train_loss : 710.3511353  Test_loss : 767.1599731\n",
      "Epoch : 127/500  Train_loss : 690.8936768  Test_loss : 727.4000854\n",
      "Epoch : 128/500  Train_loss : 780.4375610  Test_loss : 689.8093872\n",
      "Epoch : 129/500  Train_loss : 764.1743164  Test_loss : 749.3465576\n",
      "Epoch : 130/500  Train_loss : 824.4208984  Test_loss : 693.3487549\n",
      "[./nets/net-130.ckpt] SAVED\n",
      "Epoch : 131/500  Train_loss : 748.2982788  Test_loss : 832.7271729\n",
      "Epoch : 132/500  Train_loss : 732.5070801  Test_loss : 698.3717041\n",
      "Epoch : 133/500  Train_loss : 717.1525269  Test_loss : 746.6203613\n",
      "Epoch : 134/500  Train_loss : 640.9138184  Test_loss : 667.2921143\n",
      "Epoch : 135/500  Train_loss : 843.6865234  Test_loss : 759.5401611\n",
      "[./nets/net-135.ckpt] SAVED\n",
      "Epoch : 136/500  Train_loss : 716.4820557  Test_loss : 690.3591919\n",
      "Epoch : 137/500  Train_loss : 696.9007568  Test_loss : 656.5562134\n",
      "Epoch : 138/500  Train_loss : 647.8637085  Test_loss : 808.0552979\n",
      "Epoch : 139/500  Train_loss : 693.3884277  Test_loss : 692.4548950\n",
      "Epoch : 140/500  Train_loss : 638.1994629  Test_loss : 581.2172241\n",
      "[./nets/net-140.ckpt] SAVED\n",
      "Epoch : 141/500  Train_loss : 727.4508057  Test_loss : 794.6520996\n",
      "Epoch : 142/500  Train_loss : 598.5383911  Test_loss : 603.5504761\n",
      "Epoch : 143/500  Train_loss : 793.1034546  Test_loss : 802.8966675\n",
      "Epoch : 144/500  Train_loss : 701.3123779  Test_loss : 696.1923218\n",
      "Epoch : 145/500  Train_loss : 814.6550293  Test_loss : 683.2508545\n",
      "[./nets/net-145.ckpt] SAVED\n",
      "Epoch : 146/500  Train_loss : 682.7614746  Test_loss : 666.6358643\n",
      "Epoch : 147/500  Train_loss : 720.4708862  Test_loss : 702.3802490\n",
      "Epoch : 148/500  Train_loss : 776.4254150  Test_loss : 685.6549072\n",
      "Epoch : 149/500  Train_loss : 773.1300659  Test_loss : 666.6204834\n",
      "Epoch : 150/500  Train_loss : 757.1940308  Test_loss : 751.7543945\n",
      "[./nets/net-150.ckpt] SAVED\n",
      "Epoch : 151/500  Train_loss : 651.7188110  Test_loss : 723.0275269\n",
      "Epoch : 152/500  Train_loss : 725.4647217  Test_loss : 738.5980835\n",
      "Epoch : 153/500  Train_loss : 664.9743042  Test_loss : 656.4071655\n",
      "Epoch : 154/500  Train_loss : 679.6340332  Test_loss : 696.3034668\n",
      "Epoch : 155/500  Train_loss : 675.8688965  Test_loss : 820.6264648\n",
      "[./nets/net-155.ckpt] SAVED\n",
      "Epoch : 156/500  Train_loss : 733.8419800  Test_loss : 746.2659912\n",
      "Epoch : 157/500  Train_loss : 713.5829468  Test_loss : 655.4260864\n",
      "Epoch : 158/500  Train_loss : 658.6088867  Test_loss : 747.5565796\n",
      "Epoch : 159/500  Train_loss : 670.2831421  Test_loss : 705.0712280\n",
      "Epoch : 160/500  Train_loss : 764.2261963  Test_loss : 619.3424072\n",
      "[./nets/net-160.ckpt] SAVED\n",
      "Epoch : 161/500  Train_loss : 555.2157593  Test_loss : 663.7203979\n",
      "Epoch : 162/500  Train_loss : 652.4696045  Test_loss : 657.3425293\n",
      "Epoch : 163/500  Train_loss : 679.5738525  Test_loss : 701.5695801\n",
      "Epoch : 164/500  Train_loss : 593.3340454  Test_loss : 668.2460938\n",
      "Epoch : 165/500  Train_loss : 617.8182373  Test_loss : 701.9865723\n",
      "[./nets/net-165.ckpt] SAVED\n",
      "Epoch : 166/500  Train_loss : 633.2670898  Test_loss : 658.5029907\n",
      "Epoch : 167/500  Train_loss : 687.0012207  Test_loss : 729.1777954\n",
      "Epoch : 168/500  Train_loss : 777.8982544  Test_loss : 719.2039795\n",
      "Epoch : 169/500  Train_loss : 639.7960205  Test_loss : 693.9765625\n",
      "Epoch : 170/500  Train_loss : 608.2207031  Test_loss : 649.5938721\n",
      "[./nets/net-170.ckpt] SAVED\n",
      "Epoch : 171/500  Train_loss : 699.2315674  Test_loss : 695.6628418\n",
      "Epoch : 172/500  Train_loss : 688.4110107  Test_loss : 680.2249756\n",
      "Epoch : 173/500  Train_loss : 599.5491943  Test_loss : 647.0587769\n",
      "Epoch : 174/500  Train_loss : 692.8181152  Test_loss : 700.2449951\n",
      "Epoch : 175/500  Train_loss : 668.4260864  Test_loss : 612.3430786\n",
      "[./nets/net-175.ckpt] SAVED\n",
      "Epoch : 176/500  Train_loss : 575.9237061  Test_loss : 532.2304688\n",
      "Epoch : 177/500  Train_loss : 561.3981323  Test_loss : 576.9022217\n",
      "Epoch : 178/500  Train_loss : 751.1175537  Test_loss : 699.0014648\n",
      "Epoch : 179/500  Train_loss : 598.3728027  Test_loss : 596.1442261\n",
      "Epoch : 180/500  Train_loss : 572.0924072  Test_loss : 657.9921875\n",
      "[./nets/net-180.ckpt] SAVED\n",
      "Epoch : 181/500  Train_loss : 650.9382324  Test_loss : 596.8749390\n",
      "Epoch : 182/500  Train_loss : 599.1004639  Test_loss : 635.5881348\n",
      "Epoch : 183/500  Train_loss : 610.4556274  Test_loss : 606.8344116\n",
      "Epoch : 184/500  Train_loss : 700.4871826  Test_loss : 743.7973022\n",
      "Epoch : 185/500  Train_loss : 713.6596680  Test_loss : 714.7764893\n",
      "[./nets/net-185.ckpt] SAVED\n",
      "Epoch : 186/500  Train_loss : 584.5228882  Test_loss : 582.0783691\n",
      "Epoch : 187/500  Train_loss : 632.4243164  Test_loss : 779.9274292\n",
      "Epoch : 188/500  Train_loss : 602.5791016  Test_loss : 743.6784668\n",
      "Epoch : 189/500  Train_loss : 574.1497192  Test_loss : 625.6062012\n",
      "Epoch : 190/500  Train_loss : 604.6369019  Test_loss : 607.0225830\n",
      "[./nets/net-190.ckpt] SAVED\n",
      "Epoch : 191/500  Train_loss : 672.9689331  Test_loss : 693.4499512\n",
      "Epoch : 192/500  Train_loss : 712.1371460  Test_loss : 677.2111816\n",
      "Epoch : 193/500  Train_loss : 703.8740234  Test_loss : 729.6571045\n",
      "Epoch : 194/500  Train_loss : 632.9017334  Test_loss : 590.7612915\n",
      "Epoch : 195/500  Train_loss : 692.9404297  Test_loss : 583.7898560\n",
      "[./nets/net-195.ckpt] SAVED\n",
      "Epoch : 196/500  Train_loss : 651.5013428  Test_loss : 764.8197021\n",
      "Epoch : 197/500  Train_loss : 627.2345581  Test_loss : 737.4136963\n",
      "Epoch : 198/500  Train_loss : 673.7559204  Test_loss : 598.5454712\n",
      "Epoch : 199/500  Train_loss : 700.4040527  Test_loss : 602.0788574\n",
      "Epoch : 200/500  Train_loss : 519.8717041  Test_loss : 650.9386597\n",
      "[./nets/net-200.ckpt] SAVED\n",
      "Epoch : 201/500  Train_loss : 672.4384766  Test_loss : 533.7594604\n",
      "Epoch : 202/500  Train_loss : 620.2704468  Test_loss : 769.1512451\n",
      "Epoch : 203/500  Train_loss : 553.1373291  Test_loss : 565.6597290\n",
      "Epoch : 204/500  Train_loss : 519.4180908  Test_loss : 679.3842773\n",
      "Epoch : 205/500  Train_loss : 479.2331543  Test_loss : 640.3079834\n",
      "[./nets/net-205.ckpt] SAVED\n",
      "Epoch : 206/500  Train_loss : 631.4103394  Test_loss : 653.6219482\n",
      "Epoch : 207/500  Train_loss : 661.8679199  Test_loss : 628.8001709\n",
      "Epoch : 208/500  Train_loss : 606.6196899  Test_loss : 690.9465332\n",
      "Epoch : 209/500  Train_loss : 566.2467041  Test_loss : 640.3299561\n",
      "Epoch : 210/500  Train_loss : 598.5894775  Test_loss : 627.7149048\n",
      "[./nets/net-210.ckpt] SAVED\n",
      "Epoch : 211/500  Train_loss : 674.3041992  Test_loss : 529.6813354\n",
      "Epoch : 212/500  Train_loss : 537.6303711  Test_loss : 650.7405396\n",
      "Epoch : 213/500  Train_loss : 668.5286865  Test_loss : 631.7337646\n",
      "Epoch : 214/500  Train_loss : 622.5496826  Test_loss : 583.8592529\n",
      "Epoch : 215/500  Train_loss : 587.8972778  Test_loss : 724.2945557\n",
      "[./nets/net-215.ckpt] SAVED\n",
      "Epoch : 216/500  Train_loss : 598.4572754  Test_loss : 656.3593750\n",
      "Epoch : 217/500  Train_loss : 575.2155762  Test_loss : 557.1696167\n",
      "Epoch : 218/500  Train_loss : 512.9661255  Test_loss : 573.8666992\n",
      "Epoch : 219/500  Train_loss : 675.3866577  Test_loss : 579.6157837\n",
      "Epoch : 220/500  Train_loss : 584.0280762  Test_loss : 499.2124329\n",
      "[./nets/net-220.ckpt] SAVED\n",
      "Epoch : 221/500  Train_loss : 560.9106445  Test_loss : 606.2020264\n",
      "Epoch : 222/500  Train_loss : 580.9946899  Test_loss : 680.0852051\n",
      "Epoch : 223/500  Train_loss : 615.7019043  Test_loss : 608.6000366\n",
      "Epoch : 224/500  Train_loss : 561.0634766  Test_loss : 595.0963745\n",
      "Epoch : 225/500  Train_loss : 583.7282104  Test_loss : 578.2743530\n",
      "[./nets/net-225.ckpt] SAVED\n",
      "Epoch : 226/500  Train_loss : 510.0670471  Test_loss : 621.7675781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 227/500  Train_loss : 621.8038940  Test_loss : 553.2279663\n",
      "Epoch : 228/500  Train_loss : 602.9008789  Test_loss : 680.2860107\n",
      "Epoch : 229/500  Train_loss : 517.4963379  Test_loss : 698.3967285\n",
      "Epoch : 230/500  Train_loss : 611.5595093  Test_loss : 650.2533569\n",
      "[./nets/net-230.ckpt] SAVED\n",
      "Epoch : 231/500  Train_loss : 645.1216431  Test_loss : 654.8405151\n",
      "Epoch : 232/500  Train_loss : 697.0718994  Test_loss : 556.2886353\n",
      "Epoch : 233/500  Train_loss : 705.8296509  Test_loss : 560.7230225\n",
      "Epoch : 234/500  Train_loss : 616.0075684  Test_loss : 574.2662354\n",
      "Epoch : 235/500  Train_loss : 589.5948486  Test_loss : 623.5228882\n",
      "[./nets/net-235.ckpt] SAVED\n",
      "Epoch : 236/500  Train_loss : 601.5789795  Test_loss : 593.6699219\n",
      "Epoch : 237/500  Train_loss : 691.1862793  Test_loss : 571.5145874\n",
      "Epoch : 238/500  Train_loss : 472.5918884  Test_loss : 571.9973755\n",
      "Epoch : 239/500  Train_loss : 575.1710205  Test_loss : 582.2525024\n",
      "Epoch : 240/500  Train_loss : 562.9526367  Test_loss : 577.1205444\n",
      "[./nets/net-240.ckpt] SAVED\n",
      "Epoch : 241/500  Train_loss : 616.0845337  Test_loss : 530.4403687\n",
      "Epoch : 242/500  Train_loss : 464.1621399  Test_loss : 596.9445801\n",
      "Epoch : 243/500  Train_loss : 527.7210693  Test_loss : 610.3566284\n",
      "Epoch : 244/500  Train_loss : 577.3823853  Test_loss : 586.0071411\n",
      "Epoch : 245/500  Train_loss : 569.9918213  Test_loss : 587.7833252\n",
      "[./nets/net-245.ckpt] SAVED\n",
      "Epoch : 246/500  Train_loss : 575.0523682  Test_loss : 558.3129272\n",
      "Epoch : 247/500  Train_loss : 621.0191650  Test_loss : 502.6347351\n",
      "Epoch : 248/500  Train_loss : 597.6022949  Test_loss : 686.9410400\n",
      "Epoch : 249/500  Train_loss : 503.6415710  Test_loss : 560.2410889\n",
      "Epoch : 250/500  Train_loss : 633.7613525  Test_loss : 543.4046631\n",
      "[./nets/net-250.ckpt] SAVED\n",
      "Epoch : 251/500  Train_loss : 635.4180298  Test_loss : 600.0269165\n",
      "Epoch : 252/500  Train_loss : 513.2934570  Test_loss : 632.2086182\n",
      "Epoch : 253/500  Train_loss : 501.0806885  Test_loss : 643.6343384\n",
      "Epoch : 254/500  Train_loss : 549.3513184  Test_loss : 566.1072388\n",
      "Epoch : 255/500  Train_loss : 560.0979004  Test_loss : 629.2569580\n",
      "[./nets/net-255.ckpt] SAVED\n",
      "Epoch : 256/500  Train_loss : 590.5308838  Test_loss : 618.4128418\n",
      "Epoch : 257/500  Train_loss : 533.3625488  Test_loss : 619.1339111\n",
      "Epoch : 258/500  Train_loss : 537.0167847  Test_loss : 572.8717041\n",
      "Epoch : 259/500  Train_loss : 547.4389038  Test_loss : 615.8885498\n",
      "Epoch : 260/500  Train_loss : 613.3134155  Test_loss : 661.6704712\n",
      "[./nets/net-260.ckpt] SAVED\n",
      "Epoch : 261/500  Train_loss : 667.8711548  Test_loss : 578.6667480\n",
      "Epoch : 262/500  Train_loss : 627.8644409  Test_loss : 613.5794678\n",
      "Epoch : 263/500  Train_loss : 540.3264771  Test_loss : 599.9633179\n",
      "Epoch : 264/500  Train_loss : 543.5496826  Test_loss : 526.1799316\n",
      "Epoch : 265/500  Train_loss : 616.0903931  Test_loss : 515.6800537\n",
      "[./nets/net-265.ckpt] SAVED\n",
      "Epoch : 266/500  Train_loss : 553.4066162  Test_loss : 498.9486694\n",
      "Epoch : 267/500  Train_loss : 500.8392639  Test_loss : 582.7293701\n",
      "Epoch : 268/500  Train_loss : 511.9311829  Test_loss : 566.9083252\n",
      "Epoch : 269/500  Train_loss : 514.5886230  Test_loss : 595.8565063\n",
      "Epoch : 270/500  Train_loss : 501.5316162  Test_loss : 636.3372192\n",
      "[./nets/net-270.ckpt] SAVED\n",
      "Epoch : 271/500  Train_loss : 587.1270142  Test_loss : 602.6204834\n",
      "Epoch : 272/500  Train_loss : 445.4791870  Test_loss : 551.1798096\n",
      "Epoch : 273/500  Train_loss : 590.2310791  Test_loss : 510.1246338\n",
      "Epoch : 274/500  Train_loss : 583.7573242  Test_loss : 476.4116821\n",
      "Epoch : 275/500  Train_loss : 505.1331787  Test_loss : 584.2188721\n",
      "[./nets/net-275.ckpt] SAVED\n",
      "Epoch : 276/500  Train_loss : 463.1563110  Test_loss : 598.2551270\n",
      "Epoch : 277/500  Train_loss : 542.7431030  Test_loss : 548.5132446\n",
      "Epoch : 278/500  Train_loss : 504.9044495  Test_loss : 530.0842896\n",
      "Epoch : 279/500  Train_loss : 589.9477539  Test_loss : 473.3603516\n",
      "Epoch : 280/500  Train_loss : 612.3818970  Test_loss : 555.2690430\n",
      "[./nets/net-280.ckpt] SAVED\n",
      "Epoch : 281/500  Train_loss : 525.0170898  Test_loss : 553.6904907\n",
      "Epoch : 282/500  Train_loss : 594.7549438  Test_loss : 597.6268311\n",
      "Epoch : 283/500  Train_loss : 543.0734863  Test_loss : 582.5662231\n",
      "Epoch : 284/500  Train_loss : 507.6033325  Test_loss : 518.7245483\n",
      "Epoch : 285/500  Train_loss : 572.0458984  Test_loss : 578.3868408\n",
      "[./nets/net-285.ckpt] SAVED\n",
      "Epoch : 286/500  Train_loss : 519.0267334  Test_loss : 525.2291870\n",
      "Epoch : 287/500  Train_loss : 548.7991943  Test_loss : 628.6755371\n",
      "Epoch : 288/500  Train_loss : 522.5057373  Test_loss : 585.5730591\n",
      "Epoch : 289/500  Train_loss : 515.1563721  Test_loss : 516.8608398\n",
      "Epoch : 290/500  Train_loss : 553.7358398  Test_loss : 448.6462708\n",
      "[./nets/net-290.ckpt] SAVED\n",
      "Epoch : 291/500  Train_loss : 533.8071289  Test_loss : 594.3437500\n",
      "Epoch : 292/500  Train_loss : 518.7360840  Test_loss : 633.3334351\n",
      "Epoch : 293/500  Train_loss : 511.1167603  Test_loss : 649.2686768\n",
      "Epoch : 294/500  Train_loss : 604.0080566  Test_loss : 570.1263428\n",
      "Epoch : 295/500  Train_loss : 533.2669678  Test_loss : 489.2742920\n",
      "[./nets/net-295.ckpt] SAVED\n",
      "Epoch : 296/500  Train_loss : 469.2864380  Test_loss : 496.0275879\n",
      "Epoch : 297/500  Train_loss : 612.0067139  Test_loss : 584.3933105\n",
      "Epoch : 298/500  Train_loss : 541.9755859  Test_loss : 533.1636353\n",
      "Epoch : 299/500  Train_loss : 623.8317871  Test_loss : 648.1306152\n",
      "Epoch : 300/500  Train_loss : 548.2069702  Test_loss : 528.5615234\n",
      "[./nets/net-300.ckpt] SAVED\n",
      "Epoch : 301/500  Train_loss : 477.5815125  Test_loss : 566.9067383\n",
      "Epoch : 302/500  Train_loss : 568.7647705  Test_loss : 601.3200073\n",
      "Epoch : 303/500  Train_loss : 485.0871582  Test_loss : 500.6691895\n",
      "Epoch : 304/500  Train_loss : 547.6093750  Test_loss : 488.4883423\n",
      "Epoch : 305/500  Train_loss : 459.9868164  Test_loss : 577.6360474\n",
      "[./nets/net-305.ckpt] SAVED\n",
      "Epoch : 306/500  Train_loss : 519.6237793  Test_loss : 639.7326660\n",
      "Epoch : 307/500  Train_loss : 492.4207153  Test_loss : 532.8018799\n",
      "Epoch : 308/500  Train_loss : 512.0642700  Test_loss : 519.9192505\n",
      "Epoch : 309/500  Train_loss : 497.6968079  Test_loss : 505.5246582\n",
      "Epoch : 310/500  Train_loss : 502.5506592  Test_loss : 615.2145386\n",
      "[./nets/net-310.ckpt] SAVED\n",
      "Epoch : 311/500  Train_loss : 460.4846497  Test_loss : 571.5964355\n",
      "Epoch : 312/500  Train_loss : 577.1789551  Test_loss : 567.7054443\n",
      "Epoch : 313/500  Train_loss : 530.8601074  Test_loss : 572.3961792\n",
      "Epoch : 314/500  Train_loss : 580.6078491  Test_loss : 454.8242798\n",
      "Epoch : 315/500  Train_loss : 557.2137451  Test_loss : 466.2498169\n",
      "[./nets/net-315.ckpt] SAVED\n",
      "Epoch : 316/500  Train_loss : 574.4091187  Test_loss : 506.3695068\n",
      "Epoch : 317/500  Train_loss : 541.7765503  Test_loss : 515.4907227\n",
      "Epoch : 318/500  Train_loss : 569.9936523  Test_loss : 529.0893555\n",
      "Epoch : 319/500  Train_loss : 653.5173950  Test_loss : 479.4022522\n",
      "Epoch : 320/500  Train_loss : 488.0649719  Test_loss : 511.2888184\n",
      "[./nets/net-320.ckpt] SAVED\n",
      "Epoch : 321/500  Train_loss : 544.9136963  Test_loss : 533.3471680\n",
      "Epoch : 322/500  Train_loss : 511.4244385  Test_loss : 607.1469727\n",
      "Epoch : 323/500  Train_loss : 581.4701538  Test_loss : 525.1967773\n",
      "Epoch : 324/500  Train_loss : 517.2818604  Test_loss : 473.1120911\n",
      "Epoch : 325/500  Train_loss : 525.8717041  Test_loss : 571.0914917\n",
      "[./nets/net-325.ckpt] SAVED\n",
      "Epoch : 326/500  Train_loss : 544.0798950  Test_loss : 508.1083984\n",
      "Epoch : 327/500  Train_loss : 457.9987183  Test_loss : 590.2239990\n",
      "Epoch : 328/500  Train_loss : 592.2042847  Test_loss : 485.5036621\n",
      "Epoch : 329/500  Train_loss : 569.9440918  Test_loss : 592.1419678\n",
      "Epoch : 330/500  Train_loss : 575.5253906  Test_loss : 491.4737549\n",
      "[./nets/net-330.ckpt] SAVED\n",
      "Epoch : 331/500  Train_loss : 566.8120728  Test_loss : 528.4096680\n",
      "Epoch : 332/500  Train_loss : 576.2484131  Test_loss : 557.9765625\n",
      "Epoch : 333/500  Train_loss : 492.7064209  Test_loss : 504.6238403\n",
      "Epoch : 334/500  Train_loss : 481.2911987  Test_loss : 483.1269531\n",
      "Epoch : 335/500  Train_loss : 583.6784668  Test_loss : 487.9558105\n",
      "[./nets/net-335.ckpt] SAVED\n",
      "Epoch : 336/500  Train_loss : 492.5053406  Test_loss : 473.8242493\n",
      "Epoch : 337/500  Train_loss : 530.6912842  Test_loss : 477.6291809\n",
      "Epoch : 338/500  Train_loss : 437.9438477  Test_loss : 474.6239319\n",
      "Epoch : 339/500  Train_loss : 472.0271606  Test_loss : 512.6568604\n",
      "Epoch : 340/500  Train_loss : 432.8314209  Test_loss : 501.3833923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[./nets/net-340.ckpt] SAVED\n",
      "Epoch : 341/500  Train_loss : 549.3237305  Test_loss : 505.4910889\n",
      "Epoch : 342/500  Train_loss : 532.0353394  Test_loss : 536.1474609\n",
      "Epoch : 343/500  Train_loss : 461.1764526  Test_loss : 510.2357788\n",
      "Epoch : 344/500  Train_loss : 467.2614136  Test_loss : 578.1980591\n",
      "Epoch : 345/500  Train_loss : 539.4021606  Test_loss : 475.9904175\n",
      "[./nets/net-345.ckpt] SAVED\n",
      "Epoch : 346/500  Train_loss : 465.9628296  Test_loss : 467.9727783\n",
      "Epoch : 347/500  Train_loss : 553.7073975  Test_loss : 542.1812744\n",
      "Epoch : 348/500  Train_loss : 603.6506348  Test_loss : 543.3433838\n",
      "Epoch : 349/500  Train_loss : 429.9040527  Test_loss : 529.1341553\n",
      "Epoch : 350/500  Train_loss : 409.2247009  Test_loss : 501.1536560\n",
      "[./nets/net-350.ckpt] SAVED\n",
      "Epoch : 351/500  Train_loss : 546.6240845  Test_loss : 529.5375366\n",
      "Epoch : 352/500  Train_loss : 501.4218445  Test_loss : 522.2078857\n",
      "Epoch : 353/500  Train_loss : 516.8319702  Test_loss : 503.2602539\n",
      "Epoch : 354/500  Train_loss : 390.9700928  Test_loss : 499.4083557\n",
      "Epoch : 355/500  Train_loss : 539.5933838  Test_loss : 636.2337036\n",
      "[./nets/net-355.ckpt] SAVED\n",
      "Epoch : 356/500  Train_loss : 540.2730103  Test_loss : 533.3962402\n",
      "Epoch : 357/500  Train_loss : 519.4997559  Test_loss : 532.3754883\n",
      "Epoch : 358/500  Train_loss : 609.2848511  Test_loss : 523.5444336\n",
      "Epoch : 359/500  Train_loss : 463.9011230  Test_loss : 540.0051270\n",
      "Epoch : 360/500  Train_loss : 538.8533936  Test_loss : 548.0172119\n",
      "[./nets/net-360.ckpt] SAVED\n",
      "Epoch : 361/500  Train_loss : 482.4741211  Test_loss : 624.9516602\n",
      "Epoch : 362/500  Train_loss : 434.6638794  Test_loss : 591.1475830\n",
      "Epoch : 363/500  Train_loss : 461.0849609  Test_loss : 471.8819580\n",
      "Epoch : 364/500  Train_loss : 472.6396484  Test_loss : 443.5745239\n",
      "Epoch : 365/500  Train_loss : 507.9213257  Test_loss : 566.5058594\n",
      "[./nets/net-365.ckpt] SAVED\n",
      "Epoch : 366/500  Train_loss : 504.5812378  Test_loss : 591.7434082\n",
      "Epoch : 367/500  Train_loss : 447.3901062  Test_loss : 566.4887085\n",
      "Epoch : 368/500  Train_loss : 470.9497070  Test_loss : 502.1915283\n",
      "Epoch : 369/500  Train_loss : 569.5490723  Test_loss : 509.5929260\n",
      "Epoch : 370/500  Train_loss : 462.5401917  Test_loss : 487.9658813\n",
      "[./nets/net-370.ckpt] SAVED\n",
      "Epoch : 371/500  Train_loss : 532.2791138  Test_loss : 472.6818848\n",
      "Epoch : 372/500  Train_loss : 483.5799561  Test_loss : 480.9176025\n",
      "Epoch : 373/500  Train_loss : 463.6609802  Test_loss : 528.1521606\n",
      "Epoch : 374/500  Train_loss : 504.3515320  Test_loss : 476.7536011\n",
      "Epoch : 375/500  Train_loss : 447.4483032  Test_loss : 513.0649414\n",
      "[./nets/net-375.ckpt] SAVED\n",
      "Epoch : 376/500  Train_loss : 529.0059204  Test_loss : 469.0313416\n",
      "Epoch : 377/500  Train_loss : 491.9806824  Test_loss : 502.6769104\n",
      "Epoch : 378/500  Train_loss : 499.6378174  Test_loss : 600.0053711\n",
      "Epoch : 379/500  Train_loss : 539.1935425  Test_loss : 512.7376099\n",
      "Epoch : 380/500  Train_loss : 420.1812134  Test_loss : 485.8420410\n",
      "[./nets/net-380.ckpt] SAVED\n",
      "Epoch : 381/500  Train_loss : 526.9213257  Test_loss : 521.7281494\n",
      "Epoch : 382/500  Train_loss : 497.0169678  Test_loss : 501.0605774\n",
      "Epoch : 383/500  Train_loss : 430.4899597  Test_loss : 483.6160278\n",
      "Epoch : 384/500  Train_loss : 485.0875549  Test_loss : 533.9648438\n",
      "Epoch : 385/500  Train_loss : 544.4682617  Test_loss : 444.9829102\n",
      "[./nets/net-385.ckpt] SAVED\n",
      "Epoch : 386/500  Train_loss : 522.4692993  Test_loss : 511.1781616\n",
      "Epoch : 387/500  Train_loss : 446.9728088  Test_loss : 592.7086182\n",
      "Epoch : 388/500  Train_loss : 495.6448669  Test_loss : 529.9195557\n",
      "Epoch : 389/500  Train_loss : 474.7930298  Test_loss : 501.1213989\n",
      "Epoch : 390/500  Train_loss : 510.5988464  Test_loss : 485.7259216\n",
      "[./nets/net-390.ckpt] SAVED\n",
      "Epoch : 391/500  Train_loss : 500.6759949  Test_loss : 560.6464844\n",
      "Epoch : 392/500  Train_loss : 515.1535034  Test_loss : 481.4209595\n",
      "Epoch : 393/500  Train_loss : 530.4714355  Test_loss : 447.2758484\n",
      "Epoch : 394/500  Train_loss : 481.5377197  Test_loss : 553.4052734\n",
      "Epoch : 395/500  Train_loss : 401.8013000  Test_loss : 565.9815674\n",
      "[./nets/net-395.ckpt] SAVED\n",
      "Epoch : 396/500  Train_loss : 553.3699341  Test_loss : 510.2151184\n",
      "Epoch : 397/500  Train_loss : 523.8031616  Test_loss : 457.3663940\n",
      "Epoch : 398/500  Train_loss : 466.2363892  Test_loss : 501.7332153\n",
      "Epoch : 399/500  Train_loss : 443.0803223  Test_loss : 389.0480957\n",
      "Epoch : 400/500  Train_loss : 406.6179810  Test_loss : 515.7655029\n",
      "[./nets/net-400.ckpt] SAVED\n",
      "Epoch : 401/500  Train_loss : 423.8243103  Test_loss : 429.1936340\n",
      "Epoch : 402/500  Train_loss : 405.7908936  Test_loss : 442.5107727\n",
      "Epoch : 403/500  Train_loss : 491.5741272  Test_loss : 481.6199951\n",
      "Epoch : 404/500  Train_loss : 534.6048584  Test_loss : 593.8417969\n",
      "Epoch : 405/500  Train_loss : 430.6303101  Test_loss : 492.3134766\n",
      "[./nets/net-405.ckpt] SAVED\n",
      "Epoch : 406/500  Train_loss : 530.5979614  Test_loss : 400.2218018\n",
      "Epoch : 407/500  Train_loss : 540.3310547  Test_loss : 509.4335022\n",
      "Epoch : 408/500  Train_loss : 381.6716614  Test_loss : 488.0801697\n",
      "Epoch : 409/500  Train_loss : 520.8432007  Test_loss : 519.6322021\n",
      "Epoch : 410/500  Train_loss : 461.4316711  Test_loss : 518.4540405\n",
      "[./nets/net-410.ckpt] SAVED\n",
      "Epoch : 411/500  Train_loss : 491.9104309  Test_loss : 510.1037598\n",
      "Epoch : 412/500  Train_loss : 460.1065674  Test_loss : 551.6560669\n",
      "Epoch : 413/500  Train_loss : 497.6085815  Test_loss : 522.4934082\n",
      "Epoch : 414/500  Train_loss : 432.8783264  Test_loss : 462.6070251\n",
      "Epoch : 415/500  Train_loss : 444.0742188  Test_loss : 516.6294556\n",
      "[./nets/net-415.ckpt] SAVED\n",
      "Epoch : 416/500  Train_loss : 552.6300049  Test_loss : 447.3994751\n",
      "Epoch : 417/500  Train_loss : 441.5912781  Test_loss : 496.1583252\n",
      "Epoch : 418/500  Train_loss : 484.2466736  Test_loss : 497.6612549\n",
      "Epoch : 419/500  Train_loss : 521.3140869  Test_loss : 481.9788818\n",
      "Epoch : 420/500  Train_loss : 441.3900146  Test_loss : 494.9960632\n",
      "[./nets/net-420.ckpt] SAVED\n",
      "Epoch : 421/500  Train_loss : 573.6308594  Test_loss : 503.9451294\n",
      "Epoch : 422/500  Train_loss : 434.9962158  Test_loss : 525.7575073\n",
      "Epoch : 423/500  Train_loss : 457.7416382  Test_loss : 439.4526978\n",
      "Epoch : 424/500  Train_loss : 441.7210388  Test_loss : 449.1685791\n",
      "Epoch : 425/500  Train_loss : 591.9340820  Test_loss : 382.3026733\n",
      "[./nets/net-425.ckpt] SAVED\n",
      "Epoch : 426/500  Train_loss : 440.6993713  Test_loss : 547.0712891\n",
      "Epoch : 427/500  Train_loss : 542.2291870  Test_loss : 416.0921936\n",
      "Epoch : 428/500  Train_loss : 438.0157471  Test_loss : 442.2550659\n",
      "Epoch : 429/500  Train_loss : 547.0255127  Test_loss : 478.8190918\n",
      "Epoch : 430/500  Train_loss : 444.4706116  Test_loss : 445.4584351\n",
      "[./nets/net-430.ckpt] SAVED\n",
      "Epoch : 431/500  Train_loss : 427.2993164  Test_loss : 496.0465698\n",
      "Epoch : 432/500  Train_loss : 393.8710022  Test_loss : 513.0660400\n",
      "Epoch : 433/500  Train_loss : 450.5539551  Test_loss : 446.5651245\n",
      "Epoch : 434/500  Train_loss : 540.9899902  Test_loss : 556.3265381\n",
      "Epoch : 435/500  Train_loss : 517.1906738  Test_loss : 398.7302551\n",
      "[./nets/net-435.ckpt] SAVED\n",
      "Epoch : 436/500  Train_loss : 526.4542236  Test_loss : 463.9129333\n",
      "Epoch : 437/500  Train_loss : 443.3728333  Test_loss : 508.3433838\n",
      "Epoch : 438/500  Train_loss : 553.8695068  Test_loss : 542.4991455\n",
      "Epoch : 439/500  Train_loss : 546.6271973  Test_loss : 496.7368774\n",
      "Epoch : 440/500  Train_loss : 467.7168884  Test_loss : 547.9309692\n",
      "[./nets/net-440.ckpt] SAVED\n",
      "Epoch : 441/500  Train_loss : 496.7991028  Test_loss : 527.2487183\n",
      "Epoch : 442/500  Train_loss : 487.0483704  Test_loss : 471.7484131\n",
      "Epoch : 443/500  Train_loss : 463.4006653  Test_loss : 469.3540649\n",
      "Epoch : 444/500  Train_loss : 482.5002441  Test_loss : 443.8970642\n",
      "Epoch : 445/500  Train_loss : 388.9543762  Test_loss : 449.6164246\n",
      "[./nets/net-445.ckpt] SAVED\n",
      "Epoch : 446/500  Train_loss : 467.8768921  Test_loss : 484.5950012\n",
      "Epoch : 447/500  Train_loss : 404.7841187  Test_loss : 527.3122559\n",
      "Epoch : 448/500  Train_loss : 464.0772400  Test_loss : 484.2461548\n",
      "Epoch : 449/500  Train_loss : 479.6500244  Test_loss : 560.5338745\n",
      "Epoch : 450/500  Train_loss : 500.1073303  Test_loss : 422.8962708\n",
      "[./nets/net-450.ckpt] SAVED\n",
      "Epoch : 451/500  Train_loss : 515.0885010  Test_loss : 546.0946045\n",
      "Epoch : 452/500  Train_loss : 503.8999939  Test_loss : 524.9738770\n",
      "Epoch : 453/500  Train_loss : 465.3509521  Test_loss : 458.0774536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 454/500  Train_loss : 420.5975647  Test_loss : 501.3119812\n",
      "Epoch : 455/500  Train_loss : 391.3208618  Test_loss : 453.0982666\n",
      "[./nets/net-455.ckpt] SAVED\n",
      "Epoch : 456/500  Train_loss : 450.5901794  Test_loss : 504.9915161\n",
      "Epoch : 457/500  Train_loss : 397.4541016  Test_loss : 460.3160706\n",
      "Epoch : 458/500  Train_loss : 511.9112854  Test_loss : 571.1449585\n",
      "Epoch : 459/500  Train_loss : 527.7416992  Test_loss : 528.9648438\n",
      "Epoch : 460/500  Train_loss : 526.0844116  Test_loss : 353.5745850\n",
      "[./nets/net-460.ckpt] SAVED\n",
      "Epoch : 461/500  Train_loss : 400.3253784  Test_loss : 438.3383789\n",
      "Epoch : 462/500  Train_loss : 469.2993469  Test_loss : 378.9505615\n",
      "Epoch : 463/500  Train_loss : 432.6445923  Test_loss : 555.7593384\n",
      "Epoch : 464/500  Train_loss : 423.7343140  Test_loss : 521.4835205\n",
      "Epoch : 465/500  Train_loss : 513.4803467  Test_loss : 441.4675293\n",
      "[./nets/net-465.ckpt] SAVED\n",
      "Epoch : 466/500  Train_loss : 481.6838989  Test_loss : 498.0754395\n",
      "Epoch : 467/500  Train_loss : 461.9013062  Test_loss : 503.1905212\n",
      "Epoch : 468/500  Train_loss : 514.2086182  Test_loss : 465.3341980\n",
      "Epoch : 469/500  Train_loss : 451.7484436  Test_loss : 462.2401123\n",
      "Epoch : 470/500  Train_loss : 466.0256653  Test_loss : 482.4094238\n",
      "[./nets/net-470.ckpt] SAVED\n",
      "Epoch : 471/500  Train_loss : 477.5444946  Test_loss : 519.0569458\n",
      "Epoch : 472/500  Train_loss : 449.5636902  Test_loss : 446.3106689\n",
      "Epoch : 473/500  Train_loss : 373.1281433  Test_loss : 453.4502258\n",
      "Epoch : 474/500  Train_loss : 413.3303223  Test_loss : 447.2406921\n",
      "Epoch : 475/500  Train_loss : 423.6346741  Test_loss : 450.4606934\n",
      "[./nets/net-475.ckpt] SAVED\n",
      "Epoch : 476/500  Train_loss : 424.0702209  Test_loss : 450.7955627\n",
      "Epoch : 477/500  Train_loss : 438.2095337  Test_loss : 501.2151489\n",
      "Epoch : 478/500  Train_loss : 393.1958618  Test_loss : 522.3635864\n",
      "Epoch : 479/500  Train_loss : 481.2775879  Test_loss : 381.3679810\n",
      "Epoch : 480/500  Train_loss : 416.3884277  Test_loss : 426.7468567\n",
      "[./nets/net-480.ckpt] SAVED\n",
      "Epoch : 481/500  Train_loss : 410.1302795  Test_loss : 502.1061401\n",
      "Epoch : 482/500  Train_loss : 486.3513184  Test_loss : 565.9742432\n",
      "Epoch : 483/500  Train_loss : 479.7843628  Test_loss : 416.2687988\n",
      "Epoch : 484/500  Train_loss : 396.4324036  Test_loss : 506.6741028\n",
      "Epoch : 485/500  Train_loss : 473.6915588  Test_loss : 585.6435547\n",
      "[./nets/net-485.ckpt] SAVED\n",
      "Epoch : 486/500  Train_loss : 465.1974182  Test_loss : 514.8975220\n",
      "Epoch : 487/500  Train_loss : 498.6230469  Test_loss : 472.8879700\n",
      "Epoch : 488/500  Train_loss : 418.3284912  Test_loss : 576.4718628\n",
      "Epoch : 489/500  Train_loss : 434.8922119  Test_loss : 449.1736145\n",
      "Epoch : 490/500  Train_loss : 482.6458435  Test_loss : 462.3078003\n",
      "[./nets/net-490.ckpt] SAVED\n",
      "Epoch : 491/500  Train_loss : 418.1471558  Test_loss : 400.3048096\n",
      "Epoch : 492/500  Train_loss : 407.0402832  Test_loss : 518.3085327\n",
      "Epoch : 493/500  Train_loss : 530.0930176  Test_loss : 462.2136230\n",
      "Epoch : 494/500  Train_loss : 522.5719604  Test_loss : 438.6262817\n",
      "Epoch : 495/500  Train_loss : 485.7445374  Test_loss : 507.5515747\n",
      "[./nets/net-495.ckpt] SAVED\n",
      "Epoch : 496/500  Train_loss : 442.2741089  Test_loss : 453.7182617\n",
      "Epoch : 497/500  Train_loss : 443.2382812  Test_loss : 428.0004272\n",
      "Epoch : 498/500  Train_loss : 348.0858459  Test_loss : 450.6045227\n",
      "Epoch : 499/500  Train_loss : 402.1483154  Test_loss : 512.1154175\n",
      "Epoch : 500/500  Train_loss : 410.5394897  Test_loss : 404.3408813\n",
      "[./nets/net-500.ckpt] SAVED\n",
      "Optimization Finished\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "training_epochs = FLAGS.training_epochs\n",
    "batch_size = FLAGS.batch_size\n",
    "display_step = FLAGS.display_step\n",
    "# Plot\n",
    "n_plot = 5    # plot 5 images\n",
    "train_disp_idx = np.random.randint(mnist.train.num_examples, size=n_plot)    # fixed during train time\n",
    "test_disp_idx = np.random.randint(mnist.test.num_examples, size=n_plot)\n",
    "\n",
    "# Initialize\n",
    "sess = tf.Session(config=config)\n",
    "sess.run(init)\n",
    "\n",
    "# Optimize\n",
    "for epoch in range(training_epochs):\n",
    "    total_cost = 0.\n",
    "    n_total_batch = int(mnist.train.num_examples/batch_size)\n",
    "    rand_train_idx = np.random.randint(mnist.train.num_examples, size=batch_size)\n",
    "    rand_test_idx = np.random.randint(mnist.test.num_examples, size=batch_size)\n",
    "    \n",
    "    # Iteration\n",
    "    for i in range(n_total_batch):\n",
    "        batch_pure, _ = mnist.train.next_batch(batch_size)    # pure image\n",
    "        noise = noise_batch(batch_size)    # random noise\n",
    "        batch_crpt = occl(batch_pure, noise)   # corrupted image \n",
    "        feeds = {ph_pure: batch_pure, ph_noise: noise, ph_crpt: batch_crpt}\n",
    "        sess.run(optm, feed_dict=feeds)\n",
    "        #total_cost += sess.run(loss, feed_dict=feeds)\n",
    "    #total_cost = total_cost / mnist.train.num_examples\n",
    "    \n",
    "    train_pure = train_img[rand_train_idx]    # pure image\n",
    "    train_noise = noise_batch(batch_size)    # random noise\n",
    "    train_crpt = occl(train_pure,train_noise)   # corrupted image\n",
    "    train_feeds = {ph_pure: train_pure, ph_noise: train_noise, ph_crpt: train_crpt}\n",
    "    train_loss, tb_train_loss = sess.run([loss,_train_loss], feed_dict=train_feeds)\n",
    "    \n",
    "    test_pure = test_img[rand_test_idx]    # pure image\n",
    "    test_noise = noise_batch(batch_size)    # random noise\n",
    "    test_crpt = occl(test_pure,test_noise)   # corrupted image\n",
    "    test_feeds = {ph_pure: test_pure, ph_noise: test_noise, ph_crpt: test_crpt}\n",
    "    test_loss, tb_test_loss = sess.run([loss,_test_loss], feed_dict=test_feeds)\n",
    "\n",
    "    writer.add_summary(tb_train_loss, epoch)\n",
    "    writer.add_summary(tb_test_loss, epoch)\n",
    "    print(\"Epoch : %03d/%03d  Train_loss : %.7f  Test_loss : %.7f\" \n",
    "          % (epoch+1, training_epochs, train_loss, test_loss))   \n",
    "        \n",
    "    # Display\n",
    "    if (epoch+1) % display_step == 0:\n",
    "        train_gt_pure = train_img[train_disp_idx]    # pure image\n",
    "        train_gt_noise = noise_batch(n_plot)    # random noise\n",
    "        train_gt_crpt = occl(train_gt_pure,train_gt_noise)   # corrupted image\n",
    "        train_gt_feeds = {ph_pure: train_gt_pure, ph_noise: train_gt_noise, ph_crpt: train_gt_crpt}\n",
    "        \n",
    "        test_gt_pure = test_img[test_disp_idx]    # pure image\n",
    "        test_gt_noise = noise_batch(n_plot)    # random noise\n",
    "        test_gt_crpt = occl(test_gt_pure,test_gt_noise)   # corrupted image\n",
    "        test_gt_feeds = {ph_pure: test_gt_pure, ph_noise: test_gt_noise, ph_crpt: test_gt_crpt}\n",
    "        \n",
    "        # generated images\n",
    "        train_gen_pure, train_gen_noise, train_gen_crpt = sess.run([core_gen, shell2_gen, full_gen], \\\n",
    "                                        feed_dict=train_gt_feeds)  # 784-d vector\n",
    "        test_gen_pure, test_gen_noise, test_gen_crpt = sess.run([core_gen, shell2_gen, full_gen], \\\n",
    "                                        feed_dict=test_gt_feeds)  # 784-d vector\n",
    "        \n",
    "        # plotting results from training data\n",
    "        fig, axes = plt.subplots(nrows=4, ncols=n_plot, figsize=(10,2*n_plot))   # displaying 4*n_plot images\n",
    "        plt.setp(axes, xticks=np.arange(0,27,7), yticks=np.arange(0,27,7)) \n",
    "        for j in range(n_plot):\n",
    "            train_disp_gt_crpt = np.reshape(train_gt_crpt[j], [28,28])    # 28x28\n",
    "            axes[0, j].imshow(train_disp_gt_crpt, cmap='gray')   \n",
    "            axes[0, j].set(ylabel='gt_crpt')\n",
    "            axes[0, j].label_outer()\n",
    "            \n",
    "            train_disp_gen_pure = np.reshape(train_gen_pure[j], [28,28])    # 28x28\n",
    "            axes[1, j].imshow(train_disp_gen_pure, cmap='gray')   \n",
    "            axes[1, j].set(ylabel='gen_pure')\n",
    "            axes[1, j].label_outer()\n",
    "            \n",
    "            train_disp_gen_noise = np.reshape(train_gen_noise[j], [28,28])    # 28x28\n",
    "            axes[2, j].imshow(train_disp_gen_noise, cmap='gray')   \n",
    "            axes[2, j].set(ylabel='gen_noise')\n",
    "            axes[2, j].label_outer()\n",
    "            \n",
    "            train_disp_gen_crpt = np.reshape(train_gen_crpt[j], [28,28])    # 28x28\n",
    "            axes[3, j].imshow(train_disp_gen_crpt, cmap='gray')   \n",
    "            axes[3, j].set(ylabel='gen_crpt')\n",
    "            axes[3, j].label_outer()\n",
    "            \n",
    "#             train_disp_gt_pure = np.reshape(train_gt_pure[j], [28,28])\n",
    "#             axes[0, j].imshow(train_disp_gt_pure, cmap='gray')\n",
    "#             axes[0, j].set(ylabel='gt_pure')\n",
    "#             axes[0, j].label_outer()\n",
    "            \n",
    "#             train_disp_gt_noise = np.reshape(train_gt_noise[j], [28,28])    # 28x28\n",
    "#             axes[2, j].imshow(train_disp_gt_noise, cmap='gray')   \n",
    "#             axes[2, j].set(ylabel='gt_noise')\n",
    "#             axes[2, j].label_outer()\n",
    "                    \n",
    "        plt.savefig(outputdir+'/train/epoch %03d' %(epoch+1))    \n",
    "        plt.close(fig)\n",
    "        \n",
    "        # plotting results from testing data\n",
    "        fig, axes = plt.subplots(nrows=4, ncols=n_plot, figsize=(10,2*n_plot))   # displaying 4*n_plot images\n",
    "        plt.setp(axes, xticks=np.arange(0,27,7), yticks=np.arange(0,27,7)) \n",
    "        for k in range(n_plot):\n",
    "            test_disp_gt_crpt = np.reshape(test_gt_crpt[k], [28,28])    # 28x28\n",
    "            axes[0, k].imshow(test_disp_gt_crpt, cmap='gray')   \n",
    "            axes[0, k].set(ylabel='gt_crpt')\n",
    "            axes[0, k].label_outer()\n",
    "            \n",
    "            test_disp_gen_pure = np.reshape(test_gen_pure[k], [28,28])    # 28x28\n",
    "            axes[1, k].imshow(test_disp_gen_pure, cmap='gray')   \n",
    "            axes[1, k].set(ylabel='gen_pure')\n",
    "            axes[1, k].label_outer()           \n",
    "            \n",
    "            test_disp_gen_noise = np.reshape(test_gen_noise[k], [28,28])    # 28x28\n",
    "            axes[2, k].imshow(test_disp_gen_noise, cmap='gray')   \n",
    "            axes[2, k].set(ylabel='gen_noise')\n",
    "            axes[2, k].label_outer()\n",
    "            \n",
    "            test_disp_gen_crpt = np.reshape(test_gen_crpt[k], [28,28])    # 28x28\n",
    "            axes[3, k].imshow(test_disp_gen_crpt, cmap='gray')   \n",
    "            axes[3, k].set(ylabel='gen_crpt')\n",
    "            axes[3, k].label_outer()\n",
    "            \n",
    "#             test_disp_gt_pure = np.reshape(test_gt_pure[k], [28,28])\n",
    "#             axes[0, k].imshow(test_disp_gt_pure, cmap='gray')\n",
    "#             axes[0, k].set(ylabel='gt_pure')\n",
    "#             axes[0, k].label_outer()\n",
    "            \n",
    "#             test_disp_gt_noise = np.reshape(test_gt_noise[k], [28,28])    # 28x28\n",
    "#             axes[2, k].imshow(test_disp_gt_noise, cmap='gray')   \n",
    "#             axes[2, k].set(ylabel='gt_noise')\n",
    "#             axes[2, k].label_outer()\n",
    "                    \n",
    "        plt.savefig(outputdir+'/test/epoch %03d' %(epoch+1))    \n",
    "        plt.close(fig)\n",
    "        \n",
    "        # Save\n",
    "        if (epoch+1) % FLAGS.save_step ==0:\n",
    "            savename = savedir+\"/net-\"+str(epoch+1)+\".ckpt\"\n",
    "            saver.save(sess, savename)\n",
    "            print(\"[%s] SAVED\" % (savename))\n",
    "\n",
    "print(\"Optimization Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./nets/net-500.ckpt\n",
      "NETWORK RESTORED\n"
     ]
    }
   ],
   "source": [
    "do_restore = 1\n",
    "if do_restore == 1:\n",
    "    sess = tf.Session()\n",
    "    epoch = 500\n",
    "    savename = savedir+\"/net-\"+str(epoch)+\".ckpt\"\n",
    "    saver.restore(sess, savename)\n",
    "    print (\"NETWORK RESTORED\")\n",
    "else:\n",
    "    print (\"DO NOTHING\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmIAAAIsCAYAAABRBMX6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhBBCigRfxAghhBBCisT/A8lhpu1hvYrxAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa1fb069810>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_disp_idx = np.random.randint(mnist.test.num_examples, size=5)\n",
    "test_gt_pure = test_img[test_disp_idx]    # pure image\n",
    "test_gt_noise = noise_batch(n_plot)    # random noise\n",
    "test_gt_crpt = occl(test_gt_pure,test_gt_noise)   # corrupted image\n",
    "test_gt_feeds = {ph_crpt: test_gt_crpt}\n",
    "test_gen_pure, test_gen_noise, test_gen_crpt = sess.run([core_gen, shell2_gen, full_gen], \\\n",
    "                                                        feed_dict=test_gt_feeds)\n",
    "\n",
    "# plotting results from testing data\n",
    "fig, axes = plt.subplots(nrows=4, ncols=n_plot, figsize=(10,10))   # displaying 4*n_plot images\n",
    "plt.setp(axes, xticks=np.arange(0,27,7), yticks=np.arange(0,27,7)) \n",
    "for t in range(5):\n",
    "    test_disp_gt_crpt = np.reshape(test_gt_crpt[t], [28,28])    # 28x28\n",
    "    axes[0, t].imshow(test_disp_gt_crpt, cmap='gray')   \n",
    "    axes[0, t].set(ylabel='gt_crpt')\n",
    "    axes[0, t].label_outer()\n",
    "\n",
    "    test_disp_gen_pure = np.reshape(test_gen_pure[t], [28,28])    # 28x28\n",
    "    axes[1, t].imshow(test_disp_gen_pure, cmap='gray')   \n",
    "    axes[1, t].set(ylabel='gen_pure')\n",
    "    axes[1, t].label_outer()           \n",
    "\n",
    "    test_disp_gen_noise = np.reshape(test_gen_noise[t], [28,28])    # 28x28\n",
    "    axes[2, t].imshow(test_disp_gen_noise, cmap='gray')   \n",
    "    axes[2, t].set(ylabel='gen_noise')\n",
    "    axes[2, t].label_outer()\n",
    "\n",
    "    test_disp_gen_crpt = np.reshape(test_gen_crpt[t], [28,28])    # 28x28\n",
    "    axes[3, t].imshow(test_disp_gen_crpt, cmap='gray')   \n",
    "    axes[3, t].set(ylabel='gen_crpt')\n",
    "    axes[3, t].label_outer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
