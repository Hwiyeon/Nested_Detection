{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current version [1.3.0]\n",
      "Packages Loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import warnings\n",
    "import time\n",
    "import cPickle\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)  ## just for ignore DeprcationWarning message\n",
    "print(\"Current version [%s]\" %(tf.__version__))\n",
    "print(\"Packages Loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLAGS READY\n"
     ]
    }
   ],
   "source": [
    "# Dataset Configurations\n",
    "tf.app.flags.DEFINE_integer('img_size', 32, \"\"\"Image size of CIFAR-10 dataset\"\"\")\n",
    "tf.app.flags.DEFINE_integer('img_num', 10000, \"\"\"Number of images in one cifar batch\"\"\")\n",
    "tf.app.flags.DEFINE_integer('batch_num', 5, \"\"\"Number of cifar batches in dataset\"\"\")\n",
    "tf.app.flags.DEFINE_string('train_dir', './../../../Dataset/cifar-10-batches-py', \"\"\"Directory which contains the train data\"\"\")\n",
    "tf.app.flags.DEFINE_string('test_dir', './../../../Dataset/cifar-10-batches-py', \"\"\"Directory which contains the test data\"\"\")\n",
    "\n",
    "# Network Configurations\n",
    "tf.app.flags.DEFINE_integer('batch_size', 100, \"\"\"Number of images to process in a batch\"\"\")\n",
    "tf.app.flags.DEFINE_float('l1_ratio', 0.5, \"\"\"Ratio of level1\"\"\")\n",
    "tf.app.flags.DEFINE_float('l2_ratio', 0.5, \"\"\"Ratio of level2\"\"\")\n",
    "\n",
    "# Optimization Configurations\n",
    "tf.app.flags.DEFINE_float('lr', 0.001, \"\"\"Learning rate\"\"\")\n",
    "\n",
    "# Training Configurations\n",
    "tf.app.flags.DEFINE_integer('training_epochs', 2000, \"\"\"Number of epochs to run\"\"\")\n",
    "tf.app.flags.DEFINE_integer('display_step', 1, \"\"\"Number of iterations to display training output\"\"\")\n",
    "tf.app.flags.DEFINE_integer('save_step', 10, \"\"\"Number of interations to save checkpoint\"\"\")\n",
    "tf.app.flags.DEFINE_integer('save_max', 10, \"\"\"Number of checkpoints to remain\"\"\")\n",
    "\n",
    "\n",
    "# Save Configurations\n",
    "tf.app.flags.DEFINE_string('nets', './nets', \"\"\"Directory where to write the checkpoints\"\"\")\n",
    "tf.app.flags.DEFINE_string('outputs', './outputs', \"\"\"Directory where to save the output images\"\"\")\n",
    "tf.app.flags.DEFINE_string('tboard', './tensorboard', \"\"\"Directory where to save the tensorboard logs\"\"\")\n",
    "\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "print(\"FLAGS READY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.allow_soft_placement = True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    with tf.device('/CPU:0'):\n",
    "        with open(file, 'rb') as fo:\n",
    "            dict = cPickle.load(fo)\n",
    "        return dict\n",
    "\n",
    "def read_cifar(file):\n",
    "    with tf.device('/CPU:0'):\n",
    "        _dic = unpickle(file)\n",
    "        _img = _dic['data']/255.    # float type\n",
    "        _label = _dic['labels']    # (10000, )\n",
    "\n",
    "#         _img_shape = np.shape(_img)\n",
    "#         _img = np.reshape(np.transpose(np.reshape(_img, (-1, 3, 32, 32)), (0,2,3,1)), _img_shape) # (10000, 3072)\n",
    "\n",
    "        _img = np.transpose(np.reshape(_img, (-1, 3, 32, 32)), (0,2,3,1))\n",
    "    \n",
    "        return _img   # (10000, 32, 32, 3)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating random noise mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise_mask(prob=0.5):\n",
    "    mask = np.zeros([FLAGS.img_size, FLAGS.img_size, 3])\n",
    "    rd = np.random.random()\n",
    "    if rd > prob:\n",
    "        # threshold of the size of masks\n",
    "        uthd = FLAGS.img_size    \n",
    "        lthd = 0     \n",
    "        # mask size should be beween 14x14, 5x5\n",
    "        while(uthd>14 or lthd<5):\n",
    "            ver1 = np.random.random_integers(0, FLAGS.img_size-1, size= 2)   # vertex1\n",
    "            ver2 = np.random.random_integers(0, FLAGS.img_size-1, size= 2)    # vertex2\n",
    "            uthd = np.maximum(np.abs(ver1[0]-ver2[0]), np.abs(ver1[1]-ver2[1]))    # upperbound\n",
    "            lthd = np.minimum(np.abs(ver1[0]-ver2[0]), np.abs(ver1[1]-ver2[1]))    # lowerbound\n",
    "        xmin = np.minimum(ver1[0], ver2[0])    # left x value\n",
    "        xmax = np.maximum(ver1[0], ver2[0])    # right x value\n",
    "        ymin = np.minimum(ver1[1], ver2[1])    # top y value\n",
    "        ymax = np.maximum(ver1[1], ver2[1])    # bottom y value\n",
    "        noise = np.random.random((xmax-xmin+1, ymax-ymin+1, 3))    # random sample in [0,1]\n",
    "        mask[xmin:xmax+1, ymin:ymax+1, :] = noise    # noise mask with location\n",
    "        mask_meta = [xmin, xmax, ymin, ymax, noise, mask]\n",
    "#         mask = np.reshape(mask, [-1])\n",
    "    return mask\n",
    "\n",
    "def noise_batch(batch_num):\n",
    "    # make random noise batch\n",
    "    mask_batch = np.zeros([batch_num, FLAGS.img_size, FLAGS.img_size, 3])\n",
    "    for i in range(batch_num):\n",
    "        mask_batch[i] = noise_mask()\n",
    "    return mask_batch\n",
    "\n",
    "\n",
    "def occl(target, disturb):\n",
    "    # Occlusion generation\n",
    "    mask = (disturb==0).astype(float)\n",
    "    masked_target = np.multiply(target, mask)\n",
    "    crpt = np.add(masked_target, disturb)\n",
    "    return crpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nested Convolutional Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _nested_enc(l1, l2_s, l2, out_channel, name, filter_size=3, std=[1,2,2,1],pad='SAME', stddev=0.1):\n",
    "    l1_shape = l1.get_shape()\n",
    "    l2_shape = l2.get_shape()\n",
    "    l2_s_shape = l2_s.get_shape()\n",
    "                                   \n",
    "    #with tf.device('/CPU:0'):\n",
    "    with tf.variable_scope('level1'):\n",
    "        with tf.variable_scope(name):\n",
    "            l1_weights = tf.get_variable('weights', \n",
    "                                         [filter_size, filter_size, l1_shape[3], out_channel*FLAGS.l1_ratio], \n",
    "                                         tf.float32, \n",
    "                                         initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "            l1_biases = tf.get_variable('biases', \n",
    "                                        [out_channel*FLAGS.l1_ratio],\n",
    "                                        tf.float32, \n",
    "                                        initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "    with tf.variable_scope('level2'):\n",
    "        with tf.variable_scope(name):\n",
    "            l2_s_weights = tf.get_variable('weights_shell', \n",
    "                                           [filter_size, filter_size, l2_s_shape[3], out_channel*FLAGS.l2_ratio], \n",
    "                                           tf.float32, \n",
    "                                           initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "            l2_s_biases = tf.get_variable('biases_shell', \n",
    "                                          [out_channel*FLAGS.l2_ratio],\n",
    "                                          tf.float32, \n",
    "                                          initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "            l2_weights_1to2 = tf.get_variable('weights_1to2', \n",
    "                                           [filter_size, filter_size, l1_shape[3], out_channel*FLAGS.l2_ratio], \n",
    "                                           tf.float32, \n",
    "                                           initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "            l2_weights_2to1 = tf.get_variable('weights_2to1', \n",
    "                                           [filter_size, filter_size, l2_s_shape[3], out_channel*FLAGS.l1_ratio], \n",
    "                                           tf.float32, \n",
    "                                           initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "\n",
    "    l1_conv = tf.nn.conv2d(l1, l1_weights, strides=std, padding=pad)\n",
    "    l1_act = tf.nn.sigmoid(tf.add(l1_conv, l1_biases))\n",
    "\n",
    "    l2_s_conv = tf.nn.conv2d(l2_s, l2_s_weights, strides=std, padding=pad)\n",
    "    l2_s_act = tf.nn.sigmoid(tf.add(l2_s_conv, l2_s_biases))\n",
    "\n",
    "    l2_conv_1to1 = tf.nn.conv2d(l2[:,:,:,:l1_shape[3]], l1_weights, strides=std, padding=pad)\n",
    "    l2_conv_2to1 = tf.nn.conv2d(l2[:,:,:,l1_shape[3]:l2_shape[3]], l2_weights_2to1, strides=std, padding=pad)\n",
    "    l2_act_1 = tf.nn.sigmoid(tf.add(tf.add(l2_conv_1to1, l2_conv_2to1), l1_biases))\n",
    "    l2_conv_1to2 = tf.nn.conv2d(l2[:,:,:,:l1_shape[3]], l2_weights_1to2, strides=std, padding=pad)\n",
    "    l2_conv_2to2 = tf.nn.conv2d(l2[:,:,:,l1_shape[3]:l2_shape[3]], l2_s_weights, strides=std, padding=pad)\n",
    "    l2_act_2 = tf.nn.sigmoid(tf.add(tf.add(l2_conv_1to2, l2_conv_2to2), l2_s_biases))\n",
    "    l2_act = tf.concat((l2_act_1, l2_act_2), 3)\n",
    "        \n",
    "        \n",
    "    return l1_act, l2_s_act, l2_act\n",
    "\n",
    "def _nested_enc_init(_img, out_channel, name, filter_size=3, std=[1,2,2,1],pad='SAME', stddev=0.1):\n",
    "    # input is the input image \n",
    "    \n",
    "    #with tf.device('/CPU:0'):\n",
    "    with tf.variable_scope('level1'):\n",
    "        with tf.variable_scope(name):\n",
    "            l1_weights = tf.get_variable('weights', \n",
    "                                         [filter_size, filter_size, 3, out_channel*FLAGS.l1_ratio], \n",
    "                                         tf.float32, \n",
    "                                         initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "            l1_biases = tf.get_variable('biases', \n",
    "                                        [out_channel*FLAGS.l1_ratio],\n",
    "                                        tf.float32, \n",
    "                                        initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "    with tf.variable_scope('level2'):\n",
    "        with tf.variable_scope(name):\n",
    "            l2_s_weights = tf.get_variable('weights_shell', \n",
    "                                         [filter_size, filter_size, 3, out_channel*FLAGS.l2_ratio], \n",
    "                                         tf.float32, \n",
    "                                         initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "            l2_s_biases = tf.get_variable('biases_shell', \n",
    "                                        [out_channel*FLAGS.l2_ratio],\n",
    "                                        tf.float32, \n",
    "                                        initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "\n",
    "#     _input_img = tf.reshape(_img, [-1, FLAGS.img_size, FLAGS.img_size, 3])\n",
    "    l1_conv = tf.nn.conv2d(_img, l1_weights, strides=std, padding=pad)\n",
    "    l1_act = tf.nn.sigmoid(tf.add(l1_conv, l1_biases))\n",
    "    l2_s_conv = tf.nn.conv2d(_img, l2_s_weights, strides=std, padding=pad)\n",
    "    l2_s_act = tf.nn.sigmoid(tf.add(l2_s_conv, l2_s_biases))\n",
    "    l2_act = tf.concat((l1_act, l2_s_act), 3)\n",
    "    \n",
    "    return l1_act, l2_s_act, l2_act\n",
    "\n",
    "def _nested_enc_last(l1, l2_s, l2, out_channel, name, pad='VALID', stddev=0.1):\n",
    "    # output is an encoded vector -> Fully convolutioanl layer\n",
    "    l1_shape = l1.get_shape()    \n",
    "    l2_shape = l2.get_shape()\n",
    "    l2_s_shape = l2_s.get_shape()  \n",
    "    #l1_size = [l1_shape[1], l2_shape[2]]    # l1_shape[1], l1_shape[2] will be used in conv_transpose shape\n",
    "\n",
    "    #with tf.device('/CPU:0'):\n",
    "    with tf.variable_scope('level1'):\n",
    "        with tf.variable_scope(name):\n",
    "            l1_weights = tf.get_variable('weights', \n",
    "                                         [l1_shape[1],l1_shape[2],l1_shape[3], out_channel*FLAGS.l1_ratio], \n",
    "                                         tf.float32, \n",
    "                                         initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "            l1_biases = tf.get_variable('biases', \n",
    "                                        [out_channel*FLAGS.l1_ratio],\n",
    "                                        tf.float32, \n",
    "                                        initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "    with tf.variable_scope('level2'):\n",
    "        with tf.variable_scope(name):\n",
    "            l2_s_weights = tf.get_variable('weights_shell', \n",
    "                                         [l2_s_shape[1],l2_s_shape[2],l2_s_shape[3], out_channel*FLAGS.l2_ratio], \n",
    "                                         tf.float32, \n",
    "                                         initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "            l2_s_biases = tf.get_variable('biases_shell', \n",
    "                                        [out_channel*FLAGS.l2_ratio],\n",
    "                                        tf.float32, \n",
    "                                        initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "\n",
    "    l1_conv = tf.nn.conv2d(l1, l1_weights, strides=[1,1,1,1], padding=pad)\n",
    "    l1_act = tf.nn.sigmoid(tf.add(l1_conv, l1_biases))\n",
    "    l2_s_conv = tf.nn.conv2d(l2_s, l2_s_weights, strides=[1,1,1,1], padding=pad)\n",
    "    l2_s_act = tf.nn.sigmoid(tf.add(l2_s_conv, l2_s_biases))\n",
    "    l2_act = tf.concat((l1_act, l2_s_act), 3)\n",
    "        \n",
    "    return l1_act, l2_s_act, l2_act"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nested Convolutional Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _nested_dec(l1, l2_s, l2, out_size, out_channel, name, filter_size=3, std=[1,2,2,1], pad='SAME', stddev=0.1):\n",
    "    l1_shape = l1.get_shape()\n",
    "    l2_shape = l2.get_shape()\n",
    "    l2_s_shape = l2_s.get_shape()\n",
    "    \n",
    "    \n",
    "    l1_out_shape = [tf.shape(l1)[0],out_size[0],out_size[1],int(out_channel*FLAGS.l1_ratio)]\n",
    "    l2_s_out_shape = [tf.shape(l2_s)[0],out_size[0],out_size[1],int(out_channel*FLAGS.l2_ratio)]\n",
    "    \n",
    "    #with tf.device('/CPU:0'):\n",
    "    with tf.variable_scope('level1'):\n",
    "        with tf.variable_scope(name):\n",
    "            l1_weights = tf.get_variable('weights', \n",
    "                                         [filter_size, filter_size, out_channel*FLAGS.l1_ratio, l1_shape[3]], \n",
    "                                         tf.float32, \n",
    "                                         initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "            l1_biases = tf.get_variable('biases', \n",
    "                                        [out_channel*FLAGS.l1_ratio],\n",
    "                                        tf.float32, \n",
    "                                        initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "    with tf.variable_scope('level2'):\n",
    "        with tf.variable_scope(name):\n",
    "            l2_s_weights = tf.get_variable('weights_shell', \n",
    "                                           [filter_size, filter_size, out_channel*FLAGS.l2_ratio, l2_s_shape[3]], \n",
    "                                           tf.float32, \n",
    "                                           initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "            l2_s_biases = tf.get_variable('biases_shell', \n",
    "                                          [out_channel*FLAGS.l2_ratio],\n",
    "                                          tf.float32, \n",
    "                                          initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "            l2_weights_1to2 = tf.get_variable('weights_1to2', \n",
    "                                           [filter_size, filter_size, out_channel*FLAGS.l2_ratio, l1_shape[3]], \n",
    "                                           tf.float32, \n",
    "                                           initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "            l2_weights_2to1 = tf.get_variable('weights_2to1', \n",
    "                                           [filter_size, filter_size, out_channel*FLAGS.l1_ratio, l2_s_shape[3]], \n",
    "                                           tf.float32, \n",
    "                                           initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "\n",
    "    l1_dec = tf.nn.conv2d_transpose(l1, l1_weights, \n",
    "                                    output_shape=l1_out_shape, strides=std, padding=pad)\n",
    "    l1_act = tf.nn.sigmoid(tf.add(l1_dec, l1_biases))\n",
    "\n",
    "    l2_s_dec = tf.nn.conv2d_transpose(l1, l1_weights,\n",
    "                                       output_shape=l2_s_out_shape, strides=std, padding=pad)\n",
    "    l2_s_act = tf.nn.sigmoid(tf.add(l2_s_dec, l2_s_biases))\n",
    "\n",
    "    l2_dec_1to1 = tf.nn.conv2d_transpose(l2[:,:,:,:l1_shape[3]], l1_weights, \n",
    "                                          output_shape=l1_out_shape, strides=std, padding=pad)\n",
    "    l2_dec_2to1 = tf.nn.conv2d_transpose(l2[:,:,:,l1_shape[3]:l2_shape[3]], l2_weights_2to1,\n",
    "                                          output_shape=l1_out_shape, strides=std, padding=pad)\n",
    "    l2_act_1 = tf.nn.sigmoid(tf.add(tf.add(l2_dec_1to1, l2_dec_2to1), l1_biases))\n",
    "    l2_dec_1to2 = tf.nn.conv2d_transpose(l2[:,:,:,:l1_shape[3]], l2_weights_1to2, \n",
    "                                          output_shape=l2_s_out_shape, strides=std, padding=pad)\n",
    "    l2_dec_2to2 = tf.nn.conv2d_transpose(l2[:,:,:,l1_shape[3]:l2_shape[3]], l2_s_weights, \n",
    "                                          output_shape=l2_s_out_shape, strides=std, padding=pad)\n",
    "    l2_act_2 = tf.nn.sigmoid(tf.add(tf.add(l2_dec_1to2, l2_dec_2to2), l2_s_biases))\n",
    "    l2_act = tf.concat((l2_act_1, l2_act_2), 3)\n",
    "        \n",
    "    return l1_act, l2_s_act, l2_act\n",
    "\n",
    "    \n",
    "def _nested_dec_init(_l1, _l2_s, out_size, out_channel, name, pad='VALID', stddev=0.1):\n",
    "    # input is an encoded vector\n",
    "    # out_size is the shape for conv_transpose \n",
    "    l1_shape = _l1.get_shape()\n",
    "    l2_s_shape = _l2_s.get_shape()\n",
    "    l1_out_shape = [tf.shape(_l1)[0],out_size[0],out_size[1],int(out_channel*FLAGS.l1_ratio)]\n",
    "    l2_s_out_shape = [tf.shape(_l2_s)[0],out_size[0],out_size[1],int(out_channel*FLAGS.l2_ratio)]\n",
    "    \n",
    "    #with tf.device('/CPU:0'):\n",
    "    with tf.variable_scope('level1'):\n",
    "        with tf.variable_scope(name):\n",
    "            l1_weights = tf.get_variable('weights', \n",
    "                                         [out_size[0], out_size[1], out_channel*FLAGS.l1_ratio, l1_shape[3]], \n",
    "                                         tf.float32, \n",
    "                                         initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "            l1_biases = tf.get_variable('biases', \n",
    "                                        [out_channel*FLAGS.l1_ratio],\n",
    "                                        tf.float32, \n",
    "                                        initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "    with tf.variable_scope('level2'):\n",
    "        with tf.variable_scope(name):\n",
    "            l2_s_weights = tf.get_variable('weights_shell', \n",
    "                                         [out_size[0], out_size[1], out_channel*FLAGS.l2_ratio, l2_s_shape[3]], \n",
    "                                         tf.float32, \n",
    "                                         initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "            l2_s_biases = tf.get_variable('biases_shell', \n",
    "                                        [out_channel*FLAGS.l2_ratio],\n",
    "                                        tf.float32, \n",
    "                                        initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "\n",
    "#     l1 = tf.reshape(_l1, [-1, 1, 1, l1_shape[1]])\n",
    "#     l2_s = tf.reshape(_l2_s, [-1, 1, 1, l2_s_shape[1]])\n",
    "    l1_dec = tf.nn.conv2d_transpose(_l1, l1_weights, \n",
    "                                    output_shape=l1_out_shape, strides=[1,1,1,1], padding=pad)\n",
    "    l1_act = tf.nn.sigmoid(tf.add(l1_dec, l1_biases))\n",
    "    l2_s_dec = tf.nn.conv2d_transpose(_l2_s, l2_s_weights, \n",
    "                                    output_shape=l2_s_out_shape, strides=[1,1,1,1], padding=pad)\n",
    "    l2_s_act = tf.nn.sigmoid(tf.add(l2_s_dec, l2_s_biases))\n",
    "    l2_act = tf.concat((l1_act, l2_s_act), 3)\n",
    "    \n",
    "    return l1_act, l2_s_act, l2_act\n",
    " \n",
    "    \n",
    "def _nested_dec_last(l1, l2_s, l2, name, filter_size=3, std=[1,2,2,1], pad='SAME', stddev=0.1):\n",
    "    # output is original size image\n",
    "    l1_shape = l1.get_shape()\n",
    "    l2_shape = l2.get_shape()\n",
    "    l2_s_shape = l2_s.get_shape()\n",
    "    \n",
    "    _out_shape = [tf.shape(l1)[0],FLAGS.img_size,FLAGS.img_size,3]\n",
    "\n",
    "    #with tf.device('/CPU:0'):\n",
    "    with tf.variable_scope('level1'):\n",
    "        with tf.variable_scope(name):\n",
    "            l1_weights = tf.get_variable('weights', \n",
    "                                         [filter_size, filter_size, 3, l1_shape[3]], \n",
    "                                         tf.float32, \n",
    "                                         initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "            out_biases = tf.get_variable('biases', \n",
    "                                        [3],\n",
    "                                        tf.float32, \n",
    "                                        initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "    with tf.variable_scope('level2'):\n",
    "        with tf.variable_scope(name):\n",
    "            l2_s_weights = tf.get_variable('weights_shell', \n",
    "                                         [filter_size, filter_size, 3, l2_s_shape[3]], \n",
    "                                         tf.float32, \n",
    "                                         initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "\n",
    "    l1_dec = tf.nn.conv2d_transpose(l1, l1_weights, \n",
    "                                  output_shape=_out_shape, strides=std, padding=pad)\n",
    "    l1_act = tf.nn.sigmoid(tf.add(l1_dec, out_biases))\n",
    "    l2_s_dec = tf.nn.conv2d_transpose(l2_s, l2_s_weights, \n",
    "                                    output_shape=_out_shape, strides=std, padding=pad)\n",
    "    l2_s_act = tf.nn.sigmoid(tf.add(l2_s_dec, out_biases))\n",
    "    l2_act = tf.nn.sigmoid(tf.add(tf.add(l1_dec, l2_s_dec), out_biases))\n",
    "    \n",
    "    return l1_act, l2_s_act, l2_act"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graphs Ready\n"
     ]
    }
   ],
   "source": [
    "# Network Topology\n",
    "# n_input = FLAGS.img_size*FLAGS.img_size*3\n",
    "n_enc1 = 64\n",
    "n_enc2 = 128\n",
    "n_enc3 = 256\n",
    "n_dec1 = 128\n",
    "n_dec2 = 64\n",
    "\n",
    "# Inputs and Outputs\n",
    "ph_pure = tf.placeholder(\"float\", [None, FLAGS.img_size, FLAGS.img_size, 3])    # pure image --- core\n",
    "ph_noise= tf.placeholder(\"float\", [None, FLAGS.img_size, FLAGS.img_size, 3])    # noise --- shell1\n",
    "ph_crpt = tf.placeholder(\"float\", [None, FLAGS.img_size, FLAGS.img_size, 3])    # corrupted image   --- level2\n",
    "\n",
    "\n",
    "# Model\n",
    "def nested_ae_conv(_X):\n",
    "    l1_enc1, l2_s_enc1, l2_enc1 = _nested_enc_init(_X, n_enc1, name='enc1')    # 32->16\n",
    "    l1_enc2, l2_s_enc2, l2_enc2 = _nested_enc(l1_enc1, l2_s_enc1, l2_enc1, n_enc2, name='enc2')    # 16->8\n",
    "    l1_enc3, l2_s_enc3, l2_enc3 = _nested_enc_last(l1_enc2, l2_s_enc2, l2_enc2, n_enc3, name='enc3')    # 8->4\n",
    "#     print(l1_enc3.get_shape(), l2_s_enc3.get_shape(), l2_enc3.get_shape())\n",
    "    l1_dec1, l2_s_dec1, l2_dec1 = _nested_dec_init(l1_enc3, l2_s_enc3, [8,8], n_dec1, name='dec1')    # 4->8\n",
    "    l1_dec2, l2_s_dec2, l2_dec2 = _nested_dec(l1_dec1, l2_s_dec1, l2_dec1, [16,16], n_dec2, name='dec2')    # 8->16\n",
    "    l1_out, l2_s_out, l2_out = _nested_dec_last(l1_dec2, l2_s_dec2, l2_dec2, name='out')    #16->32\n",
    "    return l1_out, l2_s_out, l2_out\n",
    "\n",
    "# Generation\n",
    "core_gen, shell2_gen, full_gen = nested_ae_conv(ph_crpt)   # [None, n_input]\n",
    "\n",
    "# Loss & Optimizer\n",
    "with tf.name_scope(\"loss\") as scope:\n",
    "    loss = tf.reduce_mean(tf.nn.l2_loss(full_gen-ph_crpt)) + tf.reduce_mean(tf.nn.l2_loss(core_gen-ph_pure))\\\n",
    "            + tf.reduce_mean(tf.nn.l2_loss(shell2_gen-ph_noise))\n",
    "    _train_loss = tf.summary.scalar(\"train_loss\", loss)\n",
    "    _test_loss = tf.summary.scalar(\"test_loss\", loss)\n",
    "\n",
    "optm = tf.train.AdamOptimizer(learning_rate=FLAGS.lr).minimize(loss)\n",
    "\n",
    "\n",
    "print(\"Graphs Ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize Ready\n"
     ]
    }
   ],
   "source": [
    "merged = tf.summary.merge_all()\n",
    "tensorboard_path = FLAGS.tboard\n",
    "if not os.path.exists(tensorboard_path):\n",
    "    os.makedirs(tensorboard_path)\n",
    "writer = tf.summary.FileWriter(tensorboard_path)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "print(\"Initialize Ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saver ready\n"
     ]
    }
   ],
   "source": [
    "outputdir = FLAGS.outputs\n",
    "if not os.path.exists(outputdir+'/train'):\n",
    "    os.makedirs(outputdir+'/train')\n",
    "\n",
    "if not os.path.exists(outputdir+'/test'):\n",
    "    os.makedirs(outputdir+'/test')\n",
    "    \n",
    "savedir = FLAGS.nets\n",
    "if not os.path.exists(savedir):\n",
    "    os.makedirs(savedir)\n",
    "    \n",
    "saver = tf.train.Saver(max_to_keep=FLAGS.save_max)\n",
    "print(\"Saver ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 001/2000 data_batch_1,  Train_loss : 33867.5234  Test_loss : 35391.3203, Time/batch_file : 9.5317, Training time: 9.5331\n",
      "Epoch : 001/2000 data_batch_2,  Train_loss : 32329.7266  Test_loss : 32122.0039, Time/batch_file : 8.7131, Training time: 18.2465\n"
     ]
    }
   ],
   "source": [
    "#################################################\n",
    "# Parameters\n",
    "training_epochs = FLAGS.training_epochs\n",
    "batch_num = FLAGS.batch_num\n",
    "batch_size = FLAGS.batch_size\n",
    "n_total_batch = int(FLAGS.img_num/batch_size)\n",
    "display_step = FLAGS.display_step\n",
    "#################################################\n",
    "# Plot parameters\n",
    "n_plot = 5    # plot 5 images\n",
    "cifar10_train_img = read_cifar(FLAGS.test_dir+'/data_batch_1')     # (10000, 32, 32, 3)\n",
    "cifar10_test_img = read_cifar(FLAGS.test_dir+'/test_batch')     # (10000, 32, 32, 3)\n",
    "train_disp_idx = np.random.randint(FLAGS.img_num, size=n_plot)    # fixed during train time\n",
    "train_gt_pure = np.copy(np.take(cifar10_train_img, train_disp_idx, axis=0))    # (n_plot, 32, 32, 3) fixed\n",
    "test_disp_idx = np.random.randint(FLAGS.img_num, size=n_plot)\n",
    "test_gt_pure = np.copy(np.take(cifar10_test_img, test_disp_idx, axis=0))    # (n_plot, 32, 32, 3) fixed\n",
    "\n",
    "rand_train_idx = np.arange(FLAGS.img_num)    # for display loss\n",
    "rand_test_idx = np.arange(FLAGS.img_num)    # for display loss\n",
    "\n",
    "##################################################\n",
    "# Initialize\n",
    "sess = tf.Session(config=config)\n",
    "sess.run(init)\n",
    "\n",
    "cifar10_test_img = read_cifar(FLAGS.test_dir+'/test_batch')     # (10000, 32, 32, 3)\n",
    "\n",
    "#################################################\n",
    "# Optimize\n",
    "start_optm = time.time()\n",
    "for epoch in range(training_epochs):\n",
    "    for cifar_batch_idx in range(FLAGS.batch_num):\n",
    "        with tf.device('/CPU:0'):\n",
    "            start_epoch = time.time()\n",
    "            cifar_batch_name = FLAGS.train_dir+'/data_batch_%d' %(cifar_batch_idx+1)\n",
    "            cifar10_img = read_cifar(cifar_batch_name)     # (10000, 32, 32, 3)\n",
    "             \n",
    "        np.random.seed(epoch)\n",
    "        np.random.shuffle(rand_train_idx)\n",
    "        np.random.shuffle(rand_test_idx)\n",
    "\n",
    "        ##################################################\n",
    "        # Iteration\n",
    "        for batch_idx in range(n_total_batch):\n",
    "#             with tf.device('/CPU:0'):\n",
    "            batch_pure = np.take(cifar10_img, rand_train_idx[batch_size*batch_idx:batch_size*(batch_idx+1)], axis=0)   # pure image\n",
    "            noise = noise_batch(batch_size)    # random noise\n",
    "            batch_crpt = occl(batch_pure, noise)   # corrupted image \n",
    "            train_feeds = {ph_pure: batch_pure, ph_noise: noise, ph_crpt: batch_crpt}\n",
    "            sess.run(optm, feed_dict=train_feeds)\n",
    "\n",
    "#         with tf.device('/CPU:0'):\n",
    "        train_loss, tb_train_loss = sess.run([loss,_train_loss], feed_dict=train_feeds)\n",
    "\n",
    "        test_pure = np.take(cifar10_test_img,rand_test_idx[:batch_size], axis=0)    # pure image\n",
    "        test_noise = noise_batch(batch_size)    # random noise\n",
    "        test_crpt = occl(test_pure,test_noise)   # corrupted image\n",
    "        test_feeds = {ph_pure: test_pure, ph_noise: test_noise, ph_crpt: test_crpt}\n",
    "        test_loss, tb_test_loss = sess.run([loss,_test_loss], feed_dict=test_feeds)\n",
    "\n",
    "        writer.add_summary(tb_train_loss, epoch)\n",
    "        writer.add_summary(tb_test_loss, epoch)\n",
    "        \n",
    "        epoch_time = time.time() - start_epoch\n",
    "        current_time = time.time() - start_optm\n",
    "        print(\"Epoch : %03d/%03d data_batch_%d,  Train_loss : %.4f  Test_loss : %.4f, Time/batch_file : %.4f, Training time: %.4f\" \n",
    "              % (epoch+1, training_epochs, cifar_batch_idx+1, train_loss, test_loss, epoch_time, current_time))   \n",
    "        \n",
    "    # Display\n",
    "    if (epoch+1) % display_step == 0:\n",
    "        # train_gt_pure  # pure image\n",
    "        train_gt_noise = noise_batch(n_plot)    # random noise\n",
    "        train_gt_crpt = occl(train_gt_pure,train_gt_noise)   # corrupted image\n",
    "        train_gt_feeds = {ph_pure: train_gt_pure, ph_noise: train_gt_noise, ph_crpt: train_gt_crpt}\n",
    "        \n",
    "        # test_gt_pure   # pure image\n",
    "        test_gt_noise = noise_batch(n_plot)    # random noise\n",
    "        test_gt_crpt = occl(test_gt_pure,test_gt_noise)   # corrupted image\n",
    "        test_gt_feeds = {ph_pure: test_gt_pure, ph_noise: test_gt_noise, ph_crpt: test_gt_crpt}\n",
    "        \n",
    "        ##########################################################\n",
    "        # generated images\n",
    "        train_gen_pure, train_gen_noise, train_gen_crpt = sess.run([core_gen, shell2_gen, full_gen], \\\n",
    "                                        feed_dict=train_gt_feeds)  # 3072-d vector\n",
    "        test_gen_pure, test_gen_noise, test_gen_crpt = sess.run([core_gen, shell2_gen, full_gen], \\\n",
    "                                        feed_dict=test_gt_feeds)  # 3072-d vector\n",
    "        \n",
    "        ##########################################################\n",
    "        # plotting results from training data\n",
    "\n",
    "        fig, axes = plt.subplots(nrows=4, ncols=n_plot, figsize=(10,2*n_plot))   # displaying 4*n_plot images\n",
    "        plt.setp(axes, xticks=np.arange(0,31,8), yticks=np.arange(0,31,8)) \n",
    "        for j in range(n_plot):\n",
    "#                 train_disp_gt_crpt = np.reshape(train_gt_crpt[j], [FLAGS.img_size,FLAGS.img_size, 3])    # 28x28\n",
    "            axes[0, j].imshow(train_gt_crpt[j], cmap='gray')   \n",
    "            axes[0, j].set(ylabel='gt_crpt')\n",
    "            axes[0, j].label_outer()\n",
    "\n",
    "#                 train_disp_gen_pure = np.reshape(train_gen_pure[j], [FLAGS.img_size,FLAGS.img_size, 3])    # 28x28\n",
    "            axes[1, j].imshow(train_gen_pure[j], cmap='gray')   \n",
    "            axes[1, j].set(ylabel='gen_pure')\n",
    "            axes[1, j].label_outer()\n",
    "\n",
    "#                 train_disp_gen_noise = np.reshape(train_gen_noise[j], [FLAGS.img_size,FLAGS.img_size, 3])    # 28x28\n",
    "            axes[2, j].imshow(train_gen_noise[j], cmap='gray')   \n",
    "            axes[2, j].set(ylabel='gen_noise')\n",
    "            axes[2, j].label_outer()\n",
    "\n",
    "#                 train_disp_gen_crpt = np.reshape(train_gen_crpt[j], [FLAGS.img_size,FLAGS.img_size, 3])    # 28x28\n",
    "            axes[3, j].imshow(train_gen_crpt[j], cmap='gray')   \n",
    "            axes[3, j].set(ylabel='gen_crpt')\n",
    "            axes[3, j].label_outer()\n",
    "\n",
    "        plt.savefig(outputdir+'/train/epoch %03d' %(epoch+1))    \n",
    "        plt.close(fig)\n",
    "\n",
    "        # plotting results from testing data\n",
    "        fig, axes = plt.subplots(nrows=4, ncols=n_plot, figsize=(10,2*n_plot))   # displaying 4*n_plot images\n",
    "        plt.setp(axes, xticks=np.arange(0,31,8), yticks=np.arange(0,31,8)) \n",
    "        for k in range(n_plot):\n",
    "#                 test_disp_gt_crpt = np.reshape(test_gt_crpt[k], [FLAGS.img_size,FLAGS.img_size, 3])    # 28x28\n",
    "            axes[0, k].imshow(test_gt_crpt[k])   \n",
    "            axes[0, k].set(ylabel='gt_crpt')\n",
    "            axes[0, k].label_outer()\n",
    "\n",
    "#                 test_disp_gen_pure = np.reshape(test_gen_pure[k], [FLAGS.img_size,FLAGS.img_size, 3])    # 28x28\n",
    "            axes[1, k].imshow(test_gen_pure[k])   \n",
    "            axes[1, k].set(ylabel='gen_pure')\n",
    "            axes[1, k].label_outer()           \n",
    "\n",
    "#                 test_disp_gen_noise = np.reshape(test_gen_noise[k], [FLAGS.img_size,FLAGS.img_size, 3])    # 28x28\n",
    "            axes[2, k].imshow(test_gen_noise[k])   \n",
    "            axes[2, k].set(ylabel='gen_noise')\n",
    "            axes[2, k].label_outer()\n",
    "\n",
    "#                 test_disp_gen_crpt = np.reshape(test_gen_crpt[k], [FLAGS.img_size,FLAGS.img_size, 3])    # 28x28\n",
    "            axes[3, k].imshow(test_gen_crpt[k])   \n",
    "            axes[3, k].set(ylabel='gen_crpt')\n",
    "            axes[3, k].label_outer()\n",
    "\n",
    "\n",
    "        plt.savefig(outputdir+'/test/epoch %03d' %(epoch+1))    \n",
    "        plt.close(fig)\n",
    "\n",
    "        # Save\n",
    "        if (epoch+1) % FLAGS.save_step ==0:\n",
    "            savename = savedir+\"/net-\"+str(epoch+1)+\".ckpt\"\n",
    "            saver.save(sess, savename)\n",
    "            print(\"[%s] SAVED\" % (savename))\n",
    "\n",
    "print(\"Optimization Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_restore = 1\n",
    "if do_restore == 1:\n",
    "    sess = tf.Session()\n",
    "    epoch = FLAGS.training_epochs\n",
    "    savename = savedir+\"/net-\"+str(epoch)+\".ckpt\"\n",
    "    saver.restore(sess, savename)\n",
    "    print (\"NETWORK RESTORED\")\n",
    "else:\n",
    "    print (\"DO NOTHING\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_disp_idx = np.random.randint(FLAGS.img_num, size=n_plot)\n",
    "test_gt_pure = np.copy(np.take(cifar10_test_img, test_disp_idx, axis=0))    # (n_plot, 3072) fixed\n",
    "test_gt_noise = noise_batch(5)    # random noise\n",
    "test_gt_crpt = occl(test_gt_pure,test_gt_noise)   # corrupted image\n",
    "test_gt_feeds = {ph_crpt: test_gt_crpt}\n",
    "test_gen_pure, test_gen_noise, test_gen_crpt = sess.run([core_gen, shell2_gen, full_gen], \\\n",
    "                                                        feed_dict=test_gt_feeds)\n",
    "\n",
    "# plotting results from testing data\n",
    "fig, axes = plt.subplots(nrows=4, ncols=n_plot, figsize=(10,2*n_plot))   # displaying 4*n_plot images\n",
    "plt.setp(axes, xticks=np.arange(0,31,8), yticks=np.arange(0,31,8)) \n",
    "for k in range(n_plot):\n",
    "#     test_disp_gt_crpt = np.reshape(test_gt_crpt[k], [FLAGS.img_size,FLAGS.img_size, 3])    # 28x28\n",
    "    axes[0, k].imshow(test_gt_crpt[k])   \n",
    "    axes[0, k].set(ylabel='gt_crpt')\n",
    "    axes[0, k].label_outer()\n",
    "\n",
    "#     test_disp_gen_pure = np.reshape(test_gen_pure[k], [FLAGS.img_size,FLAGS.img_size, 3])    # 28x28\n",
    "    axes[1, k].imshow(test_gen_pure[k])   \n",
    "    axes[1, k].set(ylabel='gen_pure')\n",
    "    axes[1, k].label_outer()           \n",
    "\n",
    "#     test_disp_gen_noise = np.reshape(test_gen_noise[k], [FLAGS.img_size,FLAGS.img_size, 3])    # 28x28\n",
    "    axes[2, k].imshow(test_gen_noise[k])   \n",
    "    axes[2, k].set(ylabel='gen_noise')\n",
    "    axes[2, k].label_outer()\n",
    "\n",
    "#     test_disp_gen_crpt = np.reshape(test_gen_crpt[k], [FLAGS.img_size,FLAGS.img_size, 3])    # 28x28\n",
    "    axes[3, k].imshow(test_gen_crpt[k])   \n",
    "    axes[3, k].set(ylabel='gen_crpt')\n",
    "    axes[3, k].label_outer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
